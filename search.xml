<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2019年终总结：成为大海，刻不容缓]]></title>
    <url>%2F2020%2F01%2F04%2F%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%2F2019%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%EF%BC%9A%E6%88%90%E4%B8%BA%E5%A4%A7%E6%B5%B7%EF%BC%8C%E5%88%BB%E4%B8%8D%E5%AE%B9%E7%BC%93%2F</url>
    <content type="text"><![CDATA[​ 2019，被大多数温暖环绕，偶尔一些人生瞬间变得冰冷和坚硬，无法躲避。但还好，一切终将过去。总的来说，2019并不简单，生活频繁出拳（胖虎的300斤铁拳，直立对打，凭着经验和运气躲过一些，另外一些结结实实地砸在脸上。可以说每一拳都不含糊，一击重拳，甚至可以感受到在脸上凝固停滞的拳气。不过今年也算是和生活对过几招了，本想着步步清风认真生活，凭阅历自撰一本《人间攻略》，大摇大摆地走上建设社会的征途，没想到生活反手甩一本《人间骚浪贱指南》，害，2019全线崩溃，2020推倒重来，希望今年和生活再次交手能从容一些了。 「说真的，你将来打算怎么办呢？」「我打算喝完这一杯」 研究生毕业失与得 毕业 研究生三年，形象地说， 从一个小池塘跳到另外一个小池塘中，激起一朵Information Sciences期刊论文小浪花，然后扑通入水无踪影，从二十几岁精壮小伙想掏空世界的功利心来看，不值当。很难想象，这朵靠着身体抽搐翻腾出来小破花有多艰难，为啥别人的象牙塔是导师领进豪华直升电梯，直通塔顶，而我们要一步一爬，唉哟连特么象牙塔都是自己垒起来的，还好最后实验室人手一篇领域(次)顶刊，纷纷告别科研学术，有着不错的工作落脚处，也算是纯粹地感受了一把学术上知其然也知其所以然过程，人生历程多了一份体验，虽然不符预期，但也感激经历。 学校这几年忙忙忙，感觉也没忙出啥东西来，没发展别的兴趣，甜甜的恋爱还是没轮到我，好像蓝色大门里说的「 夏天就要过去了，我们好像什么都没做」「 是啊，就这样跑来跑去，什么都没做 」「那总会留下些什么吧，留下了什么，我们就会成为怎么样的大人 」 真要说有哪些值得关注的改变话，觉得还是有一些： 喜欢上了摇滚乐 性格从怂包变得小型社恐（到今天似乎消除了，大家都一样，五五开） 资源搜索能力加强，有需要的东西可以独立自学 处理感情还是一塌糊涂，不能成熟表达爱与索取爱 总在试探，都在权衡，消散热爱的能力 毕业的话，希望自己的期刊论文每年多一次引用吧，谷歌学术搜索排名能前一点，这个新年学术愿望不过分吧。 就业 面试造火箭，上班拧螺丝的故事早有耳闻，人力小姐姐萝莉外表杀人诛心可不含糊，论实力与面试的迷一样的相关函数，求职路上太多有意思的故事了，这一路也不容易。 忙着论文错过了秋招，磕磕碰碰地走上春招这条血路，太难了呀，简直比沥青未干的蜀道还难！一场场大型面试崩溃招聘现场， 黑压压一片学生，排着队递简历，精神上首先就完了。投了简历石沉大海，回来路上，真的是天空突然一道光打在你身上，自我怀疑的高光时刻。二十五岁这场人生三分之一考试，每次到交答卷才发现真的准备的不够充分。春招凭着发表的论文和一些算法竞赛的获奖，获得较多的面试机会，磕磕碰碰，沉舟侧畔，最后也算成功上岸了，有两个比较有印象的故事： 1）投了一些大厂算法类的岗位无果后，参加了一个猎头帮改简历的招聘会，那天去的早， 现场没啥人，改完简历后和猎头大叔聊了起来，咔咔咔一顿哭诉当代年轻人的不容易，委屈大王，心酸2019，为啥生活不如意都落我这个失意年轻人的大头上！（那段时间感情状态、生活状态各方面真的是DOWN到深海几万里）。猎头大叔当时说啥我忘记了，总之谈了很久，他让我加油坚持，不要放低要求去小企业得以慰藉，年轻人要有年轻人的样子！我心中一顿爆暖，离开的时候还喊了我回来，握了握手，说很开心和我谈话。 2）一家基金公司，总裁面，到了给offer阶段，我说还有另外一家在考虑，她说来不来她这里没关系，年轻人找工作要好好考虑，要选对行业和团队，不要盯着一个岗位就上，并给了很多中肯的年轻人意见。我是很信仰人生经验攻略的，这些年一路过来，可太缺参考物了，最后没去还是很谢谢这些不给年轻人画饼，并愿意指导年轻人意见的大佬。 给毕业求职的同学一些建议： 不要错过秋春招 清楚自己的满意和愿意接受的岗位 校招不像做饭，不需要等万事俱备才开始 每次多总结复盘，打铁还需自身硬 小心人力大姐 以上其实都是废话，多面几次自己就有谱了 不管怎样，还是顺利毕业了嘿！ 自我管理​ 1.睡眠 出来搬砖以来，睡眠变得规律很多，七点起床，十点半准时雇人敲晕自己。睡眠时间大概维持在七个半小时，午休半小时，持续搬砖问题不大。周末一般会把一周攒起来的抖音刷一遍，看看天地之间的沙雕，安心睡去。 记账 ​ 不得不说，当代年轻人独立买房还是很艰难，看着每项支出其实也还好，但是汇到一起每月支出都会比想象的多，要是活动多一些也存不了几个钱。今年出来搬砖后，把银行和朋友借的钱都还清，自己日常支出也能稳定下来，这个感觉还是很棒的！ ​ 定投指数基金，3000点入场，做一颗茁壮成长的韭菜！ 健康 ​ 今年做了个手术，还确诊了过敏性鼻炎，是要提醒自己该更加注意身体健康这一方面了。另外鼻炎应该是学校宿舍那台旧空调造成的，风口对着床吹，还滴水，每次在宿舍鼻子难受的不行，去到实验室马上就好了，弄到这个不治的毛病还是很痛苦的。目前的策略是上班走路走路走路！枸杞枸杞枸杞！泡脚泡脚泡脚！ 健身是没戏了，在学校都没能坚持下来，希望明年能坚持去游个泳吧。 关于书影音​ 五星电影： 极限职业（韩） 调音师（印度） 海蒂和爷爷（德） 复仇者联盟4（美） 摇摆狂潮（韩） 书籍： 《代码整洁之道—程序员的职业素养》 《代码整洁之道—Clean》 《黑客与画家》 《交换梦想》 《宇宙超度指南》 《如何成为一个厉害的人》 音乐演出 陈知游园惊梦 2019避雨屋檐巡演 Brett乐队 2019巡演 文雀乐队 2019她从来不唱我们的歌巡演 今年观影70+，还为观影事业买了个投影仪，每个周末的夜晚，100寸电子辐射的快乐，碳水化合物乐园，灵魂像膨化食品被打开时一样开心的裂开。听歌方面依然是摇滚为主，流行和民谣听一些，没想到居然Andrew Applepie成为了年度歌手，一度认为人类必须牵着手才能听Applepie，哎呀哎呀摇滚死了呀。 今年看书希望多看些编程类的工具书，数据库、理解框架是目前搬砖进阶的目标。 不要为今年读书太少而难过，去年你也没读多少 Flag！Flag！Flag！ 希望今年可以更自信地表达自己 搬出来一个人住，养条狗，或养个女朋友 尝试视频内容创作 开源一个自己感兴趣的工程项目 逛一逛动物园，打卡深圳所有公园 存钱买老婆 总的来说，2019并不值得被总结，是经历当中最难的一年，有很多至暗时刻不愿提及，没必要铭记些什么，2019过去了就过去了。用一些不太恰当又很冗长的比喻就是， 就像聊了18个月的心理医生突然告诉你，我不能再给你做咨询了，因为我已经爱上你了； 就像一个易碎的老年人正盯着你并且缓缓插队，而你只好故作无睹 ；就像班上倒数第二辅导倒数第一课后习题，并且给出详细的解题思路 。2020不敢说万事顺利，希望新一年遇到的问题都是不太复杂自己慢慢能处理好的，新年加油！ 新的一年，就不祝一帆风顺了，祝大家乘风破浪吧！ 以上]]></content>
      <categories>
        <category>年终总结</category>
      </categories>
      <tags>
        <tag>年终总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习（四）：循环神经网络]]></title>
    <url>%2F2018%2F12%2F18%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[在本章中，我们将看到循环神经网络背后的基本概念，他们所面临的主要问题（换句话说，在之前中讨论的消失／爆炸的梯度），以及广泛用于反抗这些问题的方法：LSTM 和 GRU cell（单元）。 循环神经网路主要解决带有时序性质的问题。 基本循环神经看下图中一个简单的循环神经网络图，它由输入层、一个隐藏层和一个输出层组成。我们可以看到，循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s（结果向前和向后传播后的上一次这个位置的值）。 如果我们把上面的图展开，循环神经网络也可以画成下面这个样子： 现在看起来就清楚不少了，这个网络在t时刻接收到输入$X_t$之后，隐藏层的值是$S_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$X_t$，还取决于$S_{t−1}$。我们可以使用下面的公式来表示循环神经网络的计算方法： (U,V,W都为权重) o_t=g(Vs_t)\\s_t=f(Ux_t+Ws_{t-1})式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值st−1st−1作为这一次的输入的权重矩阵，f是激活函数。 从上面的公式可以看出，循环层和全连接层的区别就是多了一个权重矩阵W。 若反复把式2代入带式1，我们将得到： o_t=g(Vs_t)=g(Vf(Ux_t+Ws_{t-1})) =g(Vf(Ux_t+Wf(Ux_{t-1}+Ws_{t-2 }))) =g(Vf(Ux_t+Wf(Ux_{t-1}+Wf(Ux_{t-2}+Ws_{t-3 })))) 从上面可以看出，循环神经网络的输出值$o_t$，是受前面历次输入值$x_t$、$x_{t−1}$、$x_{t−2}$…的影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。 再来看一个清晰一点的循环神经元层，见下图， 在每个时间步t，每个神经元都接收输入向量$x^{(t)}$和前一个时间步的输出向量$y^{(t−1)}$，如图所示。 请注意，输入和输出都是向量（当只有一个神经元时，输出是一个标量）。 每个循环神经元有两组权重：一组用于输入 $x^{(t)}$，另一组用于前一时间步长 $y^{(t−1)}$的输出。我们称这些权重向量为$w_x$和$w_y$。如下面公式所示（b是偏差项，φ(·)是激活函数，例如 ReLU），可以计算单个循环神经元的输出。 就像前馈神经网络一样，我们可以使用上一个公式的向量化形式，对整个小批量计算整个层的输出。 $Y^{(t)}$是$m×n_{neurons}$矩阵，包含在最小批次中每个实例在时间步t处的层输出（m是小批次中的实例数， $n_{neurons}$是神经元数 $X^{(t)}$是$m×n_{inputs}$矩阵，包含所有实例的输入的（$n_{inputs}$是输入特征的数量 ） $W_x$是$ n_{inputs}×n_{neurons} $矩阵，包含当前时间步的输入的连接权重的。 $W_y$是$n_{neurons}×n_{neurons}$矩阵，包含上一个时间步的输出的连接权重。 权重矩阵$W_x$和$W_y$通常连接成单个矩阵W，形状为$(n_{inputs}+n_{neurons})×n_{neurons}$（见上述公式第二行） b是大小为 $n_{neurons}$的向量，包含每个神经元的偏置项 注意， 在第一个时间步，t = 0，没有以前的输出，所以它们通常被假定为全零。 TensorFlow 中的解释基本 RNN首先，我们来实现一个非常简单的 RNN 模型，而不使用任何 TensorFlow 的 RNN 操作，以更好地理解发生了什么。 我们将使用 tanh 激活函数创建由 5 个循环神经元的循环层组成的 RNN（如下图所示的 RNN）。 我们将假设 RNN 只运行两个时间步，每个时间步输入大小为 3 的向量。 下面的代码构建了这个 RNN，展开了两个时间步骤： 1234567891011n_inputs = 3n_neurons = 5X0 = tf.placeholder(tf.float32, [None, n_inputs])# 充当经过向前向后传播后的下一时刻的输入值X1 = tf.placeholder(tf.float32, [None, n_inputs]) Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))Y0 = tf.tanh(tf.matmul(X0, Wx) + b)Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b) # 主要理解这句init = tf.global_variables_initializer() 双向循环神经网络对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话： 我的手机坏了，我打算____一部新手机。 可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的，但是如果我们也看到了后面的词是“一部新手机”，那么横线上的词填“买”的概率就大很多了。 而这个在单向循环神经网络是无法建模的，因此我们需要双向循环神经网络，如下图所示： 我们先考虑$y_2$的计算，从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个A′参与反向计算。最终的输出值$y_2$取决于$A_2$和$A_2’$，其计算方法为： y_2=g(VA_2+V'A_2')$A_2$和$A_2’$ 则分别计算 A_2=f(WA_1+Ux_2) A_2'=f(W'A_3'+U'x_2 ) 现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_{t−1}$有关；反向计算时，隐藏层的值$s_t′$与$s′_{t+1}$有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式1和式2，写出双向循环神经网络的计算方法： o_t=g(Vs_t+V's_t') s_t=f(Ux_t+Ws_{t-1 }) s_t'=f(U'x_t+W's_{t+1}')从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说U和U′、W和W′、V和V′都是不同的权重矩阵。 深度循环神经网络前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络。如下图所示 训练 RNN为了训练一个 RNN，诀窍是在时间上展开（就像我们刚刚做的那样），然后简单地使用常规反向传播（见图 14-5）。 这个策略被称为时间上的标准反向传播（BPTT）。另外可以采用截断式沿时间反向传播算法（BPTT）可以降低循环网络中每项参数更新的复杂度。简而言之，此种算法可以让我们以同样的运算能力更快地定型神经网络 。假设用长度为12个时间步的时间序列定型一个循环网络。我们需要进行12步的正向传递，计算误差（基于预测与实际值对比），再进行12个时间步的反向传递： 就像在正常的反向传播中一样，展开的网络（用虚线箭头表示）有第一个正向传递。然后使用损失函数评估输出序列$C(Y_{t_{min}},Y_{t_{min+1}},…,Y_{t_{max}})$。其中$t_{min}$ 和$t_{max}$ 是第一个和最后一个输出时间步长，不计算忽略的输出），并且该损失函数的梯度通过展开的网络向后传播（实线箭头）；最后使用在 BPTT 期间计算的梯度来更新模型参数。 请注意，梯度在损失函数所使用的所有输出中反向流动，而不仅仅通过最终输出（截断式传播，例如，在图 14-5 中，损失函数使用网络的最后三个输出Y(2)，Y(3)，和Y(4)，所以梯度流经这三个输出，但不通过Y(0)和Y(1)。而且，由于在每个时间步骤使用相同的参数W和b，所以反向传播将做正确的事情并且总结所有时间步骤。 具体BPTT的解析过程可以看这篇戳我 训练序列分类器我们训练一个 RNN 来分类 MNIST 图像。 卷积神经网络将更适合于图像分类，但这是一个你已经熟悉的简单例子。 我们将把每个图像视为 28 行 28 像素的序列（因为每个MNIST图像是28×28像素）。 我们将使用 150 个循环神经元的单元，再加上一个全连接层，其中包含连接到上一个时间步的输出的 10 个神经元（每个类一个），然后是一个 softmax 层（见图）。 建模阶段非常简单， 它和我们在之前中建立的 MNIST 分类器几乎是一样的，只是展开的 RNN 替换了隐层。 注意，全连接层连接到状态张量，其仅包含 RNN 的最终状态（即，第 28 个输出）。 另请注意，y是目标类的占位符。 1234567891011121314151617181920212223n_steps = 28n_inputs = 28n_neurons = 150n_outputs = 10learning_rate = 0.001X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.int32, [None])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)logits = tf.layers.dense(states, n_outputs)xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)loss = tf.reduce_mean(xentropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)correct = tf.nn.in_top_k(logits, y, 1)accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer() 现在让我们加载 MNIST 数据，并按照网络的预期方式将测试数据重塑为[batch_size, n_steps, n_inputs]。 我们之后会关注训练数据的重塑。 1234from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("/tmp/data/")X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))y_test = mnist.test.labels 现在我们准备训练 RNN 了。 执行阶段与第 10 章中 MNIST 分类器的执行阶段完全相同，不同之处在于我们在将每个训练的批量提供给网络之前要重新调整。 现在我们准备训练 RNN 了。 执行阶段与第 10 章中 MNIST 分类器的执行阶段完全相同，不同之处在于我们在将每个训练的批量提供给网络之前要重新调整。 123456789101112batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): for iteration in range(mnist.train.num_examples // batch_size): X_batch, y_batch = mnist.train.next_batch(batch_size) X_batch = X_batch.reshape((-1, n_steps, n_inputs)) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_train = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_test = accuracy.eval(feed_dict=&#123;X: X_test, y: y_test&#125;) print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test) 输出应该是这样的： 我们获得了超过 98% 的准确性 - 不错！ 另外，通过调整超参数，使用 He 初始化初始化 RNN 权重，更长时间训练或添加一些正则化（例如，droupout），你肯定会获得更好的结果。 你可以通过将其构造代码包装在一个变量作用域内（例如，使用variable_scope(&quot;rnn&quot;, initializer = variance_scaling_initializer())来使用 He 初始化）来为 RNN 指定初始化器。 为预测时间序列而训练首先，我们来创建一个 RNN。 它将包含 100 个循环神经元，并且我们将在 20 个时间步骤上展开它，因为每个训练实例将是 20 个输入那么长。 每个输入将仅包含一个特征（在该时间的值）。 目标也是 20 个输入的序列，每个输入包含一个值。 代码与之前几乎相同： 一般来说，你将不只有一个输入功能。 例如，如果你试图预测股票价格，则你可能在每个时间步骤都会有许多其他输入功能，例如竞争股票的价格，分析师的评级或可能帮助系统进行预测的任何其他功能。 在每个时间步，我们现在有一个大小为 100 的输出向量。但是我们实际需要的是每个时间步的单个输出值。 最简单的解决方法是将单元包装在OutputProjectionWrapper中。 单元包装器就像一个普通的单元，代理每个方法调用一个底层单元，但是它也增加了一些功能。Out putProjectionWrapper在每个输出之上添加一个完全连接的线性神经元层（即没有任何激活函数）（但不影响单元状态）。 所有这些完全连接的层共享相同（可训练）的权重和偏差项。 结果 RNN 如图所示 装单元是相当容易的。 让我们通过将BasicRNNCell包装到OutputProjectionWrapper中来调整前面的代码： 123cell =tf.contrib.rnn.OutputProjectionWrapper( tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,activation=tf.nn.relu), output_size=n_outputs) 到现在为止还挺好。 现在我们需要定义损失函数。 我们将使用均方误差（MSE），就像我们在之前的回归任务中所做的那样。 接下来，我们将像往常一样创建一个 Adam 优化器，训练操作和变量初始化操作。（省略） 生成 RNN到现在为止，我们已经训练了一个能够预测未来时刻样本值的模型，正如前文所述，可以用模型来生成新的序列。 为模型提供 长度为n_steps的种子序列, 比如全零序列，然后通过模型预测下一时刻的值；把该预测值添加到种子序列的末尾，用最后面 长度为n_steps的序列做为新的种子序列，做下一次预测，以此类推生成预测序列。 12345sequence = [0.] * n_stepsfor iteration in range(300): X_batch = np.array(sequence[-n_steps:].reshape(1, n_steps, 1) y_pred = sess.run(outputs, feed_dict=&#123;X: X_batch&#125; sequence.append(y_pred[0, -1, 0] LSTM 单元在训练长序列的 RNN 模型时，那么就需要把 RNN 在时间维度上展开成很深的神经网络。正如任何深度神经网络一样，其面临着梯度消失/爆炸的问题，使训练无法终止或收敛。很多之前讨论过的缓解这种问题的技巧都可以应用在深度展开的 RNN 网络：好的参数初始化方式，非饱和的激活函数（如 ReLU），批量规范化（Batch Normalization）， 梯度截断（Gradient Clipping），更快的优化器。 即便如此， RNN 在处理适中的长序列（如 100 输入序列）也在训练时表现的很慢。最简单和常见的方法解决训练时长问题就是在训练阶段仅仅展开限定时间步长的 RNN 网络，一种称为截断时间反向传播的算法。 在长的时间训练过程中，第二个要面临的问题时第一个输入的记忆会在长时间运行的 RNN 网络中逐渐淡去。 那么在一定时间后，第一个输入实际上会在 RNN 的状态中消失于无形。 为了解决其中的问题，各种能够携带长时记忆的神经单元的变体被提出。这些变体是有效的，往往基本形式的神经单元就不怎么被使用了。 首先了解一下最流行的一种长时记忆神经单元：长短时记忆神经单元 LSTM。 可以看下面这篇文章 理解LSTM网络 注：LSTM和GRU单元是近年来RNN成功背后的主要原因之一，特别实在自然语言的应用]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习（三）：卷积神经网络]]></title>
    <url>%2F2018%2F12%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[认识卷积神经网络全连接神经网络之所以不太适合图像识别任务，主要有三个方面的问题： 参数数量太多，一个输入1000×1000像素的图片有100万个神经元（一个像素点代表一个神经元） 没有利用像素之间的位置信息 网络层数限制，网络层数越多，其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过三层。 局部感受野（local receptive fields）在之前的全连接神经网络中，一个样例的输入被转换为一个一维向量。但在一个卷积网络中，把输入看作是一个按照28×28排列的正方形，或者当有颜色通道的时候，比如28x28x3，就是宽高都是28，且有3个颜色通道。比如下图就代表了一个输入 然后，我们通常把输入像素连接到一个隐藏层的神经元，但和全连接神经网络那样每个输入都连接一个隐藏层神经元不同的是，这里我们只是把输入图像进行局部的连接。 如此不断地重复，构建起第一个隐藏层。注意如果我们有一个28×28的输入图像，5×5的局部感受野，那么隐藏层中就会有24×24个神经元。这是因为在抵达抵达最右边或最底部的输入图像之前，我们只能把局部感受野向右或向下移动23个神经元。 如上图所示，把图中间的那个看作是可以“滑动的窗口”，他的作用是和输入相应的“感受域”下的像素做运算得到新的值。这个运算就是“卷积”运算了。图上面有详细的运算过程。实际上就是每个相应元素的值相乘，然后把得到的都加起来。这个窗口的本质是其中的数字和一个偏置构成的，通常就把这个窗口(Convolution kernel)叫做滤波器或者卷积核（相当于是全连接层里面要求的隐藏权重，它代表识别某个特征）。 这个“窗口”是可以滑动的，每次的滑动步长可以人为指定 。 池化(Pooling)它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。 Pooling的方法很多，最常用的是Max Pooling。 此外，还有平均池化（average pooling）和L2-norm池化。 卷积神经网络的层首先，让我们对卷积神经网络有一个感性的认识，下图就是一个卷积神经网络的示意图： 如上图所示，一个神经网络由若干卷积层（CONV）、Pooling层（POOL）、全连接层（FC）组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为： INPUT\rightarrow\left[\left[CONV\right]\times N\rightarrow POOL\right]\times M\rightarrow\left[FC\right]\times K也就是N个卷积层叠加，然后叠加一个Pooling层（可选），重复这个结构M次，最后叠加K个全连接层。 对于上图来说，该卷积神经网络的架构为： INPUT\rightarrow\left[\left[CONV\right]\times 1\rightarrow POOL\right]\times 2\rightarrow\left[FC\right]\times 2也就是N=1,M=2,K=2 我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而他的深度为1。接着第一个卷积层对这幅图像进行了卷积操作，得到了三个Feature Map。实际上这个卷积层包含三个Filter（卷积核，是隐藏不显示图上的），也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个超参数。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个通道(channel)。 在第一个卷积层之后，Pooling层对三个Feature Map做了下采样，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起（每个Fitlter与输入有相同的深度，然后对应相乘后总相加），得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。 最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。 至此，我们对卷积神经网络有了最基本的感性认识。接下来，我们将介绍卷积神经网络中各种层的计算和训练。 卷积层卷积层的参数是一些可学习的滤波器（卷积核，隐藏不显示，相当于全连接层的隐藏权重）构成，滤波器的宽度和高度一般不大，深度与其输入数据保持一致。见下图： 要点：卷积层有一个或多个滤波器（卷积核）构成，每个卷积核宽度和高度（这里为5×5）一般不大，深度（这里为3）与其输入数据保持一致。这里有6个不同的卷积核，得到的6个不同的activation map分别表示诸如边缘特征、形状特征等特征图，将这些activation map映射在深度方向上层叠起来就生成了输出数据。所以在用了6个过滤器（卷积层）之后，我们可以得到28×28×6的激活图。 卷积层输出值的计算我们使用一个简单的例子来讲述如何计算卷积，然后，抽象出卷积层的一些重要概念和计算方法。 假设有一个5×5的图像，使用一个3×3的滤波器进行卷积，想得到3×3的Feature Map，如下所示： 为了清楚地描述卷积的计算过程，我们首先对图像的每个像素进行编号，用$x_{i,j}$表示图像的第i行第j列元素，对filter的每个权重进行编号，用$w_{m,n}$表示第m行第n列权重，用$w_b$表示filter的偏置项；对Feature Map的每个元素进行编号，用$a_{i,j}$表示Feature Map的第i行第j列元素；用f表示激活函数（此处使用Relu函数作为激活函数）。然后使用下列公式计算卷积： a_{i,j}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+i,n+j}}}+w_b\right) 例如，对于Feature Map的左上角元素$a_{0,0}$来说，其卷积计算方法为： a_{0,0}=f\left(\sum_{m=0}^2{\sum_{n=0}^2{w_{m,n}x_{m+0,n+0}}}+w_b\right)=Relu\left(4\right)=4 按照这个公式可以依次计算出Feature Map中所有的值，下面的动画显示了整个Feature Map的计算过程： 填充和步幅假设输入形状的$n_h\times n_w$，卷积核窗口形状是$k_h\times k_w$，那么输出的形状将会是 (n_h-k_h+1)\times (n_w-k_w+1)所以卷积层的输出形状由输⼊形状和卷积核窗口形状决定。这里我们将介绍卷积层的两个超参数，填充和步幅。它们可以对给定形状的输⼊和卷积核改变输出形状。 填充（padding）是指在输入和宽的两侧填充元素（通常是0元素）。下图表示在原输入高的宽的两侧分别添加了值为0的元素，使得高和宽从3变成了5，并导致输出的高和宽由2增加到4. 卷积窗口从输⼊数组的最左上⽅开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。我们将每次滑动的⾏数和列数称为步幅（stride） 上面的计算过程中，步幅（stride）为1。当然步幅可以设为大于1的数。例如，当步幅为2时，Feature Map计算如下： 多输入通道到此我们讲了深度为1的卷积层的计算方法，如果深度大于1怎么计算呢？其实也是类似的。 下图展⽰了含2个输⼊通道的⼆维互相关计算的例⼦。在每个通道上，⼆维输⼊数组与⼆维核数组做互相关运算，再按通道相加即得到输出。图中阴影部分为第⼀个输出元及其计算所使⽤的输⼊和核数组元素： (1×1+2×2+4×3+5×4)+(0×0+1×1+3×2+4×3)=56 动画演示 内存需求CNN 的另一个问题是卷积层需要大量的 RAM，特别是在训练期间，因为反向传播需要在正向传递期间计算的所有中间值。 如果由于内存不足错误导致训练崩溃，则可以尝试减少小批量大小。 或者，您可以尝试使用步幅降低维度，或者删除几个图层。 或者你可以尝试使用 16 位浮点数而不是 32 位浮点数。 或者你可以在多个设备上分发 CNN。 CNN 架构典型的 CNN 体系结构有一些卷积层（每一个通常跟着一个 ReLU 层），然后是一个池化层，然后是另外几个卷积层（+ ReLU），然后是另一个池化层，等等。 随着网络的进展，图像变得越来越小，但是由于卷积层的缘故，图像通常也会越来越深（即更多的特征映射）。 在堆栈的顶部，添加由几个全连接层（+ ReLU）组成的常规前馈神经网络，并且最终层输出预测（例如，输出估计类别概率的 softmax 层）。 一个常见的错误是使用太大的卷积核。 通常可以通过将两个3×3内核堆叠在一起来获得与9×9内核相同的效果，计算量更少。 多年来，这种基础架构的变体已经被开发出来，导致了该领域的惊人进步。这里就不展开讲了，大家感兴趣可以去找相关的论文和资料深入了解这些流行的CNN架构。 LeNet-5LeNet-5 架构也许是最广为人知的 CNN 架构。 如前所述，它是由 Yann LeCun 于 1998 年创建的，广泛用于手写数字识别（MNIST）。 AlexNetAlexNet CNN 架构赢得了 2012 年的 ImageNet ILSVRC 挑战赛：它达到了 17% 的 top-5 的错误率，而第二名错误率只有 26%！ 它由 Alex Krizhevsky（因此而得名），Ilya Sutskever 和 Geoffrey Hinton 开发。 它与 LeNet-5 非常相似，只是更大更深，它是第一个将卷积层直接堆叠在一起，而不是在每个卷积层顶部堆叠一个池化层。 VGG它名字来源于论⽂作者所在的实验室Visual Geometry Group。VGG提出了可以通过重复使⽤简单的基础块来构建深度模型的思路. VGG块的组成规律是：连续使⽤数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。VGG⽹络同Alex Net和Le Net⼀样，VGG⽹络由卷积层模块后接全连接层模块构成。。全连接模块则跟Alex Net中的⼀样。 GoogLeNetGoogLeNet 架构是由 Christian Szegedy 等人开发的。 来自 Google Research，通过低于 7% 的 top-5 错误率，赢得了 ILSVRC 2014 的挑战赛。 这个伟大的表现很大程度上因为它比以前的 CNN 网络更深。 这是通过称为初始模块（inception modules）的子网络实现的，这使得 GoogLeNet 比以前的架构更有效地使用参数：实际上，GoogLeNet 的参数比 AlexNet 少了 10 倍（约 600 万而不是 6000 万）。 ResNet最后是，2015 年 ILSVRC 挑战赛的赢家 Kaiming He 等人开发的 Residual Network（或 ResNet），该网络的 top-5 误率低到惊人的 3.6%，它使用了一个非常深的 CNN，由 152 层组成。 能够训练如此深的网络的关键是使用跳过连接（skip connection，也称为快捷连接）：一个层的输入信号也被添加到位于下一层的输出。 让我们看看为什么这是有用的。 当训练一个神经网络时，目标是使其模拟一个目标函数h(x)。 如果将输入x添加到网络的输出中（即添加跳过连接），那么网络将被迫模拟f(x)= h(x) - x而不是h(x)。 这被称为残留学习 当你初始化一个普通的神经网络时，它的权重接近于零，所以网络只输出接近零的值。 如果添加跳过连接，则生成的网络只输出其输入的副本; 换句话说，它最初对身份函数进行建模。 如果目标函数与身份函数非常接近（常常是这种情况），这将大大加快训练速度。 由于跳过连接，信号可以很容易地通过整个网络。 深度剩余网络可以看作是一堆剩余单位，其中每个剩余单位是一个有跳过连接的小型神经网络。 现在让我们看看 ResNet 的架构（见下图）。 这实际上是令人惊讶的简单。 它的开始和结束与GoogLeNet完全一样（除了没有 dropout 层），而在两者之间只是一堆很简单的残余单位。 每个残差单元由两个卷积层组成，使用3×3的内核和保存空间维度（步幅 1，SAME填充），批量归一化（BN）和 ReLU 激活。 正如你所看到的，这个领域正在迅速发展，每年都会有各种各样的架构出现。 一个明显的趋势是 CNN 越来越深入。 他们也越来越轻量，需要越来越少的参数。 目前，ResNet 架构既是最强大的，也是最简单的，所以它现在应该是你应该使用的 。 Res Net中的跨层连接设计引申出了数个后续⼯作，稠密连接⽹络（Dense Net）是其中一个，Dense Net的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤。 还有其他一些架构可供您参考，特别是 VGGNet（2014 年 ILSVRC 挑战赛的亚军）和 Inception-v4（将 GooLeNet 和 ResNet 的思想融合在一起，实现了接近 3% 的 top-5 误差 ImageNet 分类率）。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习（二）：训练深层神经网络]]></title>
    <url>%2F2018%2F12%2F11%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前面介绍了人工神经网络，并训练了我们的第一个深度神经网络。 但它是一个非常浅的 DNN，只有两个隐藏层。 如果你需要解决非常复杂的问题，例如检测高分辨率图像中的数百种类型的对象，该怎么办？ 你可能需要训练更深的 DNN，也许有 10 层，每层包含数百个神经元，通过数十万个连接来连接。 这会相当困难 首先，你将面临棘手的梯度消失问题（或相关的梯度爆炸问题），这会影响深度神经网络，并使较低层难以训练。 其次，对于如此庞大的网络，训练将非常缓慢。 第三，具有数百万参数的模型将会有严重的过拟合训练集的风险。 在本章中，我们将从解释梯度消失问题开始，并探讨解决这个问题的一些最流行的解决方案。 接下来我们将看看各种优化器，它们可以加速大型模型的训练。 最后，我们将浏览一些流行的大型神经网络正则化技术。 使用这些工具，你将能够训练非常深的网络：欢迎来到深度学习的世界！ 梯度消失/爆炸问题反向传播算法的工作原理是从输出层到输入层，传播误差的梯度。 一旦该算法已经计算了网络中每个参数的损失函数的梯度，它就使用这些梯度来用梯度下降步骤来更新每个参数。 不幸的是，梯度往往变得越来越小，随着算法进展到较低层。 结果，梯度下降更新使得低层连接权重实际上保持不变，并且训练永远不会收敛到良好的解决方案。 这被称为梯度消失问题。 在某些情况下，可能会发生相反的情况：梯度可能变得越来越大，许多层得到了非常大的权重更新，算法发散。这是梯度爆炸的问题，在循环神经网络中最为常见 。 Xavier初始化和 He 初始化虽然这种不幸的行为已经经过了相当长的一段时间的实验观察 但直到 2010 年左右，人们才有了明显的进步。 Xavier Glorot 和 Yoshua Bengio 发表的题为《Understanding the Difficulty of Training Deep Feedforward Neural Networks》的论文分析了一些疑问，包括流行的 sigmoid 激活函数和当时最受欢迎的默认权重参数初始化技术的组合，即随机初始化时使用平均值为 0，标准差为 1 的正态分布 。 简而言之，他们表明，用这个激活函数和这个初始化方案，每层输出的方差远大于其输入的方差。网络正向，每层的方差持续增加，直到激活函数在顶层饱和。这实际上是因为logistic函数的平均值为 0.5 而不是 0（双曲正切函数的平均值为 0，表现略好于深层网络中的logistic函数） 。看一下logistic 激活函数，可以看到当输入变大（负或正）时，函数饱和在 0 或 1，导数非常接近 0。因此，当反向传播开始时， 它几乎没有梯度通过网络传播回来，而且由于反向传播通过顶层向下传递，所以存在的小梯度不断地被稀释，因此较低层确实没有任何东西可用。 Glorot 和 Bengio 在他们的论文中提出了一种显著缓解这个问题的方法。 我们需要信号在两个方向上正确地流动：在进行预测时是正向的，在反向传播梯度时是反向的。 我们不希望信号消失，也不希望它爆炸并饱和。 为了使信号正确流动，作者认为，我们需要每层输出的方差等于其输入的方差。 实际上不可能保证两者都是一样的，除非这个层具有相同数量的输入和输出连接，但是他们提出了一个很好的折衷办法，在实践中证明这个折中办法非常好：随机初始化连接权重必须如下面公式所描述的那样。其中n_inputs和n_outputs是权重正在被初始化的层（也称为扇入和扇出）的输入和输出连接的数量。 这种初始化策略通常被称为Xavier初始化（在作者的名字之后） 。很多初始化策略也都是为了保持每层的分布不变 ，这样利于传递信息 。 使用 Xavier 初始化策略可以大大加快训练速度，这是导致深度学习目前取得成功的技巧之一。 最近的一些论文针对不同的激活函数提供了类似的策略，如下表所示。 ReLU 激活函数（及其变体，包括简称 ELU 激活）的初始化策略有时称为 He 初始化（在其作者的姓氏之后）。 非饱和激活函数激活函数在深度神经网络中表现得更好，特别是 ReLU 激活函数，主要是因为它对正值不会饱和（也因为它的计算速度很快）。 不幸的是，ReLU激活功能并不完美。 它有一个被称为 “ReLU 死区” 的问题：在训练过程中，一些神经元有效地死亡，意味着它们停止输出 0 以外的任何东西。在某些情况下，你可能会发现你网络的一半神经元已经死亡，特别是如果你使用大学习率。 在训练期间，如果神经元的权重得到更新，使得神经元输入的加权和为负，则它将开始输出 0 。当这种情况发生时，由于当输入为负时，ReLU函数的梯度为0，神经元不可能恢复生机。 为了解决这个问题，你可能需要使用 ReLU 函数的一个变体，比如 leaky ReLU。这个函数定义为LeakyReLUα(z)= max(αz，z) 。超参数α定义了函数“leaks”的程度：它是z &lt; 0时函数的斜率，通常设置为 0.01。这个小斜坡确保 leaky ReLU 永不死亡；他们可能会长期昏迷，但他们有机会最终醒来。 事实上，设定α= 0.2（巨大 leak）似乎导致比α= 0.01（小 leak）更好的性能。 最后，Djork-Arné Clevert 等人在 2015 年的一篇论文中提出了一种称为指数线性单元（exponential linear unit，ELU）的新的激活函数，在他们的实验中表现优于所有的 ReLU 变体：训练时间减少，神经网络在测试集上表现的更好，如下所示。 它看起来很像 ReLU 函数，但有一些区别，主要区别在于： 首先它在z &lt; 0时取负值，这使得该单元的平均输出接近于 0。这有助于减轻梯度消失问题，如前所述。 超参数α定义为当z是一个大的负数时，ELU 函数接近的值。它通常设置为 1，但是如果你愿意，你可以像调整其他超参数一样调整它。 其次，它对z &lt; 0有一个非零的梯度，避免了神经元死亡的问题。 第三，函数在任何地方都是平滑的，包括z = 0左右，这有助于加速梯度下降，因为它不会弹回z = 0的左侧和右侧。 ELU 激活函数的主要缺点是计算速度慢于 ReLU 及其变体（由于使用指数函数），但是在训练过程中，这是通过更快的收敛速度来补偿的。 然而，在测试时间，ELU 网络将比 ReLU 网络慢。 那么你应该使用哪个激活函数来处理深层神经网络的隐藏层？ 虽然你的里程会有所不同，一般 ELU &gt; leaky ReLU（及其变体）&gt; ReLU &gt; tanh &gt; sigmoid。 如果您关心运行时性能，那么您可能喜欢 leaky ReLU超过ELU。 批量标准化尽管使用 He初始化和 ELU（或任何 ReLU 变体）可以显著减少训练开始阶段的梯度消失/爆炸问题，但不保证在训练期间问题不会回来。 在 2015 年的一篇论文中，Sergey Ioffe 和 Christian Szegedy 提出了一种称为批量标准化（Batch Normalization，BN）的技术来解决梯度消失/爆炸问题，每层输入的分布在训练期间改变的问题，更普遍的问题是当前一层的参数改变，每层输入的分布会在训练过程中发生变化（他们称之为内部协变量偏移问题）。 该技术包括在每层的激活函数之前在模型中添加操作，简单地对输入进行zero-centering和规范化，然后每层使用两个新参数（一个用于尺度变换，另一个用于偏移）对结果进行尺度变换和偏移。 换句话说，这个操作可以让模型学习到每层输入值的最佳尺度,均值。为了对输入进行归零和归一化，算法需要估计输入的均值和标准差。 它通过评估当前小批量输入的均值和标准差（因此命名为“批量标准化”）来实现。 在测试时，没有小批量计算经验均值和标准差，所以您只需使用整个训练集的均值和标准差。 这些通常在训练期间使用移动平均值进行有效计算。 因此，总的来说，每个批次标准化的层次都学习了四个参数：γ（标度），β（偏移），μ（平均值）和σ（标准差） 作者证明，这项技术大大改善了他们试验的所有深度神经网络。梯度消失问题大大减少了，他们可以使用饱和激活函数，如 tanh 甚至 sigmoid 激活函数。网络对权重初始化也不那么敏感。他们能够使用更大的学习率，显著加快了学习过程。 由于每层所需的额外计算，神经网络的预测速度较慢。 所以，如果你需要预测闪电般快速，你可能想要检查普通ELU + He初始化执行之前如何执行批量标准化。您可能会发现，训练起初相当缓慢，而渐变下降正在寻找每层的最佳尺度和偏移量，但一旦找到合理的好值，它就会加速。 当然，如果你训练的时间越长，准确性就越好，但是由于这样一个浅的网络，批量范数和 ELU 不太可能产生非常积极的影响：它们大部分都是为了更深的网络而发光。 批量标准化和初始化权重参数的意义差不多，更深的理解可以看这篇 批量标准化 梯度裁剪减少梯度爆炸问题的一种常用技术是在反向传播过程中简单地剪切梯度，使它们不超过某个阈值（这对于递归神经网络是非常有用的）。 这就是所谓的梯度裁剪。一般来说，人们更喜欢批量标准化，但了解梯度裁剪以及如何实现它仍然是有用的。 复用预训练层从零开始训练一个非常大的 DNN 通常不是一个好主意，相反，您应该总是尝试找到一个现有的神经网络来完成与您正在尝试解决的任务类似的任务，然后复用这个网络的较低层：这就是所谓的迁移学习。这不仅会大大加快训练速度，还将需要更少的训练数据。 一般包括三个步骤 1、冻结较低层：在训练过程中变量不会发生变化（通常称为冻结层）。 2、缓存冻结层：由于冻结层不会改变，因此可以为每个训练实例缓存最上面的冻结层的输出。 由于训练贯穿整个数据集很多次，这将给你一个巨大的速度提升 3、调整，删除或替换较高层：对于新任务来说最有用的高层特征可能与对原始任务最有用的高层特征明显不同。 你需要找到正确的层数来复用。 一般拥有的训练数据越多，您可以解冻的层数就越多。 Model Zoos你在哪里可以找到一个类似于你想要解决的任务训练的神经网络？ 首先看看显然是在你自己的模型目录。 这是保存所有模型并组织它们的一个很好的理由，以便您以后可以轻松地检索它们。 另一个选择是在模型动物园中搜索。 许多人为了各种不同的任务而训练机器学习模型，并且善意地向公众发布预训练模型。 TensorFlow 在 https://github.com/tensorflow/models 中有自己的模型动物园。 特别是，它包含了大多数最先进的图像分类网络，如 VGG，Inception 和 ResNet（参见第 13 章，检查model/slim目录），包括代码，预训练模型和 工具来下载流行的图像数据集。 另一个流行的模型动物园是 Caffe 模型动物园。 它还包含许多在各种数据集（例如，ImageNet，Places 数据库，CIFAR10 等）上训练的计算机视觉模型（例如，LeNet，AlexNet，ZFNet，GoogLeNet，VGGNet，开始）。 Saumitro Dasgupta 写了一个转换器，可以在 https://github.com/ethereon/caffetensorflow。 更快的优化器练一个非常大的深度神经网络可能会非常缓慢。 到目前为止，我们已经看到了四种加速训练的方法（并且达到更好的解决方案）：对连接权重应用良好的初始化策略，使用良好的激活函数，使用批量规范化以及重用预训练网络的部分。 另一个巨大的速度提升来自使用比普通渐变下降优化器更快的优化器。 在本节中，我们将介绍最流行的：动量优化，Nesterov 加速梯度，AdaGrad，RMSProp，最后是 Adam 优化。 剧透：本节的结论是，您几乎总是应该使用Adam_optimization，所以如果您不关心它是如何工作的，只需使用AdamOptimizer替换您的GradientDescentOptimizer，然后跳到下一节！ 只需要这么小的改动，训练通常会快几倍。 但是，Adam 优化确实有三个可以调整的超参数（加上学习率）。 默认值通常工作的不错，但如果您需要调整它们，知道他们怎么实现的可能会有帮助。 理解这些优化器可以看这篇 神经网络的优化方法 训练稀疏模型上面所有刚刚提出的优化算法都会产生密集的模型，这意味着大多数参数都是非零的。 如果你在运行时需要一个非常快速的模型，或者如果你需要它占用较少的内存，你可能更喜欢用一个稀疏模型优化器来代替。 这时可以使用FTRL 优化器，一种由尤里·涅斯捷罗夫（Yurii Nesterov）提出的技术。 当与 l1 正则化一起使用时，这种技术通常导致非常稀疏的模型。 学习率调整自适应下降的学习速率会更好。有三种流行的方法： 性能调度：每 N 步测量验证误差（就像提前停止一样），当误差下降时，将学习率降低一个因子λ。 指数调度：将学习率设置为迭代次数t的函数$\eta(t)=\eta_0\cdot10^{-t/r}$： 这很好，但它需要调整初始速率η0和总迭代次数r。 学习率将由每r步下降 10 倍。 幂调度：设学习率为$\eta(t)=\eta_0(1+t/r)^{-c}$。 超参数c通常被设置为 1。这与指数调度类似，但是学习率下降要慢得多。 根据Andrew Senior 等2013年的论文。 比较了使用动量优化训练深度神经网络进行语音识别时一些最流行的学习率调整的性能。 作者得出结论：在这种情况下，性能调度和指数调度都表现良好，但他们更喜欢指数调度，因为它实现起来比较简单，容易调整，收敛速度略快于最佳解决方案 。 通过正则化避免过拟合1、早起停止法 2、L1和L2正则化：注意不需要对bias正则，只对权重 3、Dropout 深度神经网络最流行的正则化技术可以说是 dropout。 它由 GE Hinton 于 2012 年提出，并在 Nitish Srivastava 等人的论文中进一步详细描述，并且已被证明是非常成功的：即使是最先进的神经网络，仅仅通过增加丢失就可以提高1-2％的准确度。 是一个相当简单的算法：在每个训练步骤中，每个神经元（包括输入神经元，但不包括输出神经元）都有一个暂时“丢弃”的概率p，这意味着在这个训练步骤中它将被完全忽略， 在下一步可能会激活（见下图 ）。 超参数p称为丢失率，通常设为 50%。 训练后，神经元不会再下降。 如果观察到模型过拟合，则可以增加 dropout 率（即，减少keep_prob超参数）。 相反，如果模型欠拟合训练集，则应尝试降低 dropout 率（即增加keep_prob）。 它也可以帮助增加大层的 dropout 率，并减少小层的 dropout 率。 dropout 似乎减缓了收敛速度，但通常会在调整得当时使模型更好。 所以，这通常值得花费额外的时间和精力。 Dropconnect是dropout的变体，其中单个连接随机丢弃而不是整个神经元。 一般而言，dropout表现会更好。 最大范数正则化另一种在神经网络中非常流行的正则化技术被称为最大范数正则化：对于每个神经元，它约束输入连接的权重w，使得$\Vert w\Vert_2 \leq r$，其中r是最大范数超参数，$\Vert \cdot \Vert$是 L2 范数。 我们通常通过在每个训练步骤之后计算 $\Vert w\Vert_2$来实现这个约束，并且如果需要的话可以剪切W ，即$w\leftarrow w\frac{r}{\Vert w\Vert_2}$。 减少r增加了正则化的数量(经常剪切W)，并有助于减少过拟合。 最大范数正则化还可以帮助减轻梯度消失/爆炸问题（如果您不使用批量标准化）。 数据增强最后一个正则化技术，数据增强，包括从现有的训练实例中产生新的训练实例，人为地增加了训练集的大小。 这将减少过拟合，使之成为正则化技术。 诀窍是生成逼真的训练实例; 例如，如果您的模型是为了分类蘑菇图片，您可以稍微移动，旋转和调整训练集中的每个图片的大小，并将结果图片添加到训练集。 这迫使模型更能容忍图片中蘑菇的位置，方向和大小。 如果您希望模型对光照条件更加宽容，则可以类似地生成具有各种对比度的许多图像。 假设蘑菇是对称的，你也可以水平翻转图片。 通过结合这些转换，可以大大增加训练集的大小。 实践指南当然，如果你能找到解决类似问题的方法，你应该尝试重用预训练的神经网络的一部分。 这个默认配置可能需要调整： 如果你找不到一个好的学习率（收敛速度太慢，所以你增加了训练速度，现在收敛速度很快，但是网络的准确性不是最理想的），那么你可以尝试添加一个学习率调整，如指数衰减。 如果你的训练集太小，你可以实现数据增强。 如果你需要一个稀疏的模型，你可以添加 l1 正则化混合（并可以选择在训练后将微小的权重归零）。 如果您需要更稀疏的模型，您可以尝试使用 FTRL 而不是 Adam 优化以及 l1 正则化。 如果在运行时需要快速模型，则可能需要删除批量标准化，并可能用 leakyReLU 替换 ELU 激活函数。 有一个稀疏的模型也将有所帮助。 使用 He 初始化随机选择权重，是否可以将所有权重初始化为相同的值？ 答：不，所有的权值都应该独立采样;它们的初值不应该相同。如果任意一层的所有神经元都有相同的权值。这就像每层只有一个神经元，而且速度要慢得多。 可以将偏置初始化为 0 吗？ 答：可以 说出 ELU 激活功能与 ReLU 相比的三个优点。 答： 它可以取负值，所以任意一层神经元的平均输出通常比使用relu激活函数(从不输出负值)时更接近于0。这有助于缓解渐变消失的问题。 它总是有一个非零的导数，这避免了神经元死亡的问题。 处处平滑，而Relu的斜率在z = 0时突然从0跳到1。这样的骤减使梯度下降效果降低，因为它会在z =0附近反弹. 在哪些情况下，您想要使用以下每个激活函数：ELU，leaky ReLU（及其变体），ReLU，tanh，logistic 以及 softmax？ 答：ELU是一个不错的默认选择。如果想要训练速度更快一些可以采用 leaky ReLU（及其变体） dropout 是否会减慢训练？ 它是否会减慢推断（即预测新的实例）？ 答：dropout确实会减慢训练速度，总的来说大概是两倍。然而，它没有对推断（预测新的实例）的影响，因为它只在训练期间打开。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>深度学习</tag>
        <tag>梯度消失/爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习（一）：神经网络与反向传播]]></title>
    <url>%2F2018%2F12%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[神经元首先我们从最简单的神经网络——神经元讲起，以下即为一个神经元（Neuron）的图示： 我们知道感知机的激活函数是阶跃函数；而当我们说神经元的时，激活函数往往选择sigmoid函数或tanh函数 ,还有Relu函数（效果较好）。如下图所示 可以看出，这个单一神经元的输入输出的映射关系其实就是一个逻辑回归（logistic regression）。 神经网络模型神经网络模型所谓神经网络就是将许多神经元联结在一起，这样，一个神经元的输出就可以是另一神经元的输入。例如，下图就是一个简单的神经网络： 这样的神经网络也称之为多层感知机（MLP），MLP 由一个（通过）输入层、一个或多个称为隐藏层的 LTU （单层感知器）组成，一个最终层 LTU 称为输出层。除了输出层之外的每一层包括偏置神经元，并且全连接到下一层。当人工神经网络有两个或多个隐含层时，称为深度神经网络（DNN）。 BP反向传播反向传播指的是计算神经⽹络参数梯度的⽅法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输⼊层的顺序，依次计算并存储⽬标函数有关神经⽹络各层的中间变量以及参数的梯度。 上图依次为利用dE/dx6求出第一个dE/dw; 利用分量的点对点继续求dE/dw; 利用求和来下一个dE/dy2 ;(这里的所以x,y总输入输出都是知道的) dE/dw是有上下夹层dE/dx与上面一层的y求出的 dE/dy是由上面一层全连接的wij 与其连接的dE/dx求出的 dE/dy2 = dE/dx4, w24 和 dE/dx5, dw25的求和得到其中，y2=w24*x4,故有dx_j/dy_i = wij 用 TensorFlow 高级 API 训练 MLP与 TensorFlow 一起训练 MLP 最简单的方法是使用高级 API TF.Learn，这与 sklearn 的 API 非常相似。DNNClassifier可以很容易训练具有任意数量隐层的深度神经网络，而 softmax 输出层输出估计的类概率。例如，下面的代码训练两个隐藏层的 DNN（一个具有 300 个神经元，另一个具有 100 个神经元）和一个具有 10 个神经元的 SOFTMax 输出层进行分类： 12345import tensorflow as tffeature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X) dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_columns) dnn_clf.fit(x=X, y=y, batch_size=50, steps=40000) 如果你在 MNIST 数据集上运行这个代码（在缩放它之后，例如，通过使用 skLearn 的StandardScaler），你实际上可以得到一个在测试集上达到 98.1% 以上精度的模型！ 1&gt;&gt;&gt; from sklearn.metrics import accuracy_score &gt;&gt;&gt; y_pred = list(dnn_clf.predict(X_test)) &gt;&gt;&gt; accuracy_score(y_test, y_pred) 0.98180000000000001 TF.Learn 学习库也为评估模型提供了一些方便的功能： 1&gt;&gt;&gt; dnn_clf.evaluate(X_test, y_test) &#123;'accuracy': 0.98180002, 'global_step': 40000, 'loss': 0.073678359&#125; 使用普通 TensorFlow 训练 DNN如果您想要更好地控制网络架构，您可能更喜欢使用 TensorFlow 的较低级别的 Python API。 在本节中，我们将使用与之前的 API 相同的模型，我们将实施 Minibatch 梯度下降来在 MNIST 数据集上进行训练。 第一步是建设阶段，构建 TensorFlow 图。 第二步是执行阶段，您实际运行计算图谱来训练模型。 构造阶段开始吧。 首先我们需要导入tensorflow库。 然后我们必须指定输入和输出的数量，并设置每个层中隐藏的神经元数量： 1import tensorflow as tfn_inputs = 28*28 # MNISTn_hidden1 = 300n_hidden2 = 100n_outputs = 10 接下来，可以使用占位符节点来表示训练数据和目标。X的形状仅有部分被定义。 我们知道它将是一个 2D 张量（即一个矩阵），沿着第一个维度的实例和第二个维度的特征，我们知道特征的数量将是28×28（每像素一个特征） 但是我们不知道每个训练批次将包含多少个实例。 所以X的形状是(None, n_inputs)。 同样，我们知道y将是一个 1D 张量，每个实例有一个入口，但是我们再次不知道在这一点上训练批次的大小，所以形状(None)。 1X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int64, shape=(None), name="y") 现在让我们创建一个实际的神经网络。 占位符X将作为输入层; 在执行阶段，它将一次更换一个训练批次（注意训练批中的所有实例将由神经网络同时处理）。 现在您需要创建两个隐藏层和输出层。 两个隐藏的层几乎相同：它们只是它们所连接的输入和它们包含的神经元的数量不同。 输出层也非常相似，但它使用 softmax 激活函数而不是 ReLU 激活函数。 所以让我们创建一个neuron_layer()函数，我们将一次创建一个图层。 它将需要参数来指定输入，神经元数量，激活函数和图层的名称： 1234567891011121314def neuron_layer(X, n_neurons, name, activation=None): with tf.name_scope(name): n_inputs = int(X.get_shape()[1]) stddev = 2 / np.sqrt(n_inputs) init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) W = tf.Variable(init, name="weights") # 使用满足分布概率来更好第初始化W权重 b = tf.Variable(tf.zeros([n_neurons]), name="biases") # multiply这个函数实现的是元素级别的相乘，也就是两个相乘的数元素各自相乘，而不是矩阵乘法 # tf.matmul才算矩阵乘法，注意区别 z = tf.matmul(X, W) + b if activation == "relu": return tf.nn.relu(z) else: return z 我们逐行浏览这个代码： 首先，我们使用名称范围来创建每层的名称：它将包含该神经元层的所有计算节点。 这是可选的，但如果节点组织良好，则 TensorBoard 图形将会更加出色。 接下来，我们通过查找输入矩阵的形状并获得第二个维度的大小来获得输入数量（第一个维度用于实例数量）。 接下来的三行创建一个保存权重矩阵的W变量。 它将是包含每个输入和每个神经元之间的所有连接权重的2D张量；因此，它的形状将是(n_inputs, n_neurons)。它将被随机初始化，使用具有标准差为2/√n的截断的正态（高斯）分布(使用截断的正态分布而不是常规正态分布确保不会有任何大的权重，这可能会减慢训练。).使用这个特定的标准差有助于算法的收敛速度更快（我们将在后面进一步讨论这一点），这是对神经网络的微小调整之一，对它们的效率产生了巨大的影响）。 重要的是为所有隐藏层随机初始化连接权重，以避免梯度下降算法无法中断的任何对称性。（例如，如果将所有权重设置为 0，则所有神经元将输出 0，并且给定隐藏层中的所有神经元的误差梯度将相同。 然后，梯度下降步骤将在每个层中以相同的方式更新所有权重，因此它们将保持相等。 换句话说，尽管每层有数百个神经元，你的模型就像每层只有一个神经元一样。） 下一行创建一个偏差的b变量，初始化为 0（在这种情况下无对称问题），每个神经元有一个偏置参数。 然后我们创建一个子图来计算z = X·W + b。 该向量化实现将有效地计算输入的加权和加上层中每个神经元的偏置，对于批次中的所有实例，仅需一次. 最后，如果激活参数设置为relu，则代码返回relu(z)（即max(0,z)），否则它只返回z。 好了，现在你有一个很好的函数来创建一个神经元层。 让我们用它来创建深层神经网络！ 第一个隐藏层以X为输入。 第二个将第一个隐藏层的输出作为其输入。 最后，输出层将第二个隐藏层的输出作为其输入。 1234with tf.name_scope("dnn"): hidden1 = neuron_layer(X, n_hidden1, "hidden1", activation="relu") hidden2 = neuron_layer(hidden1, n_hidden2, "hidden2", activation="relu") logits = neuron_layer(hidden2, n_outputs, "outputs") 要注意，logit 是在通过 softmax 激活函数之前神经网络的输出：为了优化，我们稍后将处理 softmax 计算。 正如你所期望的，TensorFlow 有许多方便的功能来创建标准的神经网络层，所以通常不需要像我们刚才那样定义你自己的neuron_layer()函数。 例如，TensorFlow 的tf.layers.dense()函数创建一个完全连接的层，其中所有输入都连接到图层中的所有神经元。 它使用正确的初始化策略来负责创建权重和偏置变量，并且默认情况下不使用激活函数（我们可以使用activate_fn参数来更改它）。 它还支持正则化和归一化参数。 我们来调整上面的代码来使用tf.layers.dense()函数，而不是我们的neuron_layer()函数。 只需导入该功能，并使用以下代码替换之前所有 dnn 构建部分： 123456with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, name="hidden1", activation=tf.nn.relu) hidden2 = tf.layers.dense(hidden1, n_hidden2, name="hidden2", activation=tf.nn.relu) logits = tf.layers.dense(hidden2, n_outputs, name="outputs") 损失函数现在我们已经有了神经网络模型，我们需要定义我们用来训练的损失函数。 正如我们在之前对 Softmax 回归所做的那样，我们将使用交叉熵。 我们将使用sparse_softmax_cross_entropy_with_logits()：它根据“logit”计算交叉熵（即，在通过 softmax 激活函数之前的网络输出），并且期望以 0 到 -1 数量的整数形式的标签（在我们的例子中，从 0 到 9）。 这将给我们一个包含每个实例的交叉熵的 1D 张量。 然后，我们可以使用 TensorFlow 的reduce_mean()函数来计算所有实例的平均交叉熵。 123with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss") 该sparse_softmax_cross_entropy_with_logits()函数等同于应用 SOFTMAX 激活函数，然后计算交叉熵，但它更高效，它妥善照顾的边界情况下，比如 logits 等于 0，这就是为什么我们没有较早的应用 SOFTMAX 激活函数。 还有称为softmax_cross_entropy_with_logits()的另一个函数，该函数在标签单热载体的形式（而不是整数 0 至类的数目减 1）。 优化器我们有神经网络模型，我们有损失函数，现在我们需要定义一个GradientDescentOptimizer来调整模型参数以最小化损失函数。没什么新鲜的; 就像我们之前中所做的那样： 12345learning_rate = 0.01with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss) 评估模型性能建模阶段的最后一个重要步骤是指定如何评估模型。 我们将简单地将精度用作我们的绩效指标。 首先，对于每个实例，通过检查最高 logit 是否对应于目标类别来确定神经网络的预测是否正确。 为此，您可以使用in_top_k()函数。 这返回一个充满布尔值的 1D 张量，因此我们需要将这些布尔值转换为浮点数，然后计算平均值。 这将给我们网络的整体准确性. 123with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) 而且，像往常一样，我们需要创建一个初始化所有变量的节点，我们还将创建一个Saver来将我们训练有素的模型参数保存到磁盘中： 12init = tf.global_variables_initializer()saver = tf.train.Saver() 建模阶段结束。 这是不到 40 行代码，但相当激烈：我们为输入和目标创建占位符，我们创建了一个构建神经元层的函数，我们用它来创建 DNN，我们定义了损失函数，我们 创建了一个优化器，最后定义了性能指标。 现在到执行阶段。 执行阶段首先，我们加载 MNIST。 我们可以像之前的章节那样使用 ScikitLearn，但是 TensorFlow 提供了自己的助手来获取数据，将其缩放（0 到 1 之间），将它洗牌，并提供一个简单的功能来一次加载一个小批量： 1234from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("/tmp/data/")n_epochs = 10001 # 迭代次数batch_size = 50 # 小批量大小 现在我们去训练模型: 1234567891011with tf.Session() as sess: init.run() for epoch in range(n_epochs): for iteration in range(mnist.train.num_examples // batch_size): # 每批量一次 X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) # 运行计算程序 acc_train = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_test = accuracy.eval(feed_dict=&#123;X: mnist.test.images, y: mnist.test.labels&#125;) print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test) save_path = saver.save(sess, "./my_model_final.ckpt") 该代码打开一个 TensorFlow 会话，并运行初始化所有变量的init节点。 然后它运行的主要训练循环：在每个时期，通过一些小批次的对应于训练集的大小的代码进行迭代。 每个小批量通过next_batch()方法获取，然后代码简单地运行训练操作，为当前的小批量输入数据和目标提供。 接下来，在每个时期结束时，代码评估最后一个小批量和完整训练集上的模型，并打印出结果。 最后，模型参数保存到磁盘。 使用神经网络进行预测现在神经网络被训练了，你可以用它进行预测。 为此，您可以重复使用相同的建模阶段，但是更改执行阶段，如下所示： 12345with tf.Session() as sess: saver.restore(sess, "./my_model_final.ckpt") # or better, use save_path X_new_scaled = mnist.test.images[:20] Z = logits.eval(feed_dict=&#123;X: X_new_scaled&#125;) y_pred = np.argmax(Z, axis=1) 首先，代码从磁盘加载模型参数。 然后加载一些您想要分类的新图像。 记住应用与训练数据相同的特征缩放（在这种情况下，将其从 0 缩放到 1）。 然后代码评估对数点节点。 如果您想知道所有估计的类概率，则需要将softmax()函数应用于对数，但如果您只想预测一个类，则可以简单地选择具有最高 logit 值的类（使用argmax()函数做的伎俩）。 微调神经网络超参数神经网络的灵活性也是其主要缺点之一：有很多超参数要进行调整。 不仅可以使用任何可想象的网络拓扑（如何神经元互连），而且即使在简单的 MLP 中，您可以更改层数，每层神经元数，每层使用的激活函数类型，权重初始化逻辑等等。 你怎么知道什么组合的超参数是最适合你的任务？ 可以使用具有交叉验证的网格搜索来查找正确的超参数 ，但是由于要调整许多超参数，并且由于在大型数据集上训练神经网络需要很多时间 。像之前讨论过的，使用随机搜索要好得多 ，另一个选择是使用诸如 Oscar 之类的工具，它可以实现更复杂的算法，以帮助您快速找到一组好的超参数. 隐藏层数量实际上已经表明，只有一个隐藏层的 MLP 可以建模甚至最复杂的功能，只要它具有足够的神经元。 但是他们忽略了这样一个事实：深层网络具有比浅层网络更高的参数效率：他们可以使用比浅网格更少的神经元来建模复杂的函数，使得训练更快。 总而言之，对于许多问题，您可以从一个或两个隐藏层开始。(MNIST 数据集上容易达到 97% 以上的准确度使用两个具有相同总神经元数量的隐藏层 )；对于更复杂的问题，您可以逐渐增加隐藏层的数量，直到您开始覆盖训练集。 但是，我们很少从头开始训练这样的网络：重用预先训练的最先进的网络执行类似任务的部分更为常见。训练将会更快，需要更少的数据 。 每层隐藏层的神经元数量不幸的是，正如你所看到的，找到完美的神经元数量仍然是黑色的艺术. 一个更简单的方法是选择一个具有比实际需要的更多层次和神经元的模型，然后使用早期停止来防止它过度拟合（以及其他正则化技术，特别是 drop out，我们将在后面）。 这被称为“拉伸裤”的方法：而不是浪费时间寻找完美匹配您的大小的裤子，只需使用大型伸缩裤，缩小到合适的尺寸。 激活函数在大多数情况下，您可以在隐藏层中使用 ReLU 激活函数（或其中一个变体 ） 对于输出层，softmax 激活函数通常是分类任务的良好选择（当这些类是互斥的时）。 对于回归任务，您完全可以不使用激活函数。 为什么通常使用逻辑斯蒂回归分类器而不是经典感知器（即使用感知器训练算法训练单层的线性阈值单元）？你如何调整感知器使之等同于逻辑回归分类器？ 答：经典感知器只有在数据集是线性可分的情况下才会收敛。相比之下，逻辑回归分类器将收敛于一个很好的解决方案，即使数据集不是线性可分的，它会输出类的概率。如果你将感知器的激活函数改为逻辑激活函数(或如果有多个神经元，则为softmax激活函数)，，则等价于逻辑回归分类器。 为什么激活函数是训练第一个 MLP 的关键因素？ 答：logistic激活函数是训练第一批MLP的关键因素，因为它是一个复杂的过程导数总是不为零的，所以梯度下降总是可以沿着斜率向下滚动。当激活函数是阶跃函数，梯度下降无法移动，因为根本没有斜率。 假设有一个 MLP 有一个 10 个神经元组成的输入层，接着是一个 50 个神经元的隐藏层，最后一个 3 个神经元输出层。所有人工神经元使用 Relu 激活函数。 输入矩阵X的形状是什么？ —— -m × 10，其中m为batch size. 隐藏层的权重向量的形状以及它的偏置向量的形状如何？ ——W_h=50 × 10，b_h = 50（一维长度） 输出层的权重向量和它的偏置向量的形状是什么？ ——W_o=50 × 3，b_o=3 网络的输出矩阵Y是什么形状？ ——Y=m × 3 写出计算网络输出矩阵的方程 —$-Y=(X\cdot W_h+b_h)\cdot W_o+b_o$ 如果你想把电子邮件分类成垃圾邮件或正常邮件，你需要在输出层中有多少个神经元？在输出层中应该使用什么样的激活函数？如果你想解决 MNIST 问题，你需要多少神经元在输出层，使用什么激活函数？ 答：只需要神经系统在输出层中的一个神经元，通常在估计概率时，使用输出层的logistic激活函数。如果你想处理mnist，你需要输出层的10个神经元，你必须替换logistic函数与softmax激活函数，它可以处理多个分裂，每个类输出一个概率。如果现在想让你的神经网络预测房屋价格，则需要一个输出神经元，不使用任何激活函数输出层。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>反向传播</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（8）：聚类（2）]]></title>
    <url>%2F2018%2F11%2F14%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[基于密度的聚类密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。 DBSCAN：一种基于高密度连通区域的密度聚类DBCSAN（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。 基础概念 对象的ϵ−领域：给定对象在半径ϵϵ内的区域 核心对象：对于给定的数目m，如果一个对象的ϵ−领域至少包含m个对象，则称该对象为核心对象。 直接密度可达：给定一个对象集合D，如果p是在q的ϵ−领域内，而q是一个核心对象，我们说对象p从对象q出发时直接密度可达的。 如图ϵ=1 ，m=5， q是一个核心对象，从对象q出发到对象p是直接密度可达的。 密度可达：如果存在一个对象链$p_1,p_2,⋅⋅⋅,p_n$，使得$p_1=q$，$p_n=p$，并且对$p_i (1≤i≤n)∈D$，有$p_{i+1}$是从$p_i$关于ϵ和m直接密度可达的，则p是从q密度可达的(也就是中间连接了多个直接密度可达)。注意密度可达不是等价关系，因为它不是对称的。如果$o_1$和$o_2$都是核心对象，则都是密度可达；如果$o_2$是核心对象$o_1$不是，则$o_1$可能是从$o_2$密度可达，反过来就不可以。（需要从核心出发到不核心） 密度相连：如果对象集合D中存在一个对象o，使得对p和q是从o关于ϵ和m密度可达的，那么对象p和q是关于ϵ和m密度相连的。 (存在中间点o，分别到q和p两条路线都是密度可达，则q和p密度相连)。不像密度可达，密度相连是等价的。 算法步骤： 下面这张图来自WIKI，图上有若干个点，其中标出了A、B、C、N这四个点，据此来说明这个算法的步骤： 1、首先随机选择A点为算法实施的切入点，我们将ϵϵ设置为图中圆的半径，对象密度个数$m（minPts）$设定为4。这里我们看到，A点的ϵϵ领域包含4个对象（自己也包含在内），大于等于$m(minPts)$，则创建A作为核心对象的新簇，簇内其他点都（暂时）标记为边缘点。 2、然后在标记的边缘点中选取一个重复上一步，寻找并合并核心对象直接密度可达的对象。对暂时标记为边缘点反复递归上述算法，直至没有新的点可以更新簇时，算法结束。这样就形成了一个以A为起始的一个聚类，为图中红色的中心点和黄色的边缘点（黄红点都形成簇） 3、如果还有Points未处理，再次新产生一个类别来重新启动这个算法过程。遍历所有数据，如果有点既不是边缘点也不是中心点，将其标记为噪音。 初始，给定数据集D中的所有对象都标记为”unvisited”。DBSCAN随机地选择一个未访问的对象p，标记p为”visited”，并检查p是否为核心对象。如果不是，标记p为噪点，否则为p创建一个新的簇C，并且将领域内所有对象都放到候选集合N中（这个集合会慢慢加大）。DBSCAN迭代地把N中不属于其他簇的对象添加C中。在此过程中，对于N中标记为”unvisited”的对象p’ ，标记为”visited”，如果它是核心对象，则将它的领域节点都添加到N中。DBSCAN继续从候选集N中添加到C，直到N的集合为空。此时，簇C完全生成，于是被输出。 从上述算法可知： 每个簇至少包含一个核心对象； 非核心对象可以是簇的一部分，构成了簇的边缘（edge）； 包含过少对象的簇被认为是噪声； DBSCAN的主要优点有： 1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。 2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。 3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 DBSCAN的主要缺点有： 1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。 2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。 3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值$m(MinPts)$联合调参，不同的参数组合对最后的聚类效果有较大影响。 OPTICS:通过点排序识别聚类结构在前面介绍的DBSCAN算法中，有两个初始参数Eps（邻域半径）和minPts(Eps邻域最小点数)需要手动设置，并且聚类的结果对这两个参数的取值非常敏感，不同的取值将产生不同的聚类结果。为了克服DBSCAN算法这一缺点，提出了OPTICS算法（Ordering Points to identify the clustering structure），翻译过来就是，对点排序以此来确定簇结构。 OPTICS是对DBSCAN的一个扩展算法。该算法可以让算法对半径Eps不再敏感。只要确定minPts的值，半径Eps的轻微变化，并不会影响聚类结果 。OPTICS并不显示的产生结果类簇，而是为聚类分析生成一个增广的簇排序，从这个排序中可以得到基于任何参数Eps和minPts的DBSCAN算法的聚类结果。 核心距离与可达距离要搞清楚OPTICS算法，需要搞清楚2个新的定义：核心距离和可达距离。 核心距离：一个对象p的核心距离是使得其成为核心对象的最小半径，如果p不是核心点，其可达距离没有定义 。 可达距离：从q到p的可达距离是$\max \{core-distance(q), dist(p,q)\}$。如果q不是核心点，其从q到p的可达距离没有定义。另外对象p关于不同的核心对象，p可能有多个可达距离。p的最小可达距离代表离一个稠密簇的距离越短，越处于核心密集地段。 举例，下图中假设minPts=3，半径是ϵϵ。那么P点的核心距离是d(1,P)，点2的可达距离是d(1,P)，点3的可达距离也是d(1,P)，点4的可达距离则是d(4,P)的距离。 OPTICS算法描述输入：样本集D, 邻域半径ϵϵ, 给定点在ϵϵ领域内成为核心对象的最小领域点数MinPts 输出：具有可达距离信息的样本点输出排序 首先创建两个队列，有序队列和结果队列。（有序队列用来存储核心对象及其该核心对象的直接可达对象，并按可达距离升序排列；结果队列用来存储样本点的输出次序。你可以把有序队列里面放的理解为待处理的数据，而结果队列里放的是已经处理完的数据）。 步骤： D: 待聚类的集合 Q: 有序队列，元素按照可达距离排序，可达距离最小的在队首 O: 结果队列，最后输出结果的点集的有序队列 首先从D中取出一个核心对象p，首先p要先标记加入结果队列，它的领域则加入有序队列。从有序队列取队首q，先把队首q标记且加入结果队列，若q不为核心对象则继续从Q队列中取队首处理；否则若为核心队列则将q的领域加入到有序队列并重新排列顺序。加入新元素后再取有序队列队首依次循环处理。算法结束，输出结果队列中的有序样本点。 得到结果队列后，使用如下算法得到最终的聚类结果： 从结果队列中按顺序取出点，如果该点的可达距离不大于给定半径ϵϵ，则该点属于当前类别，否则至步骤2 如果该点的核心距离大于给定半径ϵ，则该点为噪声，可以忽略，否则该点属于新的聚类，跳至步骤1 结果队列遍历结束，则算法结束 上面的算法处理完后，我们得到了输出结果序列，每个节点的可达距离和核心距离。我们以可达距离为纵轴，样本点输出次序为横轴进行可视化： 其中： X轴代表OPTICS算法处理点的顺序，Y轴代表可达距离。 簇在坐标轴中表述为山谷，并且山谷越深，簇越紧密 黄色代表的是噪声，它们不形成任何凹陷。 当你需要提取聚集的时候，参考Y轴和图像，自己设定一个阀值就可以提取聚集了。再来一张凹陷明显的： OPTICS的核心思想： 较稠密簇中的对象在簇排序中相互靠近 一个对象的最小可达距离给出了一个对象连接到一个稠密簇的最短路径 DPCA算法2014年6月，Alex Rodriguez和Alessandro Laio在ScienceScience上发表了一篇名为《Clustering by fast search and find of density peaks》的文章，提供了一种简洁而优美的聚类算法，是一种基于密度的聚类方法，可以识别各种形状的类簇，并且参数很容易确定。它克服了DBSCAN中不同类的密度差别大、邻域范围难以设定的问题，鲁棒性强。 在文章中提出的聚类方法DPCA算法（Desity Peaks Clustering Algorithm）基于这样一种假设：对于一个数据集，聚类中心被一些低局部密度的数据点包围，而且这些低局部密度点距离其他有高局部密度的点的距离都比较大。 一些概念： 局部密度$p_i$ ：即到对象i的距离小于$d_c$的对象个数。 高局部密度点距离（顾名思义，密度是特别局部的），其定义为： \delta_i=\min\limits_{j:p_j>p_i}d_{ij} 即在局部密度高于对象i的所有对象中，到对象i最近的距离。 而极端地，对于局部密度最大的那个对象(它没有比它更大的了)，我们设置$\delta=max(d_{ij})$，即它与离它最远的点的距离； 只有那些密度是局部或者全局最大的点（即稀疏的点）才会有远大于正常值的高局部密度点距离。 聚类过程这个聚类实例摘自作者的PPT讲演，在一个二维空间中对数据进行聚类，具体步骤如下： 1、首先计算每一个点的局部密度ρiρi，如图中，$ρ_1=7,ρ_8=5,ρ_{10}=4$ 2、然后对于每一个点i计算在局部密度高于对象i的所有对象中，到对象i最近的距离，即$ \delta_i $ 3、对每一个点，绘制出局部密度与高局部密度点距离的关系散点图 4、图上的异常点即为簇中心。如图所示，1和10两点的局部密度和高局部密度距离都很大，将其作为簇中心。 5、将其他的点分配给距离其最近的有着更高的局部密度的簇。 左图是所有点在二维空间的分布，右图是以ρρ为横坐标，以δδ为纵坐标绘制的决策图。容易发现，1和10两个点的$ρ_i$和$ \delta_i $都比较大，作为簇的中心点。26、27、28三个点的δδ也比较大，但是ρ比较小，所以是异常点。 簇中心的识别那些有着比较大的局部密度ρiρi和很大的高局部密度δiδi的点被认为是簇的中心； 而高局部密度距离δiδi较大但局部密度ρiρi较小的点是异常点； 确定簇中心之后，其他点按照距离已知簇的中心最近进行分类，也可以按照密度可达的方法进行分类。 但是，这里我们在确定聚类中心时，没有定量地分析，而是通过肉眼观察，包含很多的主观因素。因此，作者在文中给出了一种确定聚类中心个数的提醒：计算一个将ρ值和$ \delta$值综合考虑的量 \gamma_i=\rho_i\delta_i显然γ值越大，越有可能是聚类中心。因此，只需对其降序排列，然后从前往后截取若干个数据点作为聚类中心就可以了。 领域阈值$d_c$的选择：一种推荐做法是选择$d_c$，使得平均每个点的邻居数为所有点的1%~2%。 基于网格的聚类基于格子的参考这篇文章吧，感觉很少用啊，主要是STING统计信息网格算法和CLIQUE子空间聚类算法。 戳我 谱聚类谱聚类似乎也应用较广，这篇博客写的很清晰了 戳我]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>DBSCAN聚类</tag>
        <tag>STING聚类</tag>
        <tag>DPCA聚类</tag>
        <tag>谱聚类</tag>
        <tag>OPTICS聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（7）：聚类（1）]]></title>
    <url>%2F2018%2F11%2F12%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[下图简单地总结了一些聚类方法的简单划分 基于划分的聚类聚类分析最简单、最基本的版本是基于划分的聚类，它把对象组织成多个互斥的组或簇。为了使得问题说明简洁，我们假定簇个数作为背景知识给定，这个参数是划分方法的起点。 k-均值（k-mean） ：一种基于形心的技术k均值算法非常简单，它用簇$C_i$的形心代表该簇。每次确定K个类别中心，然后将各个结点归属到与之距离最近的中心点所在的Cluster，然后将类别中心更新为属于各Cluster的所有样本的均值，反复迭代，直至类别中心不再发生变化或变化小于某阈值。 下面给出该算法的伪代码 优点： 是解决聚类问题的一种经典算法，简单、快速 对处理大数据集，该算法保持可伸缩性和高效率 缺点： 必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同 只能发现球状Cluster，不适合于发现非凸形状的簇或者大小差别很大的簇 对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重 如何确定k类 https://www.cnblogs.com/yan2015/p/5239970.html k-中心点（k-mediods）：一种基于代表对象的技术k-均值算法对离群点敏感，因为这种对象远离大多数数据，因此分配到一个簇时，它们可能严重地扭曲簇的均值。因此，提出了k-中心点算法，它挑选实际对象代表簇，每个簇使用一个代表对象。这个代表对象选择的规则是：选择簇内一个点到其他点的距离之和的最小代价值，作为新的中心点。 K-中心聚类算法计算的是某点到其它所有点的距离之和最小的点，通过距离之和最短的计算方式可以减少某些孤立数据对聚类过程的影响 。下面给出算法的伪代码 基于层次的聚类尽管基于划分的聚类算法能够实现把数据集划分成指定数量的簇，但是在某些情况下，需要把数据集划分成不同层上的簇。层次聚类方法将数据组成层次结构或簇的”树” 基于层次的聚类算法（Hierarchical Clustering）可以是凝聚的（Agglomerative）或者分裂的（Divisive），取决于层次的划分是“自底向上”还是“自顶向下”。 自顶向下： 它把所有对象至于一个簇中开始，该簇是层次结构的根。然后，它把根上的簇划分为多个较小的子簇，并且递归地把这次簇划分成更小的簇，直到满足终止条件。常见的自顶向下的算法有K-means层次聚类算法。 自底向上：把数据集中的每个对象最为一个簇开始，迭代地把簇合并成为更大的簇，直到最终形成一个大簇，或者满足某个终止条件。基于自底向上算法有凝聚算法、BIRCH算法、CURE算法、变色龙算法等。 自顶向下算法Hierarchical K-means算法Hierarchical K-means算法是“自顶向下”的层次聚类算法，用到了基于划分的聚类算法那K-means，算法思路如下： 首先，把原始数据集放到一个簇C，这个簇形成了层次结构的最顶层； 使用K-means算法把簇C划分成指定的K个子簇$C_i，i=1,2,…,k$，形成一个新的层； 对于步骤2所生成的K个簇，递归使用K-means算法划分成更小的子簇，直到每个簇不能再划分（只包含一个数据对象）或者满足设定的终止条件。 如下图，展示了一组数据进行了二次K-means算法的过程 Hierarchical K-means算法一个很大的问题是，一旦两个点在最开始被划分到了不同的簇，即使这两个点距离很近，在后面的过程中也不会被聚类到一起。 对于以上的例子，红色椭圆框中的对象聚类成一个簇可能是更优的聚类结果，但是由于橙色对象和绿色对象在第一次K-means就被划分到不同的簇，之后也不再可能被聚类到同一个簇。 自底向上算法Agglomerative Clustering算法相比于Hierarchical K-means算法存在的问题，Agglomerative Clustering算法能够保证距离近的对象能够被聚类到一个簇中，该算法采用的“自底向上”聚类的思路。 算法思路，对于数据集$D，D=x_1,x_2,…,x_n$： 将数据集中的每个对象生成一个簇，得到簇列表$C，C=c_1,c_2,…,c_n$ a) 每个簇只包含一个数据对象：$c_i=x_i$； 重复如下步骤，直到C中只有一个簇： a) 从C中的簇中找到两个“距离”最近的两个簇：$\min D(ci,cj)$； b) 合并簇$c_i$和$c_j$，形成新的簇$c_{ij}； c) 从C中删除簇$c_i%和$c_j$，添加簇$c_{ij} 簇间距离计算在上面描述的算法中涉及到计算两个簇之间的距离，对于簇$C_1$和$C_2$，计算$minD(C1,C2)$，有以下几种计算方式： 最小距离： \min\ dist_{min}(C_i,C_j)=\min \mathop{\min} \limits_{p\in C_i,q\in C_j}\{|p-q|\} 两个簇之间最近的两个点的距离作为簇之间的距离，该方式的缺陷是受噪点影响大，容易产生长条状的簇。 最大距离： \min\ dist_{max}(C_i,C_j)=\min \mathop{\max} \limits_{p\in C_i,q\in C_j}\{|p-q|\} 两个簇之间最远的两个点的距离作为簇之间的距离，采用该距离计算方式得到的聚类比较紧凑。 均值距离： $m_i$和$m_j$为簇内所有点的均值坐标 \min\ dist_{mean}(C_i,C_j)=\min |m_i-m_j| 平均距离： \min\ dist_{avg}(C_i,C_j)=\min\ \frac{1}{n_in_j}\mathop{\sum}\limits_{p\in C_i,q\in C_j}\{|p-q|\}一个例子 Agglomerative聚类算法的优点是能够根据需要在不同的尺度上展示对应的聚类结果，缺点同Hierarchical K-means算法一样，一旦两个距离相近的点被划分到不同的簇，之后也不再可能被聚类到同一个簇，即无法撤销先前步骤的工作。另外，Agglomerative性能较低，并且因为聚类层次信息需要存储在内存中，内存消耗大，不适用于大量级的数据聚类，下面介绍一种针对大数据量级的聚类算法BIRCH。 BIRCH算法：使用聚类特征树BIRCH算法利用了一个树结构来帮助实现快速的聚类，这个数结构类似于平衡B+树，一般将它称之为聚类特征树(Clustering Feature Tree，简称CF Tree)。这颗树的每一个节点是由若干个聚类特征(Clustering Feature，简称CF)组成。从下图可以看看聚类特征树是什么样子的：每个节点包括叶子节点都有若干个CF，而内部节点的CF有指向孩子节点的指针，所有的叶子节点用一个双向链表链接起来。 在聚类特征树中，一个聚类特征CF是这样定义的：每一个CF是一个三元组，可以用（N，LS，SS）表示，其中N代表了这个CF中拥有的样本点的数量；LS代表了这个CF中拥有的样本点各特征维度的和向量，SS代表了这个CF中拥有的样本点各特征维度的平方和。 聚类特征本质上是定簇的统计汇总。使用聚类特征，我们可以很容易地推导出簇的许多有用的统计量，例如簇的形心$x_0$、半径R和直径D分别是 x_0=\frac{\sum_{i=1}^nx_i}{n}=\frac{LS}{n} R=\sqrt{\frac{\sum_{i=1}^n(x_i-x_0)^2}{n}} D=\sqrt{\frac{\sum_{i=1}^n\sum_{j=1}^n(x_i-x_j)^2}{n(n-1)}}=\sqrt{\frac{2nSS-2LS^2}{n(n-1)}}其中，R是成员对象到形心的平均距离，D是簇中逐对对象的平均距离。R和D都反映了形心周围簇的紧凑程度。 此外，聚类特征是可加的，也就是说，对于两个不相交的簇$C_1$和$C_2$，其聚类特征分别是$CF_1[n_1,LS_1,SS_1]$和$CF_2 [n_2,LS_2,SS_2]，合并后的簇的聚类特征是 CF_1+CF_2=[n_1+n_2,LS_1+LS_2,SS_1+SS_2]对于CF Tree，一般有几个重要参数，第一个参数是每个内部节点的最大CF数B，第二个参数是每个叶子节点的最大CF数L，第三个参数是针对叶子节点中某个CF中的样本点来说的，它是叶节点每个CF的最大样本半径阈值T，也就是说，在这个CF中的所有样本点一定要在半径小于T的一个超球体内。对于图中的CF Tree，限定了B=7， L=5， 也就是说内部节点最多有7个CF，而叶子节点最多有5个CF。 将所有的训练集样本建立了CF Tree，一个基本的BIRCH算法就完成了，对应的输出就是若干个CF节点，每个节点里的样本点就是一个聚类的簇。也就是说BIRCH算法的主要过程，就是建立CF Tree的过程。 聚类特征树CF Tree的生成下面看看怎么生成CF Tree。先定义好CF Tree的参数： 即内部节点的最大CF数B， 叶子节点的最大CF数L， 叶节点每个CF的最大样本半径阈值T。 开始时CF Tree是空的，没有任何样本，我们从训练集读入第一个样本点，将它放入一个新的CF三元组A，这个三元组的N=1，将这个新的CF放入根节点，此时的CF Tree如下图： 现在继续读入第二个样本点，发现这个样本点和第一个样本点A在半径为T的超球体范围内，即他们属于一个CF，将第二个点也加入CF A,此时需要更新A的三元组的值。此时A的三元组中N=2。此时的CF Tree如下图： 此时读取第三个节点，结果发现这个节点不能融入刚才前面的节点形成的超球体内，也就是说，需要一个新的CF三元组B来容纳这个新的值。此时根节点有两个CF三元组A和B，此时的CF Tree如下图： 当来到第四个样本点时，发现和B在半径小于T的超球体，这样更新后的CF Tree如下图： 那个什么时候CF Tree的节点需要分裂呢？假设现在的CF Tree 如下图， 叶子节点LN1有三个CF， LN2和LN3各有两个CF。叶子节点的最大CF数L=3。此时一个新的样本点来了，发现它离LN1节点最近，因此开始判断它是否在sc1,sc2,sc3这3个CF对应的超球体之内，但是很不幸，它不在，因此它需要建立一个新的CF，即sc8来容纳它。问题是我们的L=3，也就是说LN1的CF个数已经达到最大值了，不能再创建新的CF了，怎么办？此时就要将LN1叶子节点一分为二了。 将LN1里所有CF元组中，找到两个最远的CF做这两个新叶子节点的种子CF，然后将LN1节点里所有CF sc1, sc2, sc3，以及新样本点的新元组sc8划分到两个新的叶子节点上。将LN1节点划分后的CF Tree如下图： 如果内部节点的最大CF数B=3，则此时叶子节点一分为二会导致根节点的最大CF数超了，也就是说，根节点现在也要分裂，分裂的方法和叶子节点分裂一样，分裂后的CF Tree如下图： 有了上面这一系列的图，相信大家对于CF Tree的插入就没有什么问题了，总结下CF Tree的插入： 1、从根节点向下寻找和新样本距离最近的叶子节点和叶子节点里最近的CF节点（判断新节点与NLN1和NLN2谁近一些，然后继续往下） 2、如果新样本加入后，这个CF节点对应的超球体半径仍然满足小于阈值T，则更新路径上所有的CF三元组，插入结束。否则转入3. 3、如果当前叶子节点的CF节点个数小于阈值L，则创建一个新的CF节点，放入新样本，将新的CF节点放入这个叶子节点，更新路径上所有的CF三元组，插入结束。否则转入4。 4、将当前叶子节点划分为两个新叶子节点，选择旧叶子节点中所有CF元组里超球体距离最远的两个CF元组，分布作为两个新叶子节点的第一个CF节点。将其他元组和新样本元组按照距离远近原则放入对应的叶子节点。依次向上检查父节点是否也要分裂，如果需要按和叶子节点分裂方式相同。 当然，真实的BIRCH算法除了建立CF Tree来聚类，其实还有一些可选的算法步骤的，现在我们就来看看 BIRCH算法的流程。 1） 将所有的样本依次读入，在内存中建立一颗CF Tree, 建立的方法参考上一节。 2）（可选）将第一步建立的CF Tree进行筛选，去除一些异常CF节点，这些节点一般里面的样本点很少。对于一些超球体距离非常近的元组进行合并 3）（可选）利用其它的一些聚类算法比如K-Means对所有的CF元组进行聚类，得到一颗比较好的CF Tree.这一步的主要目的是消除由于样本读入顺序导致的不合理的树结构，以及一些由于节点CF个数限制导致的树结构分裂。 4）（可选）利用第三步生成的CF Tree的所有CF节点的质心，作为初始质心点，对所有的样本点按距离远近进行聚类。这样进一步减少了由于CF Tree的一些限制导致的聚类不合理的情况。 从上面可以看出，BIRCH算法的关键就是步骤1，也就是CF Tree的生成，其他步骤都是为了优化最后的聚类结果。 优点 1) 节约内存，所有的样本都在磁盘上，CF Tree仅仅存了CF节点和对应的指针。 2) 聚类速度快，只需要一遍扫描训练集就可以建立CF Tree，CF Tree的增删改都很快。 3) 可以识别噪音点，还可以对数据集进行初步分类的预处理 缺点 1) 由于CF Tree对每个节点的CF个数有限制，导致聚类的结果可能和真实的类别分布不同. 2) 对高维特征的数据聚类效果不好。此时可以选择Mini Batch K-Means 3) 如果数据集的分布簇不是类似于超球体，或者说不是凸的，则聚类效果不好。 CURE算法CURE（Clustering Using Representatives）是一种针对大型数据库的高效的聚类算法。基于划分的传统的聚类算法得到的是球状的，相等大小的聚类，对异常数据比较脆弱。CURE采用了用多个点代表一个簇的方法，可以较好的处理以上问题。并且在处理大数据量的时候采用了随机取样，分区的方法，来提高其效率，使得其可以高效的处理大量数据。 我们先看一下基于划分聚类算法的缺陷： 如上图所示，基于划分的聚类算法比如Hierarchical K-means聚类算法，不能够很好地区分尺寸差距大的簇，原因是K-means算法基于“质心”加一定“半径”对数据进行划分，导致最后聚类的簇近似“圆形”。 CURE算法核心的思想是使用一定数量的“分散的”点（scattered points）来代表一个簇（cluster），而不像是其他层次聚类算法中，只使用一个点，使得CURE算法有如下优势： 准确地识别任意形状的簇； 准确地识别尺寸差距大的簇； 很好地处理“噪点” 所以，CURE算法很好地解决了上面提到的聚类结果的缺陷，CURE算法主流程如下： Pass 1 1、从总数据中随机选取一个样本； 2、利用层次聚类算法把这个样本聚类，形成最初的簇$C_i,(i=1,2,…,k)$； 3、选取“代表点”（representative pionts）; ①对于每个簇，选取代表点（比如4个），这些点尽量分散; ②按照固定的比例α（比如20%），把每个样本点向簇的“质心”收缩，生成代表点 \{p'_{i1},p'_{i2},p'_{i3},p'_{i4}\}Pass 2 重新扫描所有的数据， 对于点p，找到距离p最近的簇，把它放到 “最近的簇”。简单来讲，是点p到簇$C_i$的距离为点p到簇$C_i$的四个“代表点 中最近的点之间的距离。 收缩系数α的取值不同，聚类结果也相应不同。当α趋于0时，所有的“代表点”都汇聚到质心，算法退化为基于“质心”的聚类；当α趋于1时，“代表点”完全没有收缩，算法退化为基于“全连接”的聚类，因此α值需要要根据数据特征灵活选取，才能得到更好的聚类结果 Chameleon变色龙算法：使用动态建模Chameleon（变色龙）是一种层次聚类算法，它采用动态建模来确定一对簇之间的相似度。在Chameleon中，簇的相似性依据以下两点评估：1）簇中对象的连接情况 ；2）簇的邻近性。也就是说，如果两个簇的互连性都很高并且它们之间又靠得很近就将其合并。 整体算法流程： 1、创建KNN图，每个节点将其最相似的k个节点用一条边连接起来； 2、使用最大流算法或者最小割算法，将kNN图分隔成小图； 也就是说簇C被划分为子簇CiCi和$C_j$，使得把C二分成$C_i$和$C_j$而被切断的边的权重之和最小。 3、将小簇进行和并，找对最大的度量值$RCRI^\alpha$的两个簇，合并条件是$RCRI^\alpha$大于某个阈值，否则结束合并。RC和RI的一个基本思想是，点之间的链接越多，这些点越可能连接成一个簇，C表示一个簇，是点的集合，|C|是集合的大小，即点的个数, $EC(A,B)$表示两个簇之间的边的数量。 相似互连度$RI(C_i,C_j)$ RI(C_i,C_j)=\frac{|EC_{\{C_i,C_j\}}|}{\frac{1}{2}(|EC_{C_i}|+|EC_{C_j}|)} 相对接近度$RC(C_i,C_j)$ RC(C_i,C_j)=\frac{\overline S_{EC_{\{C_i,C_j\}}}}{\frac{|C_i|}{|C_i|+|C_j|} \overline S_{EC_{C_j}}+ \frac{|C_i|}{|C_i|+|C_j|} \overline S_{EC_{C_j}} } 其中，$\overline S_{EC_{\{C_i,C_j\}}}$是连接$C_i$顶点和$C_j$顶点的边的平均权重，$\overline S_{EC_{C_i}}$或$\overline S_{EC_{C_j}}$是最小二分簇$C_i$（或$C_j$）的边的平均权重。]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>k均值</tag>
        <tag>k中心点</tag>
        <tag>层次聚类</tag>
        <tag>BIRCH聚类</tag>
        <tag>Chameleon变色龙聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（6）：高级分类方法]]></title>
    <url>%2F2018%2F11%2F09%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89%EF%BC%9A%E9%AB%98%E7%BA%A7%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[贝叶斯信念网络基本概念朴素贝叶斯分类有一个限制条件，就是特征属性必须有条件独立或基本独立（实际上在现实应用中几乎不可能做到完全独立）。当这个条件成立时，朴素贝叶斯分类法的准确率是最高的，但不幸的是，现实中各个特征属性间往往并不条件独立，而是具有较强的相关性，这样就限制了朴素贝叶斯分类的能力。 解决这个问题的一种算法叫贝叶斯网络（又称贝叶斯信念网络或信念网络）。 贝叶斯网络由两个成分定义：1）有向无环图（DAG）; 2)条件概率表的集合(Conditional Probability Table，CPT) 上图给出了一个布尔变量的简单贝叶斯信念网络，图中的弧可看做因果知识。换言之，一旦我们知道变量LungCanner的结果，那么变量FamilyHistory和Smoker就不再提供关于PostiveXRay的任何附近信息。 DAG中每一个节点表示一个随机变量，可以是可直接观测变量或隐藏变量，而有向边表示随机变量间的条件依赖；条件概率表中的每一个元素对应DAG中唯一的节点，存储此节点对于其所有直接前驱节点的联合条件概率。贝叶斯网络有一条极为重要的性质，就是我们断言每一个节点在其直接前驱节点的值制定后，这个节点条件独立于其所有非直接前驱前辈节点。这条特性的重要意义在于明确了贝叶斯网络可以方便计算联合概率分布。 一般情况先，多变量非独立联合条件概率分布有如下求取公式： P(x_1,x_2,..,x_n)=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1,x_2,...,x_{n-1}) 而在贝叶斯网络中，由于存在前述性质，任意随机变量组合的联合条件概率分布被化简成 P(x_1,x_2,...,x_n)=\prod_{i=1}^nP(x_i|Parents(x_i)) 其中，$P(x_1,…,x_n)$是X的值的特定组合的概率，Parents表示xi的直接前驱节点的联合 ，而$P(x_i|Parents(x_i))$的值对应于CPT概率表的值。 上图是一个有向无环图（DAG） ，不过仅有这个图的话，只能定性给出随机变量间的关系，如果要定量，还需要一些数据，这些数据就是每个节点对其直接前驱节点的条件概率（也就是CPT表的概率），而没有前驱节点的节点则使用先验概率表示。 没有前驱的节点用先验概率表示；以及CPT条件概率，例如P(H=0|R=0)=0.9（真实账号为假，头像也为假的概率） 有了这些数据，不但能顺向推断，还能通过贝叶斯定理进行逆向推断。例如，现随机抽取一个账户，已知其头像为假，求其账号也为假的概率： 如果给出所有节点的条件概率表，则可以在观察值不完备的情况下对任意随机变量进行统计推断。上述方法就是使用了贝叶斯网络。 训练贝叶斯网络构造与训练贝叶斯网络分为以下两步： 1、确定随机变量间的拓扑关系，形成DAG。这一步通常需要领域专家完成，而想要建立一个好的拓扑结构，通常需要不断迭代和改进才可以。 2、训练贝叶斯网络。如果不训练的，我们只能知道定性的网络，而不能定量。实际上这一步也就是要完成条件概率表(CPT表)的构造，如果每个随机变量的值都是可以直接观察的，像我们上面的例子，那么这一步的训练是直观的，方法类似于朴素贝叶斯分类。但是通常贝叶斯网络的中存在隐藏变量节点，那么训练方法就是比较复杂，例如使用梯度下降法。 性能如何贝叶斯网络已经广泛于临床，生物，征信等领域。其强大之处在于两点 1.贝叶斯网络最强大之处在于从每个阶段结果所获得的概率都是数学与科学的反映，换句话说，假设我们了解了足够多的信息，根据这些信息获继而得统计知识，网络就会告诉我们合理的推断。 2.贝叶斯网络最很容易扩展(或减少,简化),以适应不断变化的需求和变化的知识。 使用频繁模式分类关联分类回顾一下之前的关联规则，显示了规则的置信度和支持度 age=youth \land credit=ok \Rightarrow buys\_computer=yes \\ [support =20\%，confidence=93\%]从分类角度，置信度类似于规则的准准确度。例如，93%的置信度意味着D中身为年轻人并且信誉度为OK的顾客中，93%属于类buysconputer=yes。支持度20%意味着D中20%的顾客是青年，信誉为OK，并且属于类buyscomputer=yes 一般而言，关联规则的分类包括以下步骤： 1、挖掘数据，找出频繁项集，即找出数据经常出现的属性-值对 2、分析频繁项集，产生每个类的关联规则，它们满足置信度和支持度标准 3、组织规则，形成基于规则的分类器 这里，我们考察以下三种分类方法1）CBA ; 2) CMAR ; 3)CPAR CBA最早、最简单的关联分类算法是CBA。CBA使用迭代的方法挖掘频繁项集，类似于Apriori算法。CBA使用了一种启发式方法构造分类器，其中规则按照它们的置信度和支持度递减优先级排序，如果当中一组规则具有相同的前件，则选取具有最高置信度的规则代表该集合。在对新元组分类是，使用满足该元组第一个规则对它进行分类。分类器还包含一个默认规则，具有最低优先级。 CMARCMAR和CBA在频繁项集挖掘和构建分类器都不同，CMAR采用FP-growth算法的变形来发现满足最小支持的最小置信度的规则完全集。构造分类器时，如果新元组X只匹配一个规则，则简单地把规则的类标号给这个元组。如果多个规则满足X，把这些规则形成一个集合S。CBA将集合S中最大置信度的规则的类标号指派给X，而CMAR考虑多个规则。它根据S的类标号将规则分类，不同组中的规则具有不同的类标号，然后CMAR使用X2X2卡方度量，根据组中规则的统计相关联找出相关性“最强的”规则组，再把该类标号指派个X元组。这样，它就考虑了多个规则，不是像CBA一样只考虑一个规则。CMAR在准确率和复杂的都比CBA更有效一点。 CPARCPAR和CMAR相差不多，它通过FOIL算法而不是FP-growth来挖掘规则。同样也将集合S的规则按类分组。然而，CPAR根据期望准确率，使用每组中最好的k个规则预测X元组的类标号，通过考虑组中最好的k个规则而不是所有规则。在大数据集上，CPAR和CMAR准确率差不多，但产生的规则要比CMAR少的多。 基于有区别力的频繁模式分类如果我们把所有频繁模式都添加到特征空间，可能许多模式是冗余，还可能因特征太多而过分拟合，导致准确率降低。因此，一种好的做法是使用特征选择，删除区别能力较弱的特征，其一般框架步骤(两步法)如下： 特征产生：频繁模式的集合F形成候选特征 特征选择： 通过信息增等度量对F进行特征选择，得到选择后的频繁模式FsFs，数据集DD变换成D′D′ 学习分类模型：在数据集D′D′建立分类器 为了提高两步法的效率，考虑将步骤1和步骤2合并为一步。即有可能只挖掘具有高度区别能力的频繁模式的集合，而不是完全集。DDPMine算法采用这种方法，它首先把训练数据变换到一个称频繁模式树或FP树的紧凑树结构，然后再该树种搜索有区别能力的模式。 在准确率和效率两个方面，DDPMine都优于最先进的关联分类方法。 k-近邻分类对于，k-近邻分类算法，位置元组每次都被指派到它的k个最近邻（距离度量）的多数类。]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>贝叶斯信念网络</tag>
        <tag>关联分类</tag>
        <tag>k近邻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（5）：分类]]></title>
    <url>%2F2018%2F11%2F06%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%EF%BC%9A%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[这一章很多概念都是之前就接触了，就不一一记录了，这里记录一些感兴趣的吧。 决策树剪枝为了避免决策树过拟合数据，一般要对决策树进行剪枝：先剪枝和后剪枝 在先剪枝方法中，通过提前停止树的构造（例如，通过决定在给定的结点上不再分裂或划分训练样本的子集）而对树“剪枝”。在构造树时，统计意义下的度量，如信息增益、基尼指数等，可以用于评估分裂的优劣。如果在一个结点划分样本将导致低于预定义阈值的分裂，则给定子集的进一步划分将停止。然而，选取一个适当的阈值是困难的。较高的阈值可能导致过分简化的树，而较低的阈值可能使得树的化简太少。 第二种更常用的方法是后剪枝，它由“完全生长”的树之后再剪去分枝，通过用叶子节点替换要删除的分枝。CART使用的代价复杂度剪枝算法是后剪枝方法的一个实例。该方法把树的复杂度看做树叶节点的个数和树的错误率的函数，如果减去节点N的子树导致较小的代价复杂度，则剪掉该子树；否则，保留该子树。 可伸缩性与决策树已有的决策树算法，如ID3、C4.5和CART都是为相对较小的数据集规模。另外，大部分情况下，大规模的训练数据不能放在内存！因此，由于训练元组在主存和高速缓存换进换出，决策树的构造可能变得效率低下。最近，已经提出了一些可以解决伸缩问题的决策树算法，例如，RainForest（雨林）能适应可用的内存量，采用了一种新的数据结构形式。 转为AVC集的聚集信息的数据结构来存放 另外一种方法是采用树构造的自助乐观算法（BOAT），它采用了统计学计数，创建给定训练数据的一些较小的样本（或子集），其中每个子集都能放在内存中。使用每个子集构造一颗树，导致多棵树，并使用它们构造一个新树。 使用IF-THEN规则分类基于规则的分类器使用一组IF-THEN规则进行分类。一个IF-THEN的规则R1一般表示形式有如下两种 123R1:IF age=youth AND student=yes THEN buys_conmputer = yes R1:IF age=youth ^ student=yes THEN buys_computer = yes 规则R可以用覆盖率和准确率来评估。给定类标记的数据集D中的一个元组X，设$n_{covers}$为规则R覆盖的元组数，$n_{covers}$为R正确分类的元组数，|D|是D中的总元组数，可将R的覆盖率和准确率定义为： coverage(R)=\frac{n_{cover}}{|D|} \\ accuracy(R)=\frac{n_{correct}}{n_{covers}}如何建立基于规则的分类器呢 1、根据决策树提取规则 对从根到树叶节点的每条路经创建一个规则。沿着给定路经上的每个属性-值的逻辑AND形成规则前件（“IF”部分）。叶结点包含类预测，形成规则后件（“THEN”部分）。由于这些规则都是直接从书中提取的，所以它们是互斥的和穷举的（互斥意味不可能存在规则冲突），因此规则的序不重要——它们是无序的。 由于每个树叶对应一个规则，所以提取的规则集的量也很多。所以有两种解决方法，第一种是先对决策树剪枝，然后提取规则。另外一种是直接提取规则，然后修剪规则，对于不能提高规则的估计准确率的任何条件都可以删减，从而泛化该规则。 2、使用顺序覆盖算法 顺序覆盖算法是最广泛使用的挖掘分类规则取集的方法，有许多流行的顺序覆盖算法，包括AQ、CN2和最近提出的RIPPER。算法的一般策略如下：一次学习一个规则，每学习一个规则，就删除该规则覆盖的元组，并在剩下的元组上重负该过程。 从最一般的规则开始，即从规则前件条件为空的规则开始。该规则是： 1IF THEN loan_decision = accept 然后，我们考虑每个可以添加到该规则中可能属性测试。Learn_One_Rule采用一种贪心策略，每次选择最能提高规则质量的属性。目前，我们使用规则的准确率作为质量度量。假设Learn_One_Rule发现属性测试income=high最大限度地提高了当前（空）规则的准确率。把它添加到条件中，当前规则变成 1IF income=high THEN loan_decision = accept 下一次迭代时，再次考虑可能的属性测试，结果选中credit_rating=excellent，当前规则增长，变成 1IF income=high AND credit_rating=excellent THEN loan_decision = accept 重复该过程，直到结果规则达到可接受的质量水平。另外，贪心策略如果不自觉选到一个很差的属性怎么办，为了减少这种发生的几率，可以选出最好的k个而不是最好的一个属性添加到当前规则。 规则质量的度量Learn_One_Rule需要度量规则的质量，之前我们用的是准确率。但准确率本身并非规则质量的可靠估计。这里介绍几个相对有用的几种度量：1）、熵 ；2）、信息增益；3）考虑覆盖率的统计检验 我们想知道给定属性测试到condition中是否导致更好的规则，我们称新的条件为condition’，换言之，我们想知道R’是否比R好。 熵：D是condition’覆盖元组集合，而$p_i$是D中$C_i$类的概率。熵越小，condition’越好。熵更偏向于覆盖单个类大量元组和少量其他类元组的条件。 信息增益：FOIL算法是一种学习一阶逻辑规则的顺序覆盖算法，FOIL用下式估计扩展condition’s而获得信息 FOIL\_Gain=pos'\times (log_2\frac{pos'}{pos'+neg'}-log_2\frac{pos}{pos+neg})它偏向于具有高准确率并且覆盖许多正元组的规则 似然率统计量 Likelihood\_Ratio=2\sum_{i=1}^mf_ilog(\frac{f_i}{e_i})其中，m是类数，$f_i$是这些元组类i的观测概率，$e_i$是规则随机预测时类i的期望频率。似然率有助于识别具有显著覆盖率的规则。 CN2使用熵和似然率检验，而FOIL的信息增益被RIPPER使用。 规则剪枝之前说了可以在决策树生成之后对规则剪枝，有很多剪枝策略。这里介绍FOIL使用的一种简单但很有效的方法，给定规则R，有： FOIL\_Prune(R)=\frac{pos-neg}{pos+neg}其中，pos和neg分别为规则R覆盖的正元组数和负元组数。这个值将随着R在剪枝集上的准确率增加而增加。因此，如果R剪枝后版本的FOIL_Prune值较高，则对R剪枝。 如何使用规则分类器来预测元组类标号呢？如果正常的话，R1是唯一满足的规则，则该规则激活，返回X的类预测。但如果有多个规则被触发，它们指定了不同的类，这时则需要一种解决冲突的策略来决定激活哪一个规则。我们考察两种，即规模序和规则序： 规模序：方案吧最高优先权赋予给”最苛刻”要求的规则，其中苛刻性用规则前件的规模度量（类似于树的深度） 规则序：这种序可以是基于类的或基于规则的。使用基于类的序，类按”重要性”递减排序，如按普遍性的降序排序；基于规则的序，或者根据领域专家的建议，把规则组织成一个优先权列表。 使用统计显著性检验选择模型在前面我们已经使用了 一些策略来测算分类器的准确率（例如K折交叉验证）。在这里，我们假设经处理，最后生成了两个分类器，他们的评估度量都不相同，那么我们应该选择哪个分类器呢？ 直观的看法当然是选择指标好的那个分类器呀，但是 实际上这种差别很有可能是偶然的。我们为了判定这种差别是否是偶然的，还需要进行统计显著性检验。 此外，希望得到平均错误率的置信界，使得我们可以做出这样的陈述：”对于未来样本的95%，观测到的均值将不会偏离正、负两个标准差”或者”一个模型比另外一个模型好，误差幅度为±4±4” 这里用的是显著性检验是t-检验。知乎上给出了相关的解释 ： 知乎t检验解释 https://www.zhihu.com/question/60321751/answer/399954823 对于10-折交叉验证（k=10）的第ii轮，设$err(M_1)_i$(或$err(M_2)_i$)是模型$M_1$(或$M_2$)在第i轮的错误率。对$M_1$的错误率取平均值得到$M_1$的平均错误率，记为$\overline {err}(M_1)$，类似的，可以得到$\overline {err}(M_2)$。两个模型差的方差记为$var(M_1-M_2)$。在我们的例子中，k=10，这里的k个样本是从每个模型的10-折交叉验证得到的错误率。逐对比较t-统计量按下式计算： t=\frac{\overline{err}(M_1)-\overline{err}(M_2)}{\sqrt{var(M_1-M_2)/k}} 其中 var(M_1-M_2)=\frac{1}{k}\sum_{i=1}^k[err(M_1)_i-err(M_2)_i-(\overline{err}(M_1)-\overline{err}(M_2))]^2为了计算$M_1$和$M_2$是否显著不同，计算t并选择显著水平sig。实践中，通常使用5%或1%的显著水平。然后，查找t-分布表。通常该表以自由度为行（k个样本具有k-1个自由度，对于我们的例子，自由度为9），显著水平为列。假定要确定$M_1$和$M_2$之间的差对总体的95%（即sig=5%或0.05）是否显著不同。然而，由于t分布是对称的，通常只显示分布上部的百分点，因此，找z=sig/2=0.025的表值，其中z也称为置信界。如果t&gt;z或t&lt;-z，则t落在拒斥域，在分布的尾部。这意味着可以拒绝$M_1$和$M_2$的均值相同的原假设，并断言两个模型之间存在统计的显著的差别。否则，如果不能拒绝原假设，于是断言$M_1$和$M_2$之间的差可能是随机的。 如果有两个检验集而不是单个检验集，则使用t-检验的非逐对版本，其中两个模型的均值之间的方差估计为： var(M_1-M_2)=\sqrt{\frac{var(M_1)}{k_1}+\frac{var(M_2)}{k_2}} 其中，k1和k2分别用于M1和M2的交叉验证样本数（在我们的情况下，10-折交叉验证的轮）。这也称为两个样本的t检验。在查t分布表时，自由度取两个模型的最小自由度。]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>决策树剪枝</tag>
        <tag>IF-THEN规则分类</tag>
        <tag>t检验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（4）：高级模式挖掘]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89%EF%BC%9A%E9%AB%98%E7%BA%A7%E6%A8%A1%E5%BC%8F%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[于大量的研究、问题的多方面扩展和广泛的应用研究。频繁模式挖掘已经远远超过了事务数据。这里，我们介绍其他多种挖掘模式类型的方法，包括多层模式、多维模式、稀有模式、负模式、受约束的频繁模式和巨型模式挖掘。书上的内容都是比较浅显，更多的是介绍性的东西，可能需要到实际工作上才能理解更深一些。 挖掘多层关联规则对于许多应用，在较高的抽象层（抽象的大类，例如电脑，而不是具体某种品牌和型号的电脑）发现的强关联规则，可能有很高的支持度，但可能是常识性知识。我们希望往下钻，在更细节的层次发现新颖的模式。另外一方面，在较低或原始抽象层，可能有太多的零散模式，其中一些只不过是较高层模式的平方特化。 在这种较低层或原始层数据中很难发现有趣的规则模式，例如，“Dell Studio XPS 16 Notebook”和“Logitech VX Nano Cordless Laser Mouse”每个都在很少一部分事务中出现，则可能很难找到涉及它们的强关联规则。少数人同时购买它们，使得该商品集不太可能满足最小支持度。然而，我们预料，在这些商品的泛化抽象之间，如在”Dell Notebook” 和”Cordless Mouse”之间，可望更容易发现强规则。这种在多个抽象层的数据上挖掘产生的规则称为多层关联规则，一般采用如下自顶向下的方法： 对于所有层使用一致的支持度（称作一致支持度）：在每一层挖掘时，使用相同的最小支持度阈值 然而，一致支持度方法有一些困难。较低层次抽象的项不大可能象较高层次抽象的项出现得那么频繁。如果最小支持度阈值设置太高，可能丢掉出现在较低抽象层中有意义的关联规则。如果阈值设置太低，可能会产生出现在较高抽象层的无兴趣的关联规则。这导致了下面的方法： 在较低层使用递减的支持度（称作递减支持度）：每个抽象层有它自己的最小支持度阈值。抽象层越低，对应的阈值越小。 使用基于项或基于分组的最小支持度：例如，用户可以根据产品价格或者根据感兴趣的商品设置最小支持度，对如”价格超过1000美元的照相机”或”平板电脑” 设置特别低的支持度，以便特别关注这类商品的关联模式 为了从具有不同支持度阈值的组中挖掘混合项模式，通常在挖掘中取所有组的最低支持度阈值。这将避免过滤掉有价值的模式，该模式包含来自具有最低支持度阈值组的项。同时，每组的最小支持阈值应该保持，以免从每个组产生无趣的项集。 检查多层关联规则冗余性挖掘多层关联规则一个严重的副作用是，由于项之间的“祖先”关系，可能产生一些多个抽象层上的冗余的规则，例如：”desktop computer”是”IBM desktop computer”的祖先，有以下规则： 如果后一个具有较小一般性的规则不提供新的信息，应当删除它。让我们看看如何来确定。规则 R1 是规则 R2 的祖先，如果R1能够通过将R2中的项，用它在概念分层（分配比率）中的祖先替换得到，则可以将R2删除。 以上诉规则例子：假定规则(6.9)具有 70%置信度，8%支持度，并且大约四分之一的”desktop computer”销售是”IBM desktop computer”。可以期望规则(6.10)具有大约 70%的置信度（由于所有的”IBM desktop computer”样本也是” desktop computer”样本）和 2%（即，8%×1/4）的支持度。也就是说，根据实际销量的分层可以通过R1推到出R2的规则与规则(6.10)相差无几，则R2规则是冗余的。 挖掘多维关联规则本节，你将学习挖掘多维关联规则的方法。多维关联规则是涉及多个属性或谓词的规则（例如，关于顾客的 buys 和顾客的 age 的规则）。我们把规则中每个不同的谓词称作维。例如： age(X, "20...29") \land buys(X, "laptop")⇒buys(X, "HP\ printer")其中，数据库属性可能是分类属性和量化属性（数值），对于量化属性，挖掘多维相关规则的计数可以分为两种基本方法： 第一种方法：使用预定义的概念分层对量化属性离散化，例如，income 的概念分层可以用于以区间值，如“0…20K”代替 第二种方法：根据数据的分布，将量化属性离散化或聚类到“箱” 正如前面讨论的，我们可以把量化属性离散化为多个区间，而后在关联挖掘时把它们看做是分类属性。然而，这种简单的离散化可能导致产生大量的规则，其中许多规则可能没什么用。这里我们介绍三种方法，帮助克服这一困难，以便发现新颖的关联关系： 1、数据立方体方法 2、基于聚类的方法 3、揭示异常行为的统计学方法 挖掘稀有模式和负模式迄今为止，介绍的都是挖掘频繁模式，然而，有时令人感兴趣的不是频繁模式，而是发现稀有的模式（例如钻石表的销售是稀有的），或发现反映项之间的负相关的模式（例如发现顾客频繁地购买经典可口可乐或无糖可乐，但不可能一起都买）。 稀有模式：是指其支持度低于（或远低于）用户指定的最小支持度阈值的模式。然而，由于大多数项集的出现频度通常都低于甚至远低于最小支持度阈值，因此实践中允许用户指定稀有模式的其他条件是可取的。 负相关模式：如果项集X和Y 都是频繁的，但很少一起出现$(sup(X \cup Y) &lt; sup(X) \times sup(Y))$ ，则项集X和Y是负相关的，并且$X\cup Y$ 是负相关模式.如果$(sup(X \cup Y) \ll sup(X) \times sup(Y))$， 则项集X和Y是强负相关的，并且$X\cup Y$是强负相关模式。 然而，上面这个公式度量不是零不变的，只能有效地解决非零事务的数据。如果数据库存在大量的零事务，则应该使用零不变度量Kulczynski，下面给出具体定义： 零不变负相关模式：假设项集X和Y都是频繁的，即$sup(X)\geq min_sup$ ，$sup(Y)\geq min_sup$ ， 其中$min_sup$是最小支持度阈值。如果有$(P(X|Y)+P(Y|X))/2&lt;\epsilon$，其中$\epsilon$是负模式阈值，则$X\cup Y$是负相关模式。 基于约束的频繁模式挖掘作为限制搜索空间的约束条件，这种策略称为基于约束的挖掘。 元规则就是挖掘用户感兴趣的规则的语法形式，例如： P_1(X,Y)\land P_2(X,W) \Rightarrow buys(X, "officesoftware")其中，P1和P2是谓词变量，在挖掘过程中被例示为给定数据库的属性；X是变量，代表顾客；Y和W是分别赋给P1和P2的属性值。 对于规则约束，如何使用规则约束对搜索空间进行剪枝？主要有两种方法：1、对模式空间剪枝；2、数据空间剪枝 对模式空间剪枝根据约束如何与模式挖掘过程配合，模式剪枝约束可以分为五类：1）反单调的；2）单调的；3）简洁的；4）可转变的；5）不可转变的（这个不重要） 反单调的：规则约束$”sum(I.price)\leq 100”$，如果一个候选项集中的商品价格和大于 100 美元，则该项集可以由搜索空间剪去，因为向该项集中进一步添加项将会使它更贵，因此不可能满足限制。换一句话说，如果一个项集不满足该规则限制，它的任何超集也不可能满足该规则限制。如果一个规则具有这一性质，则称它是反单调的。 单调的：规则约束$”sum(I.price)\geq 100”$，集合中的单价和大于 100，进一步添加更多的项到此项集将增加价格，并且总是满足该限制。因此，在项集 I 上进一步检查该限制是多余的。换言之，如果一个项集满足这个规则限制，它的所有超集也满足。如果一个规则具有这一性质，则称它是单调的。 简洁：对于这类约束，我们可以枚举并且仅仅列出所有确保满足该限制的集合。因为有一个精确“公式”，产生满足简洁限制的所有集合，在挖掘过程中不必迭代地检验规则限制 可转变的约束：有些限制不属于以上三类。然而，如果项集中的项以特定的次序排列，则对于频繁项集挖掘过程，限制可能成为单调的或反单调的。例如，限制“avg(I.price)”既不是反单调的，也不是单调的。然而，如果事务中的项以单价的递增序添加到项集中，则该限制就成了反单调的。类似的，如果是递减顺序添加则是单调的。 对数据空间剪枝第二种对基于约束的频繁模式挖掘的搜索空间进行剪枝的方法是对数据空间剪枝。这种策略是剪掉对其后挖掘过程中可满足模式的产生没有贡献的数据片段。例如，对于约束是数据简洁的，如果一个挖掘查询要求被挖掘模式必须包含数码相机，则可以在挖掘过程开始减剪掉所有不包含数码相机的事务。对于约束的反单调的，基于当前模式，如果一个数据项不满足数据反单调约束，则可以剪掉它，因为在剩下的挖掘过程中，它不能对当前模式的超模式的产生有任何贡献。 巨型模式对于数据库有数百或者数千维的数据，用已介绍方法来挖掘高维数据是非常低效的，一种是使用垂直格式数据，之前已经介绍过了。另外一种新的方向是用模型融合，用于巨型模式，即非常长的模式(例如蛋白质的DNA长序列)。这种方法在模式搜索空间中跳跃，得到巨型频繁模式完全集的一个很好的近似解。 对于Apriori和FP-growth算法，会不可避免产生大量中型模式，使得它不可能达到巨型模式。因此提出了模式融合，它融合了少量较短的频繁模式，形成巨型模式候选。 书上提到了核模式的概念，这里没怎么看懂，觉得是核模式代表了一定的鲁棒性。但是书上也直接给出了推论，巨型模式比较短模式有更多的核模式，更鲁棒。也就是说，如果从该模式中去掉少量项，则结果模式会有类似的支集。巨型模式较低层的核模式叫做核后代。所以基于这个特性，模式融合可以融合少量较短的频繁模式。这也是它为什么被称为模式融合的原因 ，因此巨型模式可以通过合并其核模式的真子集产生，例如， abcef可以通过它的两个核模式ab和cef产生。而不需要用单个项增量地扩展，而是与池中多个模式进行凝聚，这样能够迅速地到达巨型模式。 模型融合包括以下两个阶段： 1、池初始化 模式融合假定有一个短频繁模式的初始池。这是一个短长度的频繁模式挖掘集。这个初始值可以用任意已有的有效挖掘算法挖掘。 2、迭代的模式融合 用户首先指定一个参数K的值（K代表挖掘模式的最大个数），然后从当前池中随机地选取K个种子，对于每个种子，我们找出直径为ττ的球内的所有模式。然后，每个”球“中的所有模式融合在一起，形成一个超模式集，这些超模式形成新的池，然后再从这个新的池子中随机地找到K个种子，然后重复上面的工作，一直迭代，直到不能融合为止 因此，该方法可以绕过中型模式，通往巨型模式。 7.5/7.6的内容暂时不是特别重要，用到再补。]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>多层挖掘模式</tag>
        <tag>多维模式挖掘</tag>
        <tag>稀有模式和负模式挖掘</tag>
        <tag>基于约束模式挖掘</tag>
        <tag>巨型模式挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（3）：挖掘频繁模式、关联和相关性]]></title>
    <url>%2F2018%2F11%2F02%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89%EF%BC%9A%E6%8C%96%E6%8E%98%E9%A2%91%E7%B9%81%E6%A8%A1%E5%BC%8F%E3%80%81%E5%85%B3%E8%81%94%E5%92%8C%E7%9B%B8%E5%85%B3%E6%80%A7%2F</url>
    <content type="text"><![CDATA[基本概念关联规则关联规则可以用以下表示： computer\Rightarrow antivirus\_software[support=2\%;confidence=60\%]规则的支持度和置信度是两个规则兴趣度度量，它们分别反映发现规则的有用性和确定性。上诉关联规则的支持度（表示同时包含A和B的事务占所有事物的比例）为2%，意味所分析的事务的2%显示 计算机和杀毒软件被同时购买。置信度（表示包含A的事务同时也包含B的比例） 60%意味购买计算机的顾客 60%的几率也购买财务管理软件。一般如果关联规则满足最小支持度阈值和最小置信度阈值，则认为关联规则是有趣的，是值得关注的现象。 频繁项集、闭项集、极大项集设任务相关的数据 D 是数据库事务的集合，每一个事务有一个标识符，称作 TID。设A、B是两个项集，有： support(A\Rightarrow B)=P(A\cup B) \\ confidence(A\Rightarrow B)=P(B\vert A)同时满足最小支持度阈值(min_sup)和最小置信度阈值(min_conf)的规则称作强规则 项的集合称为项集。包含 k 个项的项集称为 k-项集。项集的出现频率是包含项集的事务数，简称为项集的频率、支持计数或计数。如果项集满足最小支持度，则称它为频繁项集。频繁 k -项集的集合通常记作 LkLk。一般而言，关联规则的挖掘是一个两步的过程： 1、找出所有频繁项集：根据定义，这些项集出现的频繁性至少和预定义的最小支持计数一样。 2、由频繁项集产生强关联规则：根据定义，这些规则必须满足最小支持度和最小置信度。 从大型数据集中挖掘项集的主要挑战是，这种挖掘常常产生大量满足最小支持度（min_sup）阈值的项集，当min_sup设置的很低的时候尤其如此，这是因为如果一个项集的频繁的（项集每个项计数都满足最小支持度），则它的每个子集都是频繁的。因此，得到的频繁项集的总个数太大了，为了更好的计算和存储，引入了闭频繁项集和极大频繁项集的概念。 闭频繁项集：指这个项集X既是频繁项集又是闭项集，闭项集是指不存在真超项集Y和此项集X具有相同的支持度计数 极大频繁项集：指这个项集X既是频繁项集又是极大项集，极大项集是指不存在频繁的真超项集Y，它已经是最大规模频繁项集了。 一个举例：(AB项集为非闭是因为和ABC项集具有相同的支持度计数，ABC为非极大是存在频繁项集ABCD) 挖掘频繁项集的方法Apriori 算法算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。Apriori 使用一种称作逐层搜索的迭代方法，k项集用于探索(k+1)项集。首先，找出频繁 1-项集的集合。该集合记作 L1。L1用于找频繁 2-项集的集合 L2，而L2用于找L3，如此下去，直到不能找到频繁 k-项集. 所以关键在于须看看如何用频繁项集$L_k$找到频繁项集$L_{k+1}$.具体是由以下两步组成： 1、连接步：为找 $L_k$，通过$L_{k - 1}$与自己连接产生候选 k-项集的集合。假定事务或项集中的项按字典次序排序，如果$L_{k - 1}$它们前(k-2)个项相同的，则可以执行连接操作。 2、剪枝步：连接操作得到$C_k$，$C_k$是 $L_k$的超集，可以通过扫描数据库计算支持度从而在$C_k$里确定$L_k$。然而，$C_k$可能很大，这样所涉及的计算量就很大。为压缩 $C_k$，可以用以下办法使用 Apriori 先验性质：任何非频繁的(k-1)项集都不是可能是频繁 k项集的子集(频繁项集的子集一定是频繁的)。因此，如果一个候选 k项集的(k-1)子集不在 $L_{k - 1}$中 ($L_{k - 1}$包含所有频繁的k-1项集，若某个k-1项集不在里面则是不频繁的)，则该候选也不可能是频繁的，从而可以由$C_k$中删除。 一个例子: aproori算法过程：假设最小支持度计数为2，即min_sup=2 其中，$L_2$连接步寻找$L_3$，要将$L_2$的前（3-2）个相同的项连接起来，得到\{\{I1,I2,I3\}, \{I1,I2,I5\}, \{I1,I3,I5\}, \{I2,I3,I4\}, \{I2,I3,I5\}, \{I2,I4,I5\}\}，然后执行剪枝步，扫描整个数据库，可以得到剩下的\{\{1,I2,I3\}, \{I1,I2,I5\}\}。或者使用Apriori 先验性质：举\{I2,I4,I5\}项集为例，\{I2,I4,I5\}的2项子集为\{I2,I4\}, \{I2,I5\} 和 \{I4,I5\}。但\{I4, I5\}不是$L_2$的元素，因此不是频繁的。同理L3L3连接步得到\{I1, I2, I3, I5\}的其中一个3项集\{I2,I3,I5\}不是$L_3$的元素，因此\{I1, I2, I3, I5\}也不是频繁的。 由频繁项集产生关联规则一旦由数据库 D 中的事务找出频繁项集，由它们产生强关联规则是直接了当的（强关联规则满足最小支持度和最小置信度）。对于置信度，可以用下式，其中条件概率用项集支持度计数表示 confidence(A\Rightarrow B)=P(A\vert B)=\frac{support\_count(A\cup B)}{support\_count(A)} FP-growth算法正如我们已经看到的，在许多情况下，Apriori 的候选产生-检查方法大幅度压缩了候选项集的大小，并导致很好的性能。然而，它可能受两种超高开销的影响： 它可能需要产生大量候选项集。例如，如果有 10^4个频繁 1-项集，则 Apriori 算法需要产生多达 10^7个候选 2-项集 它可能需要重复地扫描数据库 “ 可以设计一种方法，挖掘全部频繁项集，而不产生候选吗？”一种有趣的方法称作频繁模式增长，或简单地，FP-增长，它采取如下分治策略：将提供频繁项集的数据库压缩到一棵频繁模式树（或 FP-树），但仍保留项集关联信息；然后，将这种压缩后的数据库分成一组条件数据库（一种特殊类型的投影数据库），每个关联一个频繁项，并分别挖掘每个数据库。 FP-growth算法的优点是采用了高级的数据结构。那么这种高级的数据结构是什么呢？实际上就是FP树。 FP树是一种输入数据的压缩表示。他通过把事务映射到FP树上来构造一条路径。这样如果不同事务之间的重叠路径越多，那么就有理由认为他们是频繁项集。由于不同的事务可能会有若干个相同的项，因此它们的路径相互重叠越多，则使用FP树结构获得的压缩效果越好。 FP-growth算法的基本过程1）构建FP数。 2）从FP树中挖掘频繁项集 依然是用之前那个例子： 数据库的第一次扫描与 Apriori 相同，它导出频繁项（1-项集）的集合，并得到它们的支持度计数（频繁性）。设最小支持度计数为 2。频繁项的集合按支持度计数的递减序排序。结果集或表记作 L。这样，我们有 L = [I2:7, I1:6, I3:6, I4:2, I5:2]。 然后，FP-树构造如下：首先，创建树的根结点，用“null”标记。二次扫描数据库 D。每个事务中的项按 L 中的次序处理（即，根据递减支持度计数排序）并对每个事务创建一个分枝。例如，第一个事务“T100: I1, I2, I5”按 L 的次序包含三个项\{ I2, I1, I5\}，导致构造树的第一个分枝[(I2:1), (I1:1), (I5:1)]。该分枝具有三个结点，其中，I2 作为根的子女链接，I1 链接到 I2，I5 链接到 I1。第二个事务 T200 按 L 的次序包含项 I2 和 I4，它导致一个分枝，其中，I2 链接到根，I4 链接到 I2。然而，该分枝应当与 T100 已存在的路径共享前缀 I2。这样，我们将结点 I2 的计数增加 1，并创建一个新结点(I4:1)，它作为(I2:2)的子女链接。一般地，当为一个事务考虑增加分枝时，沿共同前缀上的每个结点的计数增加 1，为随在前缀之后的项创建结点并链接。 按TID顺序T100到T900，不断创建节点和连接，并更新节点的支持度计数，知道完成FP树的构建 构建好FP树后，开始利用FP树挖掘频繁项集。FP-树挖掘处理如下。由长度为 1 的频繁模式（初始后缀模式）开始，构造它的条件模式基（一个“子数据库”， 由 FP-树中与后缀模式一起出现的前缀路径集组成）。然后，构造它的（条件）FP-树，并递归地在该树上进行挖掘。模式增长通过后缀模式与由条件 FP-树产生的频繁模式连接实现。 FP-树的挖掘总结在表 6.1 中，细节如下。让我们首先考虑 I5，它是 L 中的最后一个项，而不是第一个。其原因随着我们解释 FP-树挖掘过程就会清楚。I5 出现在上图 的 FP-树的两个分枝。（I5 的出现容易通过沿它的结点链到。）它的两个对应前缀路径是[(I2, I1:1)&gt;和&lt;(I2, I1, I3:1)]，它们形成I5 的条件模式基。然后以及条件模式基和最小支持度计数构建条件FP树，它的条件 FP-树只包含单个路径[(I2:2, I1:2)] (括号里面为路径形式，给出构建的FP树种每个节点的支持度计数)；不包含 I3，因为它的支持度计数为 1，小于最小支持度计数。最后，I5与该路径产生频繁模式的所有组合（I5与路径的所有组合一定是要包含I5的）。组合的支持度计数是根据与结合的节点的支持数决定的。 类似的，对于 I4，它的两个前缀形成条件模式基\{(I2 I1:1), (I2:1)\}，产生一个单结点的条件 FP-树&lt; I2:2&gt;，并导出一个频繁模式 I2 I4:2。与以上分析类似，I3 的条件模式基是\{(I2 I1:2), (I2:2), (I1:2)\}。它的条件 FP-树有两个分枝&lt; I2:4, I1:2&gt;和&lt; I1:2&gt;，如图 6.9 所示，它产生模式集：\{I2 I3:4, I1 I3:2, I2 I1 I3:2\}. FP-增长方法将发现长频繁模式的问题转换成递归地发现一些短模式，然后与后缀连接。它使用最不频繁的项作后缀，提供了好的选择性。该方法大大降低了搜索开销。 使用垂直数据格式挖掘频繁项集Apriori算法和FP-growth算法都是从TID-项集格式（即\{TID : itemset \}）的事务集中挖掘频繁模式，其中TID是事务标识符， 而itemset是事务TID中购买的商品。这种数据格式称为水平数据格式。或者，数据也可以用项-TID集格式（即\{item ： TID_set\}）表示，这种格式称为垂直数据格式。 通过取每对频繁项的TID集的交，可以在该数据集上进行挖掘。项集的支持度计数为TID-集的元素个数。 强规则不一定是有趣的大部分关联规则挖掘算法使用支持度-置信度框架。尽管使用最小支持度和置信度阈值排除了一些无兴趣的规则的探查，仍然会产生一些对用户来说不感兴趣的规则。当A与B是负相关时，规则 A ⇒ B 的置信度有一定的欺骗性。因此，支持度和置信度度量不足以过滤掉无趣的关联规则，为了处理这个问题，可以使用相关性度量来扩充关联规则的支持度-置信度框架。这导致如下形式的相关规则 A\Rightarrow B[support , confidence, correlation]提升度（lift）提升度（lift）是一种简单的相关性度量，A和B出现之间的提升度可以通过计算下式得到 lift(A,B)=\frac{P(A\cup B)}{P(A)P(B)}如果lift(A,B)值小于1，则A的出现和B的出现是负相关的，意味一个出现可能导致另一个不出现。如果值大于1，则A和B是正相关的，意味着每一个的出现都蕴含另一个的出现。如果结果值等于1，则A和B是独立的，它们之间没有相关性。 使用提升度的相关分析如果我们要分析如下的关联规则： buys(X, "computer games")\Rightarrow buys(X, "videos") \\ [support = 40\%, confidence=66\%] 且有下面的事务相依表： 使用卡方检测的相关分析 模式评估度量比较最近，另外一些模式评估度量引起了关注，本书介绍了四种这样的度量：全置信度、最大置信度、Kulczynsji和余弦。然后，比较它们的有效性，并且与提升度和卡方检测$X^2$进行比较。 全置信度： all\_conf(A,B)=\frac{sup(A\cup B)}{max\{sup(A), sup(B)\}}=min\{P(A|B),P(B|A)\} 最大置信度： max\_conf(A,B)=max\{P(A|B),P(B|A)\} Kulczynski: Kulc(A,B)=\frac{1}{2}(P(A|B)+P(B|A)) 余弦： cosine(A,B)=\frac{sup(A\cup B)}{\sqrt{sup(A)\times sup(B)}}=\sqrt{P(A|B)\times P(B|A)} 对于评估所发现的模式联系，哪一个度量最好呢？对于零事务提升度和卡方检测效果都不好，零事务是指不包含任何考察项集的事务。典型地，零事务的个数可能远远大于个体的购买的个数，因为许多人都即不买牛奶也不买咖啡。另一方面，上面的新的四种度量都能解决零事务，因为他们的定义都消除了零事务的影响。一般的，推荐Kluc优先。]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>频繁模式</tag>
        <tag>Apriori算法</tag>
        <tag>FP-Growth算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（2）：数据预处理]]></title>
    <url>%2F2018%2F11%2F02%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[冗余和相关分析分类属性的$X^2$卡方检测对于分类属性，两个属性A和B的相关联系可以通过X2X2(卡方)检测。 以下为一个典型的四格卡方检验，我们想知道喝牛奶对感冒发病率有没有影响： 通过简单的统计我们得出喝牛奶组和不喝牛奶组的感冒率为30.94%和25.00%，两者的差别可能是抽样误差导致，也有可能是牛奶对感冒率真的有影响。 得到的感冒率可能是抽样误差导致，也有可能是牛奶对感冒率真的有影响。 为了确定真实原因,我们先假设喝牛奶对感冒发病率是没有影响的,即喝牛奶喝感冒时独立无关的,所以我们可以得出感冒的实际发病率是(43 + 28)/(43 + 28 + 96 + 84)= 28.29% 所以,理论的四格表应该如下表所示: 即下表: 如果喝牛奶喝感冒真的是独立无关的,那么四格表里的理论值和实际值差别应该会很小。 $X^2$卡方检测值可以用下式计算： x^2=\sum\frac{(A-T)^2}{T}其中，A为实际值，T为理论值。$x^2$值用于衡量实际值与理论值的差异程度和相对大小，值越小属性越独立无关，值越大，属性是统计相关的。 根据上面的卡方检验公式，有 x^2=\frac{(43-39.3231)^2}{39.3231}+\frac{(28-31.6848)^2}{31.6848}+\frac{(96-99.6769)^2}{99.6769}+\frac{(84-80.3152)^2}{80.3152}=1.077卡方分布的临界值: 上一步我们得到了卡方的值,但是如何通过卡方的值来判断喝牛奶和感冒是否真的是独立无关的?也就是说,怎么知道无关性假设是否可靠? 答案是,通过查询卡方分布的临界值表。这里需要用到一个自由度的概念,自由度等于V =(行数- 1)*(列数- 1),对四格表,自由V = 1度。对V = 1,喝牛奶和感概冒95%率不相关的卡方分布的临界概率是:3.84。即如果卡方大于3.84,则认为喝牛奶和感冒有95%的概率相关，有统计联系。 显然1.077 &lt; 3.84,没有达到卡方分布的临界值,所以喝牛奶和感冒独立不相关的假设成立,说明两者之间没说明联系。 数值类型的皮尔逊相关系数 r_{A,B}=\frac{\sum_{i=1}^n(a_i-\bar A)(b_i-\bar B)}{n\sigma_A \sigma_B} 其中，$\bar A$和$\bar B$为均值，和$\sigma_A$ 和$\sigma_B$为标准差。 数值类型的协方差在概率学和统计学中，协方差和方差是两个类型的度量，评估两个属性如何一起变化。 Cov(A,B)=\frac{\sum_{i=1}^n(a_i-\bar A)(b_i-\bar B)}{n}=E(A\cdot B)-\bar A\bar B 其中，$E(A\cdot B)$表示期望 ，用均值表示。 协方差值为0表示具有独立性，协方差越大代表两个属性会一起变化。 小波变换知乎这里讲的很清楚了，主要用于选出有效的特征属性。 戳我]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>卡方检测</tag>
        <tag>皮尔逊相关系数</tag>
        <tag>小波变换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘概念与技术笔记（1）：数据认识]]></title>
    <url>%2F2018%2F11%2F01%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E8%AE%A4%E8%AF%86%2F</url>
    <content type="text"><![CDATA[数据的基本统计描述中心趋势度量：均值、中位数和众数尽管均值是描述数据集的最有用的单个量，但是它并非总是度量数据中心的最佳方法。主要问题是，均值对极端值（例如、离群点）很敏感。为了解决这个问题，我们可以采用结尾均值。结尾均值是丢弃高低极端值后的均值。 对于倾斜（非对称）数据，数据中心的更好的度量是中位数。 度量数据散布：极差、四分位数、方差、标准差和四分位数极差极差：集合的最大值减去最小值 四分位数第1个四分位数记作$Q_1$，是第25个百分位数，第2个为50%，第3个四分位数记作$Q_3$，第75个百分位数。其中，第1个和第3个百分位数之间的距离是散布的一种简单度量，它给出被数据的中间一半所覆盖的范围。该距离称为四分位数极差（IQR），定义为 IQR=Q_3-Q_1识别离群点的通常规则是，挑选落在第3个四分位数之上或第1个四分位数之下至少1.5×IQR处的值。 盒图是一种流行的分布的直观表示，盒图表示了五数概括： 盒的端点一般在Q1和Q3四分位数上，使得盒的长度是四分位数极差 中位数用盒内的线标记 盒外的两条线（称为胡须）延伸到集合的最大和最小值 方差和标准差：标准差是方差的平方根，低的标准差表示数据观测趋向于非常靠近均值。 分位数图分位数图是一种观察数据分布的简单有效的方法。首先，它显示所有的数据（允许用户评估总的情况和不寻常的出现），并将数据由小到大排序，每个观测值$x_(i)$ 与一个百分数 $f_i$ 配对。下图表示了单价数据的分位数图。 分位数-分位数图，或 q-q 图对着另一个的对应分位数，绘制一个单变量分布的分位数。它是一种强有力的直观表示工具，使得用户可以观察从一个分布到另一个是否有移位。 假定对于变量单价，我们有两个观测集，取自两个不同的分店。每组数据都已按递增序排序。下图给出两个部门的QQ图（分位数-分位数图） 度量数据的相似性和相异性一般的，如果两个对象i和j不相似，则他们的相似性度量将返回0。反之两个对象相似则返回1。 数据矩阵和相异性矩阵数据矩阵称对象-属性结构，形式为n×p（n个对象p个属性）矩阵存放n个数据对象，每行对应于一个对象；相异性矩阵存放n个对象两两之间的相似度量，是个n×n对称矩阵(类似皮尔逊相关系数) 分类属性的邻近性度量如何计算分类属性所刻画对象之间的相异性？两个对象i和j之间的相异性可以根据不匹配率来计算： d(i,j)=\frac{p-m}{p} 其中，m是匹配的数目（即i和j取值相同状态的属性数），而p是刻画对象的属性总数。假设我们有表2.2中的4个对象的数据样本，每个对象3个属性，其中只有一个分类属性test-1，在上面的式子中，当对象i和j属性匹配时， $d(i,j)=0$ 当对象不匹配时， $d(i,j)=1$. 布尔属性的邻近性度量上面表示两个对象的取0或1的属性数目(q,s,r,t) 对于对称的二元属性（布尔属性），是指每个属性都同样重要。基于对称二元属性的相异性称作对称的二元相异性。如果对象i和j都用对称的二元属性刻画，则i和j的相异性为： d(i,j)=\frac{r+s}{q+r+s+t} 互补的,相似性可用下式计算： sim(i，j)=1-d(i,j) 一个例子：下面gender为对称属性，其余为非对称属性（共6个），这里我们只考虑患者(对象)非对称属性，值Y(yes)和P(positive)都设置为1，N(no,negative)为0. 采用上面非对称的二元相异性计算公式。 数值属性的相异性：闵可夫斯基距离 d(i,j)=\sqrt[p]{|x_{i1}-x_{j1}|^p+|x_{i2}-x_{j2}|^p+...+|x_{i1}-x_{j1}|^p}偏序属性的邻近性度量偏序属性的值之间具有意义的序或排位，例如size属性的序列值[small, medium, large]. 计算这种偏序属性首先计算状态在序数属性上的排名，并映射到[0, 1]数值上。然后把转换后的数值用闵可夫斯基距离来计算相似性。排名转换公式如下： z_{if}=\frac{r_{if}-1}{M_f-1}其中，属性$f$有$M_f$个有序的状态，表示排位$1,2…M_f$。排位$r_{if}$表示当前属性状态排名。 一个例子： test-2偏序属性，有三个状态，即$M_f$=3，四个对象转换为排位分别为3、1、2、3。然后分别映射为1.0、0.0、0.5、1.0数值，最后可以使用欧几里得距离来计算如下的相异性矩阵。 混合类型属性的相异性解决这种情况的方法是讲所有类型一起处理，把所有有意义的属性转换到共同区间[0.0 , 1.0]上 1、如果是数值，用归一化。2、如果是类别属性，匹配为1不匹配为0。3、偏序将排位先转为数值，再按数值的归一化处理。 一个例子：（混合了分类、偏序、和数值） 之前，处理test-1(分类属性)和test-2(偏序属性)的过程已经给出，可以使用它们之前的相异性矩阵。所以这里首先计算test-3(数值属性)的相异性矩阵。有max=64，min=22，比较对象用归一化处理后，得到test-3的相异性矩阵： 其中，d(3,1)是对象1和对象3每个不同属性的相似性矩阵计算得到的值，总的处理方式还是归一化。 余弦相似性给出了四个文档的词频向量，用于比较这些文档之间的相似性。]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>相似性</tag>
        <tag>分位数图</tag>
        <tag>QQ图</tag>
        <tag>相异性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2F2018%2F09%2F24%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[五问动态规划问：动态规划是什么？ 答：动态规划是一种通过“大而化小”的思路解决问题的算法。区别于一些固定形式的算法，如二分法，宽度优先搜索法，动态规划没有实际的步骤来规定第一步做什么第二步做什么。所以更加确切的说，动态规划是一种解决问题的思想。这种思想的本质是，一个规模比较大的问题（假如用2-3个参数可以表示），是通过规模比较小的若干问题的结果来得到的（通过取最大，取最小，或者加起来之类的运算）所以我们经常看到的动态规划的核心——状态转移方程都长成这样： f[i][j] = f[i - 1][j] + f[i][j - 1] f[i] = max\{f[j] if j < i and …\} + 1 f[i][j] = f[0][j - 1] \&\& judge(1,i) || f[1][j - 1] \&\& judge(2,i) ||问：动态规划什么时候可以用？ 答：动态规划解决的一定是最优化问题。一个问题必须有重叠的子问题和最优子结构，才能使用动态规划取解决问题。 问：动态规划的常见类型有哪些？ 矩阵型 序列型 双序列型 划分型 区间型 背包型 问：什么样的问题适合使用动态规划？ 答：可以使用动态规划的问题一般都有一些特点可以遵循。如题目的问法一般是三种方式： 求最大值/最小值 求可不可行 求方案总数 如果你碰到一个问题，是问你这三个问题之一的，那么有90%的概率是使用动态规划来求解。要重点说明的是，如果一个问题让你求出“所有的”方案和结果，则肯定不是使用动态规划。 解决一个动态规划问题的步骤是什么？ 答：首先判断是否是动态规划的问题，如果是，则尝试将其进行分类常见类型，找到对应的类别和相似的问题。接着从下面的4个要素去逐步剖析解决这道题： 状态是什么 状态转移方程是什么 状态的初始值是什么 问题要求的最后答案是什么 每个步骤分析完成之后，就基本上解决了整道动态规划的问题。 动态规划相关题交叉字符串题目：给出三个字符串:s1、s2、s3，判断s3是否由s1和s2交叉构成。 样例 比如 s1 = “aabcc” s2 = “dbbca” 当 s3 = “aadbbcbcac”，返回 true. 当 s3 = “aadbbbaccc”， 返回 false. 思路： 1.这题我们利用动态规划加记忆化搜索。如果能够进行交叉组成，利用动态规划，建立 $boolean dp[i][j]$， 意思是s1的第i为 和s2的第j为是否能够够成s3的i + j 长度的交叉字符串。不一定要每个字符交替插入，s1 = aa, s2 = d 也可以组成s3 = aad。记忆矩阵这里要清楚定义，一个维度是s1的长度，一个维度是s2的长度。 2.因此状态转移方程就可以写成: dp[i][j] = 1) dp[i][j-1] if s3[i+j -1] == s2[j-1] 2) dp[i-1][j] if s3[i+j-1] == s1[i-1] 3.初始条件要注意，我们这里是把记忆矩阵建立为$test$，因此第一行就是没有s1的情况下看看s2能不能与s3配对，第一列就是在没有s2的情况下能不能与s3配对。4.最后就是看最右下角的dp。时间复杂度是o(n*m) 1234567891011121314151617181920212223242526272829class Solution: """ @params s1, s2, s3: Three strings as description. @return: return True if s3 is formed by the interleaving of s1 and s2 or False if not. @hint: you can use [[True] * m for i in range (n)] to allocate a n*m matrix. """ def isInterleave(self, s1, s2, s3): # write your code here if s1 is None or s2 is None or s3 is None: return False if len(s1) + len(s2) != len(s3): return False # 初始边界s1行s2列的false interleave = [[False] * (len(s2) + 1) for i in range(len(s1) + 1)] interleave[0][0] = True for i in range(len(s1)): interleave[i + 1][0] = s1[:i + 1] == s3[:i + 1] for i in range(len(s2)): interleave[0][i + 1] = s2[:i + 1] == s3[:i + 1] for i in range(len(s1)): for j in range(len(s2)): interleave[i + 1][j + 1] = False if s1[i] == s3[i + j + 1]: interleave[i + 1][j + 1] = interleave[i][j + 1] if s2[j] == s3[i + j + 1]: interleave[i + 1][j + 1] |= interleave[i + 1][j] return interleave[len(s1)][len(s2)] 最长上升子序列描述：给定一个整数序列，找到最长上升子序列（LIS，不要求一定连续），返回LIS的长度。例如现在有序列A=\{1,2,3,-1,-2,7,9\}，它的最长上升子序列为\{1,2,3,7,9\}，长度为5. 思路：dp[i] 表示走到第i个元素时的当前最大连续子序列的长度 ，这样对A[i]有两种可能： 1、如果A[i]之前的元素A[j]，其中$jdp[i]$，那么可以把A[i]拼接到A[j]的后面 2、如果之前的元素都比A[i]大，则A[i]自己成为最大的上升自序，长度为1 123456789101112131415class Solution: """ @param nums: The integer array @return: The length of LIS (longest increasing subsequence) """ def longestIncreasingSubsequence(self, nums): # write your code here if nums is None or not nums: return 0 dp = [1] * len(nums) # 边界初始条件 for curr, val in enumerate(nums): for prev in xrange(curr): if nums[prev] &lt; val: # 如果之前的元素大于等于curr，则dp[curr]为初始的1 dp[curr] = max(dp[curr], dp[prev] + 1) #状态转移方程 return max(dp) 单词拆分 ++描述：给定字符串 s 和单词字典 dict，确定 s 是否可以分成一个或多个以空格分隔的子串，并且这些子串都在字典中存在。 样例：给出s = “lintcode”，dict = [“lint”,”code”]返回 true 因为“lintcode”可以被空格切分成“lint code” 思路：如果最大字典长度为k，f[i]的状态由前面i-k到i-1之间决定，这中间任何一段属于dict则f[I]为True 1234567891011121314151617181920class Solution: # @param s: A string s # @param dict: A dictionary of words dict def wordBreak(self, s, dict): if len(dict) == 0: return len(s) == 0 n = len(s) f = [False] * (n + 1) f[0] = True # 初始边界 maxLength = max([len(w) for w in dict]) #先计算字典中最大长度，减少复杂度 for i in xrange(1, n + 1): for j in range(1, min(i, maxLength) + 1): #不必遍历i之前的所有j if not f[i - j]: continue if s[i - j:i] in dict: f[i] = True break # 有一个满足即可判断下一个f[i+1] return f[n] 数字三角形描述：给定一个数字三角形，找到从顶部到底部的最小路径和。每一步可以移动到下面一行的相邻数字上 样例：比如，给出下列数字三角形： 123456[ [2], [3,4], [6,5,7], [4,1,8,3]] 从顶到底部的最小路径和为11 ( 2 + 3 + 5 + 1 = 11)。 思路：自底向上的动态规划, 当前位置由左下或者右下最小值决定，时间复杂度O(n), python3 实现 ， triangle数组代表第i行第j个数字 1234567891011121314class Solution: """ @param triangle: a list of lists of integers @return: An integer, minimum path sum """ def minimumTotal(self, triangle): rows = len(triangle) dp = [[0] * len(triangle[row]) for row in range(rows)] for i in range(len(triangle[rows - 1])): dp[rows - 1][i] = triangle[rows - 1][i] #初始边界 for i in range(rows - 2, -1, -1): for j in range(i + 1): dp[i][j] = min(dp[i + 1][j], dp[i + 1][j + 1]) + triangle[i][j] #状态转移方程 return dp[0][0] 最小路径和描述：给定一个只含非负整数的m×n网格，找到一条从左上角到右下角的可以使数字和最小的路径。 123456789101112131415class Solution: """ @param grid: a list of lists of integers. @return: An integer, minimizes the sum of all numbers along its path """ def minPathSum(self, grid): for i in range(len(grid)): for j in range(len(grid[0])): if i == 0 and j &gt; 0: grid[i][j] += grid[i][j-1] # 第一行，只在左边 elif j == 0 and i &gt; 0: grid[i][j] += grid[i-1][j] # 第一列，只在右边 elif i &gt; 0 and j &gt; 0: grid[i][j] += min(grid[i-1][j], grid[i][j-1]) # 由上面和左面的最小路径决定 return grid[len(grid) - 1][len(grid[0]) - 1] 爬楼梯 ++描述：假设你正在爬楼梯，需要n步你才能到达顶部。但每次你只能爬一步或者两步，你能有多少种不同的方法？ 123456789101112131415class Solution: """ @param n: An integer @return: An integer """ def climbStairs(self, n): # write your code here if n == 0: return 1 if n &lt;= 2: return n result=[1,2] for i in range(n-2): result.append(result[-2]+result[-1]) return result[-1] 不同的路径描述：有一个机器人的位于一个 m × n 个网格左上角。机器人每一时刻只能向下或者向右移动一步。机器人试图达到网格的右下角。问有多少条不同的路径？ 思路：有左边一格的路径数和上面一格的路径数决定 12345678910111213141516class Solution: def uniquePaths(self, m, n): paths = [[0] * n for i in range(m)] #初始边界 # initial rows paths[0][0] = 1 for x in range(1, m): paths[x][0] = paths[x - 1][0] #边界 # initail columns for y in range(1, n): paths[0][y] = paths[0][y - 1] #边界 for x in range(1, m): for y in range(1, n): paths[x][y] = paths[x -1][y] + paths[x][y - 1] return paths[m - 1][n - 1] 编辑距离 +题目：给出两个单词word1和word2，计算出将word1 转换为word2的最少操作次数。你总共三种操作方法： 插入一个字符 删除一个字符 替换一个字符 样例：给出 work1=”mart” 和 work2=”karma”，返回 3。（先进行2个替换，后面进行1个插入） 思路：f[i][j代表第一个字符串以i结尾匹配上（编辑成）第二个字符串以j结尾的字符串，最少需要多少次编辑。 123456789101112131415161718192021222324class Solution: """ @param word1: A string @param word2: A string @return: The minimum number of steps. """ def minDistance(self, word1, word2): n, m = len(word1), len(word2) f = [[0] * (m + 1) for _ in range(n + 1)] for i in range(n + 1): f[i][0] = i # 边界 for j in range(m + 1): f[0][j] = j # 边界 for i in range(1, n + 1): for j in range(1, m + 1): if word1[i - 1] == word2[j - 1]: f[i][j] = min(f[i - 1][j - 1], f[i - 1][j] + 1, f[i][j - 1] + 1) # equivalent to f[i][j] = f[i - 1][j - 1] else: #分别代表替换，插入，删除 f[i][j] = min(f[i - 1][j - 1], f[i - 1][j], f[i][j - 1]) + 1 return f[n][m] 正则表达式匹配 ++描述：实现支持‘.’和‘*‘的正则表达式匹配。’.’匹配任意一个字母。’*’匹配零个或者多个前面的元素。匹配应该覆盖整个输入字符串，而不仅仅是一部分。返回true 和 false 样例 12345678910111213isMatch("aa","a") → falseisMatch("aa","aa") → trueisMatch("aaa","aa") → falseisMatch("aa", "a*") → trueisMatch("aa", ".*") → trueisMatch("ab", ".*") → trueisMatch("aab", "c*a*b") → true 123456789101112131415161718192021222324class Solution(object): # DP def isMatch(self, s, p): dp = [[False for i in range(0,len(p) + 1)] for j in range(0, len(s) + 1)] dp[0][0] = True for i in range(1, len(p) + 1): if (p[i - 1] == '*'): dp[0][i] = dp[0][i - 2] for i in range(1, len(s) + 1): for j in range(1, len(p) + 1): if p[j - 1] == '*': dp[i][j] = dp[i][j - 2] if s[i - 1] == p[j - 2] or p[j - 2] == '.': dp[i][j] |= dp[i-1][j] else: if s[i - 1] == p[j - 1] or p[j - 1] == '.': dp[i][j] = dp[i - 1][j - 1] return dp[len(s)][len(p)] # 懒癌版 def isMatch(self, s, p): # '$'字符规则代表匹配字符串的末尾，匹配返回一个Match 对象，否则返回None return re.match(p + '$', s) != None 不同的二叉查找树 II描述：给出n，生成所有由1…n为节点组成的不同的二叉查找树 样例：给出n = 3，生成所有5种不同形态的二叉查找树： 123451 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \2 1 2 3 123456789101112131415161718192021222324252627"""Definition of TreeNode:class TreeNode: def __init__(self, val): this.val = val this.left, this.right = None, None"""class Solution: # @paramn n: An integer # @return: A list of root def generateTrees(self, n): # write your code here return self.dfs(1, n) def dfs(self, start, end): if start &gt; end: return [None] res = [] for rootval in range(start, end+1): LeftTree = self.dfs(start, rootval-1) RightTree = self.dfs(rootval+1, end) for i in LeftTree: for j in RightTree: root = TreeNode(rootval) root.left = i root.right = j res.append(root) return res 乘积最大子序列 +描述：找出一个序列中乘积最大的连续子序列（至少包含一个数） 样例：比如, 序列 [2,3,-2,4] 中乘积最大的子序列为 [2,3] ，其乘积为6。 思路：这道题和maximal subarray思路一样，不同的是对于加法加上负数会变小，加上正数会变大；而对于乘法，乘以正数有可能变大也有可能变小（原数是负数的情况下），乘以负数也有可能变大或者变小 所以需要两个变量：min_p表示行进到当前subarray能得到的最小的积max_p表示行进到当前subarray能得到的最大的积 对于某一个subarray来说，它最大的积，有可能来自之前的最大积乘以一个正数，或者之前的最小积乘以一个负数，或者nums[i]就是最大的因此 $max_p = max(nums[i], max_p × nums[i], min_p × nums[i])$最小积同理 最后用res变量跟踪一下全局最大 12345678910111213141516171819202122class Solution: """ @param nums: An array of integers @return: An integer """ def maxProduct(self, nums): if not nums: return None global_max = prev_max = prev_min = nums[0] for num in nums[1:]: if num &gt; 0: curt_max = max(num, prev_max * num) curt_min = min(num, prev_min * num) else: curt_max = max(num, prev_min * num) curt_min = min(num, prev_max * num) global_max = max(global_max, curt_max) prev_max, prev_min = curt_max, curt_min return global_max 通配符匹配描述：判断两个可能包含通配符“？”和“*”的字符串是否匹配。匹配规则如下： 123'?' 可以匹配任何单个字符。'*' 可以匹配任意字符串（包括空字符串）。两个串完全匹配才算匹配成功。 样例： 1234567isMatch("aa","a") → falseisMatch("aa","aa") → trueisMatch("aaa","aa") → falseisMatch("aa", "*") → trueisMatch("aa", "a*") → trueisMatch("ab", "?*") → trueisMatch("aab", "c*a*b") → false 123456789101112131415161718192021222324252627class Solution: """ @param s: A string @param p: A string includes "?" and "*" @return: A boolean """ def isMatch(self, s, p): # write your code here n = len(s) m = len(p) f = [[False] * (m + 1) for i in range(n + 1)] f[0][0] = True if n == 0 and p.count('*') == m: return True for i in range(0, n + 1): for j in range(0, m + 1): if i &gt; 0 and j &gt; 0: f[i][j] |= f[i-1][j-1] and (s[i-1] == p[j-1] or p[j - 1] in ['?', '*']) if i &gt; 0 and j &gt; 0: f[i][j] |= f[i - 1][j] and p[j - 1] == '*' if j &gt; 0: f[i][j] |= f[i][j - 1] and p[j - 1] == '*' return f[n][m] 打劫房屋 ++描述：假设你是一个专业的窃贼，准备沿着一条街打劫房屋。每个房子都存放着特定金额的钱。你面临的唯一约束条件是：相邻的房子装着相互联系的防盗系统，且 当相邻的两个房子同一天被打劫时，该系统会自动报警。给定一个非负整数列表，表示每个房子中存放的钱， 算一算，如果今晚去打劫，你最多可以得到多少钱 在不触动报警装置的情况下。 样例：给定 [3, 8, 4], 返回 8. 12345678910111213141516171819202122232425262728293031323334353637class Solution: """ @param A: An array of non-negative integers @return: The maximum amount of money you can rob tonight """ def houseRobber(self, A): if not A: return 0 if len(A) &lt;= 2: return max(A) f = [0] * len(A) f[0], f[1] = A[0], max(A[0], A[1]) for i in range(2, len(A)): f[i] = max(f[i - 1], f[i - 2] + A[i]) return f[len(A) - 1] # 使用滚动数组版本class Solution: """ @param A: An array of non-negative integers @return: The maximum amount of money you can rob tonight """ def houseRobber(self, A): if not A: return 0 if len(A) &lt;= 2: return max(A) f = [0] * 3 f[0], f[1] = A[0], max(A[0], A[1]) for i in range(2, len(A)): f[i % 3] = max(f[(i - 1) % 3], f[(i - 2) % 3] + A[i]) return f[(len(A) - 1) % 3] 买卖股票的最佳时机描述：假设你有一个数组，它的第i个元素是一支给定的股票在第i天的价格。设计一个算法来找到最大的利润。你最多可以完成 k 笔交易。你不可以同时参与多笔交易(你必须在再次购买前出售掉之前的股票) 样例：给定价格 = [4,4,6,1,1,4,2,5], 且 k = 2, 返回 6.（1买4卖，2买5卖） 123456789101112131415161718192021222324class Solution: """ @param k: an integer @param prices: an integer array @return: an integer which is maximum profit """ def maxProfit(self, k, prices): # write your code here size = len(prices) if k &gt;= size / 2: return self.quickSolve(size, prices) dp = [None] * (2 * k + 1) dp[0] = 0 for i in range(size): for j in range(min(2 * k, i + 1) , 0 , -1): dp[j] = max(dp[j], dp[j - 1] + prices[i] * [1, -1][j % 2]) return max(dp) def quickSolve(self, size, prices): sum = 0 for x in range(size - 1): if prices[x + 1] &gt; prices[x]: sum += prices[x + 1] - prices[x] return sum 解码方法+描述：有一个消息包含A-Z通过以下规则编码，现在给你一个加密过后的消息，问有几种解码的方式 1234'A' -&gt; 1'B' -&gt; 2...'Z' -&gt; 26 样例：给你的消息为12，有两种方式解码 AB(12) 或者 L(12). 所以返回 2 1234567891011121314151617181920class Solution: # @param &#123;string&#125; s a string, encoded message # @return &#123;int&#125; an integer, the number of ways decoding def numDecodings(self, s): # Write your code here if s == "" or s[0] == '0': return 0 dp=[1,1] for i in range(2,len(s) + 1): if 10 &lt;= int(s[i - 2 : i]) &lt;=26 and s[i - 1] != '0': dp.append(dp[i - 1] + dp[i - 2]) elif int(s[i-2 : i]) == 10 or int(s[i - 2 : i]) == 20: dp.append(dp[i - 2]) elif s[i-1] != '0': dp.append(dp[i-1]) else: return 0 return dp[len(s)] 完美平方描述：给一个正整数 n, 找到若干个完全平方数(比如1, 4, 9, … )使得他们的和等于 n。你需要让平方数的个数最少。 样例： 给出 n = 12, 返回 3 因为 12 = 4 + 4 + 4。给出 n = 13, 返回 2 因为 13 = 4 + 9 123456789101112131415161718def numSquares(self, n): # write your code here dp = [] import sys for i in range(n+1): dp.append(sys.maxint) i = 0 while i * i &lt;= n: dp[i*i] = 1 i += 1 for i in range(n+1): j = 1 while j * j &lt;= i: dp[i] = min(dp[i], dp[i-j*j] + 1) j += 1 return dp[n]]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重点掌握]]></title>
    <url>%2F2018%2F09%2F21%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E9%87%8D%E7%82%B9%E6%8E%8C%E6%8F%A1%2F</url>
    <content type="text"><![CDATA[二叉树123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138class Node(): #节点类 def __init__(self,data = -1): self.data = data self.left = None self.right = Noneclass Tree(): #树类 def __init__(self): self.root = Node() def add(self,data): # 为树加入节点 node = Node(data) if self.root.data == -1: #如果树为空，就对根节点赋值 self.root = node else: myQueue = [] treeNode = self.root myQueue.append(treeNode) while myQueue: #对已有的节点进行层次遍历 treeNode = myQueue.pop(0) if not treeNode.left: treeNode.left = node return elif not treeNode.right: treeNode.right = node return else: myQueue.append(treeNode.left) myQueue.append(treeNode.right) def pre_order_recursion(self,root): #递归实现前序遍历 if not root: return print root.data, self.pre_order_recursion(root.left) self.pre_order_recursion(root.right) def pre_order_stack(self,root): #堆栈实现前序遍历（非递归） if not root: return myStack = [] node = root while myStack or node: while node: #从根节点开始，一直寻找他的左子树 print node.data, # 先序，进栈前就要读取了 myStack.append(node) # 先存进栈，以后还需要它的右节点 node = node.left node = myStack.pop() #while结束表示当前节点node为空，即前一个节点没有左子树了 node = node.right #开始查看它的右子树 def in_order_recursion(self,root): #递归实现中序遍历 if not root: return self.in_order_recursion(root.left) print root.data, self.in_order_recursion(root.right) def in_order_stack(self,root): #堆栈实现中序遍历（非递归） if not root: return myStack = [] node = root while myStack or node: #从根节点开始，一直寻找它的左子树 while node: myStack.append(node) node = node.left node = myStack.pop() # 中序，弹出来后才读取 print node.data, node = node.right def post_order_recursion(self,root): #递归实现后序遍历 if not root: return self.post_order_recursion(root.left) self.post_order_recursion(root.right) print root.data, def post_order_stack(self, root): # 堆栈实现后序遍历（非递归） # 先遍历根节点，再遍历右子树，最后是左子树，这样就可以转化为和先序遍历一个类型了，最后只把遍历结果逆序输出就OK了。 if not root: return myStack1 = [] myStack2 = [] node = root while myStack1 or node: while node: myStack2.append(node) myStack1.append(node) node = node.right node = myStack1.pop() node = node.left while myStack2: print myStack2.pop().data, def level_order_queue(self,root): #队列实现层次遍历（非递归） if not root : return myQueue = [] node = root myQueue.append(node) while myQueue: node = myQueue.pop(0) print node.data, if node.left: myQueue.append(node.left) if node.right: myQueue.append(node.right) if __name__ == '__main__': #主函数 datas = [2,3,4,5,6,7,8,9] tree = Tree() #新建一个树对象 for data in datas: tree.add(data) #逐个加入树的节点 print '递归实现前序遍历：' tree.pre_order_recursion(tree.root) print '\n堆栈实现前序遍历' tree.pre_order_stack(tree.root) print "\n\n递归实现中序遍历：" tree.in_order_recursion(tree.root) print "\n堆栈实现中序遍历：" tree.in_order_stack(tree.root) print '\n\n递归实现后序遍历：' tree.post_order_recursion(tree.root) print '\n堆栈实现后序遍历：' tree.post_order_stack(tree.root) print '\n\n队列实现层次遍历：' tree.level_order_queue(tree.root)！ 二分查找12345678910111213141516171819# 二分查找def BinarySearch(array,t): low = 0 height = len(array)-1 while low &lt; height: mid = (low+height)/2 if array[mid] &lt; t: low = mid + 1 elif array[mid] &gt; t: height = mid - 1 else: return array[mid] return -1if __name__ == "__main__": print BinarySearch([1,2,3,34,56,57,78,87],57) 广度优先与深度优先下面的代码强调一下: dfs和bfs区别（重点） pop()和pop(0) order加入w的时机 判断w的条件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172'''深度优先遍历： 是一种用于遍历树或者图的算法。沿着树的深度遍历树的节点，尽可能深地搜索树的分支。当节点v的所在边都被搜索过了。搜索将回溯到节点v的那条边的起始节点。直到已发现从源节点可达的所有节点为止。如果还存在未发现的节点，则选择其中一个作为源节点并重复上述过程，整个进程反复进行直到所有节点都被访问为止.广度优先遍历：从根节点开始，沿着树的宽度遍历树的节点，如果所有节点都被访问，则算法终止'''class Graph(object): def __init__(self, nodes, sides): # nodes表示用户输入的点，int型，sides表示用户输入的边，是一个二元组(u, v) # self.sequence是字典，key是点，value是与key相连的边 self.sequence = &#123;&#125; # self.side是临时变量，主要用于保存与 指定点v 相连接的点 self.side = [] for node in nodes: for side in sides: u, v = side if node == u: self.side.append(v) elif node == v: self.side.append(u) # 第二层主要是遍历属于这个点的所有边，然后将点和边组成字典 self.sequence[node] = self.side self.side = [] # print self.sequence # &#123;1: [2, 3], 2: [1, 4, 5], 3: [1, 6, 7], 4: [2, 8], 5: [2, 8], 6: [3, 7], 7: [3, 6], 8: [4, 5]&#125; def dfs(self, node0): # order里面存放的是具体的访问路径，已经遍历的了 queue, order = [], [] queue.append(node0) while queue: v = queue.pop() # 取出最后一个，为上一个刚加入节点的连接节点 order.append(v) # 深度优先先加入，注意这个order的加入顺序 for w in self.sequence[v]: # 两边 # 不在order表示没被遍历， if w not in order and w not in queue: queue.append(w) return order # bfs同理 def bfs(self, node0): queue, order = [], [] queue.append(node0) order.append(node0) while queue: v = queue.pop(0) # 层次遍历按迅速取出 for w in self.sequence[v]: # if w not in order and w not in queue: if w not in order order.append(w) # 没被遍历就直接将两边加入order queue.append(w) return order def main(): nodes = [i+1 for i in xrange(8)] sides = [(1, 2), (1, 3), (2, 4), (2, 5), (4, 8), (5, 8), (3, 6), (3, 7), (6, 7)] G = Graph(nodes, sides) print G.dfs(1) print G.bfs(1) if __name__ == '__main__': main()]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串解题]]></title>
    <url>%2F2018%2F09%2F19%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%A7%A3%E9%A2%98%2F</url>
    <content type="text"><![CDATA[字符串相关题_python版 最长无重复字符子串长度题目：给定一个字符串，请找出其中无重复字符的最长子字符串。例如，在”abcabcbb”中，其无重复字符的最长子字符串是”abc”，其长度为 3。 思路：遍历字符串中的每一个元素。借助一个辅助键值对来存储某个元素最后一次出现的下标。用一个整形变量存储当前无重复字符的子串开始的下标。 12345678910111213141516171819202122# 分析 a b c d e f g d 此时从最近重复的前一个字符d的后一位开始记，即e标记为start。此时继续取下一个数， 例如a，它的前一个字符下标为d[s[i]]=0，若d[s[i]]&lt;start，则不需要更新start,否则更新start。新的无重复子串变为e f g d aclass Solution: """ @param: s: a string @return: an integer """ def lengthOfLongestSubstring(self, s): # write your code here res = 0 if s is None or len(s) == 0: return res d = &#123;&#125; # 存储某个元素最后一次出现的下标 tmp = 0 # 存储每次循环中最长的子串长度 start = 0 # 记录最近重复字符所在的位置+1 for i in range(len(s)): # 下标 # 判断当前字符是否在字典中和当前字符的下标是否大于等于最近重复字符的所在位置 if s[i] in d and d[s[i]] &gt;= start: # 这里的d[s[i]]为前一个重复的下标 start = d[s[i]] + 1 tmp = i - start + 1 d[s[i]] = i res = max(res, tmp) return res 最长回文字符串思路一：中心扩展法。根据回文的特性，显然所有的回文串都是对称的。长度为奇数回文串以最中间字符的位置为对称轴左右对称，而长度为偶数的回文串的对称轴在中间两个字符之间的空隙。可否利用这种对称性来提高算法效率呢？答案是肯定的。我们知道整个字符串中的所有字符，以及字符间的空隙，都可能是某个回文子串的对称轴位置。可以遍历这些位置，在每个位置上同时向左和向右扩展，直到左右两边的字符不同，或者达到边界。对于一个长度为n的字符串，这样的位置一共有n+n-1=2n-1个 ，时间复杂度O(n^2) 12345678910111213141516171819202122232425class Solution(object): def longestPalindrome(self, s): str_length = len(s) max_length = 0 # 记录最大字符串长度,不是对称长度 start = 0 # 记录位置 for i in range(str_length): # 当前下标位置 # 对称位置在对称轴间隙，偶数 if i - max_length &gt;= 1 and s[i-max_length-1: i+1] == s[i-max_length-1: i+1][::-1]: # 记录当前开始位置 start = i - max_length - 1 max_length += 2 continue # 对称位置在对称字符，奇数 if i - max_length &gt;= 0 and s[i-max_length: i+1] == s[i-max_length: i+1][::-1]: start = i - max_length max_length += 1 # 返回最长回文子串 return s[start: start + max_length]if __name__ == '__main__': s = "babad" # s = "cbbd" sl = Solution() print(sl.longestPalindrome(s)) 思路二：马拉车算法 看这篇 KMP算法过程描述看这篇 其实就是,对模式串p进行预处理,得到前后缀的部分匹配表,使得我们可以借助已知信息,算出可以右移多少位.即 kmp = 朴素匹配 + 移动多位. 1234567891011121314151617181920212223242526272829303132333435#KMP def kmp_match(self, s, p): m = len(s) n = len(p) cur = 0 # 起始指针cur，累积移动数 table = self.partial_table(p) while cur &lt;= m-n: # 长度不够就终止 for i in range(n): # 一次p从头开始匹配的长度 if s[i+cur] != p[i]: # 移动位数 = 已匹配的字符数 - 对应的部分匹配值 # 有了部分匹配表,我们不只是单纯的1位1位往右移,可以一次移动多位 cur += max(i - table[i-1], 1) break else: # 执行了break就不会执行这句，相当于for循环里所有都满足 s[i+cur] == p[i] return cur # 返回匹配开始的位置 return -1 # 匹配失败 #部分匹配表 def partial_table(self, p): '''''partial_table("ABCDABD") -&gt; [0, 0, 0, 0, 1, 2, 0]''' prefix = set() table = [0] for i in range(1, len(p)): # 从1开始进行前后缀比较 prefix.add(p[:i]) # 前缀每次累加就行 postfix = set() for j in range(1, i+1): # i+1 因为i需要包括 postfix.add(p[j:i+1]) # print(prefix, postfix) # print(prefix&amp;postfix, len(prefix&amp;postfix)) # table.append(len((sorted((prefix&amp;postfix),key = len)or &#123;''&#125;).pop())) if prefix&amp;postfix: table.append(max(map(len,prefix&amp;postfix))) else: table.append(0) return table 字符串去重1string = 'abc123456ab2s'r = ''.join(x for i, x in enumerate(string) if string.index(x) == i) 统计一个字符串中英文字母、空格、数字的个数12345678910111213141516171819#!/usr/bin/python# -*- coding: UTF-8 -*- import strings = raw_input('请输入一个字符串:\n')letters = 0space = 0digit = 0others = 0for c in s: if c.isalpha(): letters += 1 elif c.isspace(): space += 1 elif c.isdigit(): digit += 1 else: others += 1print 'char = %d,space = %d,digit = %d,others = %d' % (letters,space,digit,others)]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组解题]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E7%BB%84%E8%A7%A3%E9%A2%98%2F</url>
    <content type="text"><![CDATA[数组相关题_python版 寻找某个值的区间（leetcode 34 Search for a Range）题目：这题要求在一个排好序可能有重复元素的数组里面找到包含某个值的区间范围。要求使用O(log n)的时间，所以我们采用两次二分查找。 For Example：Given [5, 7, 7, 8, 8, 10] and target value 8,return [3, 4]. 123456789101112131415161718192021222324252627282930class Solution: # @param A, a list of integers # @param target, an integer to be searched # @return a list of length 2, [index1, index2] def searchRange(self, A, target): left = 0 right = len(A) - 1 result = [-1, -1] while left &lt;= right: mid = (left + right) / 2 if A[mid] &gt; target: right = mid - 1 elif A[mid] &lt; target: left = mid + 1 else: # 找到时 result[0] = mid result[1] = mid i = mid - 1 # 向前找 while i &gt;= 0 and A[i] == target: result[0] = i i -= 1 i = mid + 1 # 向后找 while i &lt; len(A) and A[i] == target: result[1] = i i += 1 break return result 第K个数的问题题目：这题是一道很好的面试题目，首先题目短小，很快就能说清题意而且有很多种解法。从简单到复杂的解法都有，梯度均匀。解决它不需要预先知道特殊领域知识。 这题有很多思路： 按从大到小全排序，然后取第k个元素，时间复杂度O(nlogn)，空间复杂度O(1) 利用堆进行部分排序。维护一个大根堆，将数组元素全部压入堆，然后弹出k次，第k个就是答案。时间复杂度O(klogn)O(klogn)，空间复杂度O(n)O(n) 选择排序，第k次选择后即可得到第k大的数，时间复杂度O(nk)，空间复杂度O(1) 以上三种方法时间复杂度太高。下面介绍两种更好的方法： 维持K大小的堆排序（优先队列） 123456789101112131415161718'''用容量为K的最大堆来存储最小的K个数。最大堆的堆顶元素就是最小K个数中的最大的一个。每次扫描一个数据X，如果X比堆顶元素Y大，则不需要改变原来的堆。如果X比堆顶元素小，那么用X替换堆顶元素Y，在替换之后，X可能破坏了最大堆的结构，需要调整堆来维持堆的性质。用优先队列思想也一样，只不过k大小的队列每次移动的元素量较大，堆会好一些。'''# 使用了heapq的内置数据结构，用了一个trick 因为默认是创建小顶堆，所以在添加元素的时候加个 负号就变成大顶堆了。import heapqclass Solution: def GetLeastNumbers_Solution(self, tinput, k): if k &gt; len(tinput) or k == 0: return [] heap = [] for num in tinput: if len(heap) &lt; k: heapq.heappush(heap, -num) else: if -num &gt; heap[0]: heapq.heapreplace(heap, -num) return sorted(list(map(lambda x: x*-1, heap))) 快速排序，时间复杂度近似O（n） 1234567891011121314151617181920'''利用快速排序的思想，从数组S中随机找出一个元素X，把数组分为两部分Sa和Sb。Sa中的元素大于等于X，Sb中元素小于X。这时有两种情况：1. Sa中元素的个数小于k，则Sb中的第k-|Sa|个元素即为第k大数；2. Sa中元素的个数大于等于k，则返回Sa中的第k大数。时间复杂度近似为O(n)'''def qselect(A,k): if len(A)&lt;k:return A pivot = A[-1] right = [pivot] + [x for x in A[:-1] if x&gt;=pivot] rlen = len(right) if rlen==k: return right if rlen&gt;k: return qselect(right, k) else: left = [x for x in A[:-1] if x&lt;pivot] return qselect(left, k-rlen) + right for i in range(1, 10): print qselect([11,8,4,1,5,2,7,9], i) 求根算法（ LeetCode 69）题目：计算并返回 x 的平方根，其中 x 是非负整数。 由于返回类型是整数，结果只保留整数的部分，小数部分将被舍去。 思路一：直接从1到x/2之间遍历，判断是否是平方根的条件是，i*i小于等于x并且小于等于x并且(i+1)*(i+1)大于x，则返回i。超时 。 思路二：二分查找法。初始化i=0，j=x，mid=0。进入循环，找到中间值mid = (i + j) / 2，如果mid&gt;x / mid，表示mid不是平方根，且数值过大，则j=mid。如果mid小于等于x / mid，则判断(mid + 1) &gt; x / (mid + 1)，表示mid*mid小于x，并且mid再加1后的平方就会比x大，这表示mid就是那个平方根，返回mid。否则表示mid过小，i=mid。 12345678910111213141516171819class Solution(object): def mySqrt(self, x): """ :type x: int :rtype: int """ if x==0 or x==1: return x i=0 j=x mid=0 while True: mid=(i+j)/2 if mid&gt;x/mid: j=mid else: if (mid+1)&gt;x/(mid+1): return mid i=mid 数组中后面的数减前面的数的差的最大值题目：如何求数组中数对差最大。数对差是指一个数组中某两个元素a和b（并且a排在b的前面），a减去b所得到的差值。 思路一：遍历存储最大值 思路二：首先求出数组中任意一对相邻的数据之间的差值，得到一个新的数组。如果某两个数据之间的数对差最大，也就是说这两个数据之间的差值最大。假设这两个数据的位置是i和j，那么这两个位置之间的数据是a[i]，a[i+1]，a[i+2]……，a[j-1]，a[j]。那么a[i]-a[j]=(a[i]-a[i+1])+(a[i+1]-a[i+2])+……(a[j-1]-a[j])，括号中的数据是相邻数据的差值，都已经在前面求出来了。然后这个问题就转化为了求数组中连续的子数组和最大的问题，这个问题可以通过动态规划问题求出来。 1234567891011121314151617181920212223242526272829import randomdef Max(firstNum,secondNum): if firstNum&gt;=secondNum: return firstNum else: return secondNum def Count(array): gapArray=[] length=len(array) #遍历一遍记录相邻两个之间的gap for i in xrange(length-1): gap=array[i]-array[i+1] gapArray.append(gap) #转化为子集合最大和问题 max=-((1&lt;&lt;32)-1) sum=0 for i in xrange(len(gapArray)): sum+=gapArray[i] max=Max(max,sum) if sum&lt;0: sum=0 return max array=[]for i in xrange(20): array.append(random.randint(0,50))print arrayprint "max gap:"+str(Count(array)) 合并多个有序数组1234567891011121314151617181920212223242526272829303132333435#采用归并排序算法#拆解到最后，实际变成两个数组进行排序def MergeSort(nums): #请牢记传入的参数是多维数组 #此处是递归出口 if len(nums) &lt;= 1: return nums mid = len(nums) // 2 #记住此处得到的也是多维数组 Left = MergeSort(nums[:mid]) Right = MergeSort(nums[mid:]) #print(Left[0], Right[0]) #要传入的参数是数组中第一个索引处的值 return Sort_list(Left[0], Right[0])def Sort_list(Left, Right): #存储排序后的值 res = [] a = 0 b = 0 while a &lt; len(Left) and b &lt; len(Right): if Left[a] &lt; Right[b]: res.append(Left[a]) a += 1 else: res.append(Right[b]) b += 1 res = res + Left[a:] + Right[b:] # 转为二维数组 res = [res] return res 两个有序数组求差集思路一：依次取出较小数组的元素，然后再另外一个数组上进行二分查找 思路二：用齐头并进的两个下标 12345678910111213141516171819class Solution: def intersect(self, nums1, nums2): nums3 = [] nums1.sort() nums2.sort() m = len(nums1) n = len(nums2) i = 0 j = 0 while i&lt;m and j&lt;n: if nums1[i] == nums2[j]: nums3.append(nums1[i]) i += 1 j += 1 elif nums1[i] &gt; nums2[j]: j += 1 else: i += 1 return nums3 两个集合如何求并集，交集；1234567python集合支持一系列标准操作，包括并集、交集、差集和对称差集，例如： a = t | s # t 和 s的并集 b = t &amp; s # t 和 s的交集 c = t – s # 求差集（项在t中，但不在s中） 给定一个数组求中位数(中位数，就是数组排序后处于数组最中间的那个元素) 思路：和TOP k问题一样，这里就不写了。（先排序再取中位数不优）]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表解题]]></title>
    <url>%2F2018%2F09%2F11%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E9%93%BE%E8%A1%A8%E8%A7%A3%E9%A2%98%2F</url>
    <content type="text"><![CDATA[链表相关题_python版 在O(1)时间删除链表结点题目：给定单向链表的头指针和一个结点指针，定义一个函数在O(1)时间删除该结点。 思路：我们要删除结点i，先把i的下一个结点i.next的内容复制到i，然后在把i的指针指向i.next结点的下一个结点即i.next.next，它的效果刚好是把结点i给删除了。需要考虑如果这个节点是链表的尾节点那么就需要从头遍历这个链表了。（通常，在单向链表中，删除一个链表的结点，都会先从表头开始遍历整个链表，找到需要删除的结点的前一个结点，然后将这个结点的(指向下一个结点的)指针元素指向需要删除结点的下一个结点，最后把需要删除的结点删除．但此过程的平均时间复杂度为 O(n)． ） 12345678910111213141516171819class ListNode: def __init__(self, value): self.value = value self.next_ = Nonedef deleteNode(self,pHead,Node): if Node == None or pHead == None: return if Node.next != None: # else情况1：只有一个Node节点；情况2：Node节点在尾巴 Node.val = Node.next.val Node.next = Node.next.next elif Node == pHead:# 如果链表只有一个节点，那么就把头节点删掉就好了 pHead.val = None else: pNode = pHead # 把Node节点删除，然后接上一个None while pNode.next != Node: pNode = pNode.next pNode.next = None return pHead 合并两个排序的链表(要求不新建链表)题目：输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。 思路：非递归情况：找到两个链表中头节点值相对更小的链表，将其作为主链表，第二个链表中的元素则不断加入到主链表中。具体策略是：主链表定义两个指针，指向两个相邻的元素。当第二个链表中的元素值小于主链表中第二个指针时，将第二个链表的当前元素插入到主链表两个指针指向的元素中间，并调整指针指向。 不要让链表断开，考虑链表为空的几种情况。 12345678910111213141516171819202122232425262728293031323334353637# ============非递归版本===============def Merge(self, pHead1, pHead2): if not pHead1: return pHead2 if not pHead2: return pHead1 mainHead = pHead1 if pHead1.val &lt;= pHead2.val else pHead2 # 主链 secHead = pHead2 if mainHead == pHead1 else pHead1 # 副链 mergeHead = mainHead mainNext = mainHead.next # 主链第二个指针 while mainNext and secHead: if secHead.val &lt;= mainNext.val: # 副链节点插入到两个指针之间 mainHead.next = secHead # 第一个指针连接到副链头指针 secHead = secHead.next # 副链头指针后移 mainHead.next.next = mainNext # 副链头指针连接到第二个指针 mainHead = mainHead.next # 第一个指针后移（变成原副链的头指针），插入操作第二个指针不需要后移 else: mainHead = mainNext mainNext = mainNext.next if not mainNext: # 副链元素都比第二个指针要大，不能插入，要拼接 mainHead.next = secHead return mergeHead# ==================递归版本=================class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 is None: # 处理末尾状态，pHead1为空，要拼接的就是pHead2了 return pHead2 if pHead2 is None: return pHead1 if pHead1.val &lt; pHead2.val: pHead1.next = self.Merge(pHead1.next,pHead2) return pHead1 # 返回整段拼接后的链表 else: pHead2.next = self.Merge(pHead1,pHead2.next) return pHead2 二叉搜索树与双向链表题目：输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向 ，例如 思路： 核心算法依旧是中序遍历 不是从根节点开始，而是从中序遍历得到的第一个节点开始 定义两个辅助节点listHead(链表头节点)、listTail(链表尾节点)。事实上，二叉树只是换了种形式的链表；listHead用于记录链表的头节点，用于最后算法的返回；listTail用于定位当前需要更改指向的节点。了解了listHead和listTail的作用，代码理解起来至少顺畅80%。 过程图示例 1234567891011121314151617181920212223242526'''稍微多说一句，其实这段代码也就5行，2行是中序遍历的代码；3行是更改节点指向的代码，为if、else行。if语句段只有在中序遍历到第一个节点时调用，自此之后listHead不变，listTail跟随算法的进度。对比中序遍历可以看出来，实际上只是中序遍历中的第八行代码被上述的if-else语句替代了，仅此而已。'''class TreeNode: def __init__(self, x): self.left = None self.right = None self.val = xclass Solution: def __init__(self): self.listHead = None self.listTail = None def Convert(self, pRootOfTree): if pRootOfTree==None: return self.Convert(pRootOfTree.left) if self.listHead==None: # if/else替换中序遍历存储值 self.listHead = pRootOfTree # if这一段只有中序遍历的第一个节点出现，即最左子树 self.listTail = pRootOfTree # 此时，链表头尾指针都指向中序第一个节点 else: self.listTail.right = pRootOfTree # 尾指针与中序下一个节点互连，有right属性是因为上一步self.listTail和self.listHead已经指向pRootOfTree了 pRootOfTree.left = self.listTail self.listTail = pRootOfTree # 尾指针指向中序下一个节点 self.Convert(pRootOfTree.right) return self.listHead 翻转部分链表题目：给定一个单链表的头指针 head， 以及两个整数 a 和 b下标，在单链表中反转 linked_list[a-b] 的结点，然后返回整个链表的头指针 。 思路：采用翻转单链表的思路，回顾一下 123456789101112131415161718'''翻转单链表思路很简单：1-&gt;2-&gt;3-&gt;4-&gt;5，遍历链表，把1的next置为None，2的next置为1，以此类推，5的next置为4。得到反转链表。'''class Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if pHead==None or pHead.next==None: return pHead pre = None # 前指针 cur = pHead # 当前指针 while cur!=None: tmp = cur.next # 记录下一个指针，为下一步当前指针后移准备 cur.next = pre pre = cur cur = tmp return pre 1234567891011121314151617181920212223242526272829303132333435363738# 翻转部分链表class Solution: def reverseBetween(self, head, m, n): # 计算需要逆至的节点数 reverse_length = n - m + 1 pre_head = None # 初始化要记录的前驱节点 result = head # 最终转换后要返回的链表头结点 # 将head向后移动m-1个位置 while head and m &gt; 1: pre_head = head head = head.next m -= 1 # 记录翻转后的链表尾部，翻转后的尾巴即为当前head modify_list_tail = head pre = None # 前指针 # 逆置n - m + 1个节点 while head and reverse_length: #和翻转单链表一样，翻转后和第一、第三段是断开的 tmp = head.next head.next = pre pre = head head = tmp reverse_length -= 1 # 此时，尾巴为空， modify_list_tail指向最后一个非空元素 # 连接逆置后的链表尾与第三段的头结点结合，此时head已经指向第三段正序的头结点 modify_list_tail.next = head # 如果pre_head不为空，说明不是从第一个节点开始逆至，即m&gt;1 if pre_head is not None: # pre_head指向第一段最后一个元素，连接逆序后的头结点 pre_head.next = pre #pre指向逆序后头结点，head为第三段的头结点 else: # 此时m=1，则逆置后的头结点就是链表的头结点，即翻转整个单链表 result = pre return result 链表插入排序(leetcode 147 Insertion Sort List)题目：利用插入排序对链表进行排序 思路：1-&gt;3-&gt;2-&gt;4-&gt;null，将头结点和后面的部分断开，变成1-&gt;null和3-&gt;2-&gt;4-&gt;null，1-&gt;null看做是排好序的部分，添加的时候依次取后面的那部分的节点，比如在这里，先取3，然后对前面排好序的链表从前往后遍历，找到应该插入的位置即可。 12345678910111213141516171819202122232425262728293031"""Definition of ListNodeclass ListNode(object): def __init__(self, val, next=None): self.val = val self.next = next"""class Solution: """ @param: head: The first node of linked list. @return: The head of linked list. """ def insertionSortList(self, head): # write your code here if head is None: return None if head.next is None: return head l=ListNode(-1) while head: node=l # 每次从头开始 fol=head.next # 保持下一个，防止断开 while node.next and node.next.val &lt; head.val: # 插入到node和node.next之间 node = node.next head.next = node.next # 先连接后面head-&lt;node.next node.next = head # 再连接前面 head = fol return l.next 链表归并排序(leetcode 148 Sort List)题目：要求我们用O(nlogn)算法对链表进行排序 12345678910111213141516171819202122232425262728293031323334353637383940414243class ListNode(object): def __init__(self, val, next=None): self.val = val self.next = next class Solution: # 归并法 def sortList(self, head): # write your code here if head is None or head.next is None: return head pre = head slow = head # 使用快慢指针来确定中点 fast = head while fast and fast.next: pre = slow slow = slow.next fast = fast.next.next left = head right = pre.next # 第二段头结点 pre.next = None # 从中间打断链表 left = self.sortList(left) right = self.sortList(right) return self.merge(left,right) def merge(self, left, right): #合并两个有序链表 pre = ListNode(-1) # 新链表 first = pre # 新链表头结点 while left and right: if left.val &lt; right.val: pre.next = left pre = left left = left.next else: pre.next = right pre = right right = right.next if left: pre.next = left else: pre.next = right return first.next 两两交换链表中相邻的两个元素题目：交换链表中相邻的两个元素。 注意第一个节点与第二个节点要交换位置，而第二个节点不用与第三个节点交换位置。 如要交换链表中A-&gt;B-&gt;C-&gt;D中的B和C需要做如下操作（交换B和C）： 将A指向C 将B指向D 将C指向B 思路：在头节点之前加一个假节点就可以使所有的交换都符合上面的情况。 12345678910111213class Solution(object): def swapPairs(self, head): dummy = ListNode(-1) dummy.next = head # 假结点连接原链表 temp = dummy # 头结点 while temp.next and temp.next.next: node1 = temp.next # node1是B node2 = temp.next.next # node2是C temp.next = node2 # A指向C node1.next = node2.next # B指向D node2.next = node1 # C指向B temp = temp.next.next # 跳过两个 return dummy.next 判断两个链表相交和相交的第一个节点思路1：链表两个链表的长度差diff，然后快指针先走diff步，然后快慢指针一起走。直到两个指针相同，否则无相交节点。（需要先遍历得到两个链表长度） 思路2：两个指针一起走，当一个指针p1走到终点时，说明p1所在的链表比较短，让p1指向另一个链表的头结点开始走，直到p2走到终点，让p2指向短的链表的头结点，那么，接下来两个指针要走的长度就一样了 ，然后就可以一起走，直到两个指针相同。(若无交点，可以一开始在链表尾巴的设置一个标志点) 123456789101112131415# 思路二class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1, p2 = pHead1, pHead2 while p1 != p2: p1 = pHead2 if not p1 else p1.next p2 = pHead1 if not p2 else p2.next return p1 分隔链表题目：给定一个链表以及一个目标值，把小于该目标值的所有节点都移至链表的前端，大于或等于目标值的节点移至链表的尾端，同时要保持这两部分在原先链表中的相对位置。 思路：两个链表指针，一个负责收集比目标小的，一个收集大于等于目标的。 1234567891011121314151617181920212223class Solution(object): def partition(self, head, x): dummy = ListNode(-1) dummy.next = head small_dummy = ListNode(-1) large_dummy = ListNode(-1) # small_prev和large_prev往后遍历增加，small_dummy和large_dummy则负责最后作为返回头结点 small_prev = small_dummy large_prev = large_dummy while dummy.next: # head第一个节点 curr = dummy.next if curr.val &lt; x: small_prev.next = curr small_prev = small_prev.next else: large_prev.next = curr large_prev = large_prev.next dummy = dummy.next large_prev.next = None # 最后指针置为none small_prev.next = large_dummy.next # large_dummy对应的是大链表的第一个数 return small_dummy.next # 返回的是small_dummy 重排链表将单向链表L0→L1→…→Ln-1→Ln转化为L0→Ln→L1→Ln-1→L2→Ln-2→…的形式，也就是从头部取一个节点，从尾部取一个节点，直到将原链表转化成新的链表。 思路： 去中间节点，将链表分为两段. 翻转后一段 拼接 1234567891011121314151617181920212223242526class Solution: def reorderList(self, head): if not head: return # split &#123;1,2,3,4,5&#125; to &#123;1,2,3&#125;&#123;4,5&#125; fast = slow = head while fast and fast.next: slow = slow.next fast = fast.next.next head1 = head head2 = slow.next slow.next = None # reverse the second &#123;4,5&#125; to &#123;5,4&#125; cur, pre = head2, None while cur: tmp = cur.next # 标记下一个 cur.next = pre pre = cur cur = tmp # merge cur1, cur2 = head1, pre while cur2: nex1, nex2 = cur1.next, cur2.next cur1.next = cur2 cur2.next = nex1 cur1, cur2 = nex1, nex2]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典排序总结]]></title>
    <url>%2F2018%2F09%2F11%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[先看一个排序算法可视化大概了解一下经典的排序算法。 排序算法可视化 冒泡排序 BubbleSort介绍冒泡排序(bubble sort)：每个回合都从第一个元素开始和它后面的元素比较，如果比它后面的元素更大的话就交换，一直重复，直到这个元素到了它能到达的位置。每次遍历都将剩下的元素中最大的那个放到了序列的“最后”(除去了前面已经排好的那些元素)。注意检测是否已经完成了排序，如果已完成就可以退出了。 步骤 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对第0个到第n-1个数据做同样的工作。这时，最大的数就“浮”到了数组最后的位置上。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 源代码Python源代码（错误版本）： 1234567def bubble_sort(arry): n = len(arry) #获得数组的长度 for i in range(n): for j in range(i+1, n): if arry[i] &gt; arry[j] : #如果前者比后者大 arry[i],arry[j] = arry[j],arry[i] #则交换两者 return arry 注：上述代码是没有问题的，但是实现却不是冒泡排序，而是选择排序（原理见选择排序），注意冒泡排序的本质是“相邻元素”的顺序交换，而非每次完成一个最小数字的选定。 Python源代码（正确版本）： 12345678def bubble_sort(arry): n = len(arry) #获得数组的长度 for i in range(n): # 这里n-i有可能是最后的下标，如果用j和j+1会超过数组限制，所以应该用j-1和j，把 range改为（1，n-i） for j in range(1, n-i): # 每轮找到最大数值，n-i因为前面已经确定i个最大值，只需比较剩下n-i个 if arry[j-1] &gt; arry[j] : #如果前者比后者大 arry[j-1],arry[j] = arry[j],arry[j-1] #则交换两者 return arry 优化1： 某一趟遍历如果没有数据交换，则说明已经排好序了，因此不用再进行迭代了。用一个标记记录这个状态即可。 Python源代码： 123456789101112def bubble_sort2(ary): n = len(ary) for i in range(n): flag = False # 标记 for j in range(1, n - i): if ary[j] &lt; ary[j-1]: ary[j], ary[j-1] = ary[j-1], ary[j] flag = True # 某一趟遍历如果没有数据交换，则说明已经排好序了，因此不用再进行迭代了 if not flag: break return ary 选择排序 SelectionSort介绍每个回合都选择出剩下的元素中最大的那个，选择的方法是首先默认第一元素是最大的，如果后面的元素比它大的话，那就更新剩下的最大的元素值，找到剩下元素中最大的之后将它放入到合适的位置就行了。和冒泡排序类似，只是找剩下的元素中最大的方式不同而已。 步骤 在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 以此类推，直到所有元素均排序完毕。 源代码123456789def select_sort(ary): n = len(ary) for i in range(0,n): min = i # 最小元素下标标记，这句是最重要的 for j in range(i+1,n): if ary[j] &lt; ary[min] : min = j # 找到最小值的下标 ary[min],ary[i] = ary[i],ary[min] # 交换两者 return ary 插入排序 Insertion Sort介绍插入排序的工作原理是，对于每个未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 对具有nn个数据元素的序列进行排序时，插入排序需要进行n−1n−1趟插入。进行第j(1≥j≥n−1)j(1≥j≥n−1)趟插入时，前面已经有jj个元素排好序了，第jj趟将aj+1aj+1插入到已经排好序的序列中，这样既可使前面的j+1j+1个数据排好序。 步骤 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果被扫描的元素（已排序）大于新元素，将该元素后移一位 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 源代码1234567891011# 插入排序def insert_sort(ary): n = len(ary) for i in range(1, n): pre_key = i - 1 # 往前的下标 mark = ary[i] # 记录当前元素 这两句很重要 while pre_key &gt;= 0 and ary[pre_key] &gt; mark: # 找到第一个比mark的小的元素或到头时结束 ary[pre_key+1] = ary[pre_key] # 往后移一位 pre_key -= 1 ary[pre_key+1] = mark # 找到并插入（想象一个简单的例子） return ary 希尔排序 ShellSort介绍希尔排序，也称递减增量排序算法，实质是分组插入排序。由 Donald Shell 于1959年提出。希尔排序是非稳定排序算法。 希尔排序的基本思想是：先将整个待排元素序列分割成若干个子序列（由相隔某个“增量”的元素组成的）分别进行直接插入排序，然后依次缩减增量再进行排序，待整个序列中的元素基本有序（增量足够小）时，再对全体元素进行一次直接插入排序。因为直接插入排序在元素基本有序的情况下（接近最好情况），效率是很高的，因此希尔排序在时间效率上比前两种方法有较大提高。 n=10的一个数组49, 38, 65, 97, 26, 13, 27, 49, 55, 4为例 第一次 gap = 10/2 = 5 49 38 65 97 26 13 27 49 55 4 1A 1B 2A 2B 3A 3B 4A 4B 5A 5B 1A, 1B, 2A, 2B等为分组标记，数字相同的表示在同一组，同组进行直接插入排序 第二次 gap = 5 / 2 = 2，排序后 13 27 49 55 4 49 38 65 97 26 1A 1B 1C 1D 1E 2A 2B 2C 2D 2E 第三次 gap = 2 / 2 = 1 4 26 13 27 38 49 49 55 97 65 1A 1B 1C 1D 1E 1F 1G 1H 1I 1J 第四次 gap = 1 / 2 = 0 排序完成得到数组： 4 13 26 27 38 49 49 55 65 97 源代码1234567891011121314def shell_sort(ary): count = len(ary) gap = round(count/2) # round精度不够可以考虑用math.floor() # 双杠用于整除（向下取整），在python直接用 “/” 得到的永远是浮点数，用round()得到四舍五入值 while gap &gt;= 1: # 不要忘了这句 for i in range(gap, count): # 前面的gap间隔数组视为已排好序，每次插入到排好序数组中 cur = ary[i] preindex = i -gap # 往前的下标 while preindex &gt;= 0 and ary[preindex] &gt; cur: # 到这里与插入排序一样了 ary[preindex+gap] = ary[preindex] # 往后移 preindex -= gap ary[preindex+gap] = cur # 插入 gap = round(gap/2) return ary 归并排序 Merge Sort介绍归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 首先考虑下如何将将二个有序数列合并。这个非常简单，只要从比较二个数列的第一个数，谁小就先取谁，取了后就在对应数列中删除这个数。然后再进行比较，如果有数列为空，那直接将另一个数列的数据依次取出即可。 源代码12345678910111213141516171819202122232425# 归并排序 def merge_sort(self, ary): if len(ary) &lt;= 1: return ary median = int(len(ary)/2) # 二分分解 left = self.merge_sort(ary[:median]) # 先自调用，最里一层只有一个单元素 right = self.merge_sort(ary[median:]) return self.merge(left, right) # 合并成有序数组 def merge(self, left, right): '''合并操作，将两个有序数组left[]和right[]合并成一个大的有序数组''' res = [] i = j = k = 0 while(i &lt; len(left) and j &lt; len(right)): if left[i] &lt; right[j]: res.append(left[i]) i += 1 else: res.append(right[j]) j += 1 res = res + left[i:] + right[j:] return res 快速排序 QuickSort介绍快速排序是图灵奖得主C.R.A Hoare于1960年提出的一种划分交换排序。它采用了一种分治的策略，通常称其为分治法。分治法的基本思想是：将原问题分解为若干个规模更小但结构与原问题相似的子问题。递归地解这些子问题，然后将这些子问题组合为原问题的解。 以一个数组作为示例，取区间第一个数为基准数。 0 1 2 3 4 5 6 7 8 9 72 6 57 88 60 42 83 73 48 85 初始时，i = 0; j = 9; X = a[i] = 72 由于已经将a[0]中的数保存到X中，可以理解成在数组a[0]上挖了个坑，可以将其它数据填充到这来。 从j开始向前找一个比X小或等于X的数。当j=8，符合条件，将a[8]挖出再填到上一个坑a[0]中。a[0]=a[8]; i++; 这样一个坑a[0]就被搞定了，但又形成了一个新坑a[8]，这怎么办了？简单，再找数字来填a[8]这个坑。这次从i开始向后找一个大于X的数，当i=3，符合条件，将a[3]挖出再填到上一个坑中a[8]=a[3]; j–; 数组变为： 0 1 2 3 4 5 6 7 8 9 48 6 57 88 60 42 83 73 88 85 i = 3; j = 7; X=72 再重复上面的步骤，先从后向前找，再从前向后找。 步骤利用分治法可将快速排序分为三步： 从数列中挑出一个元素作为“基准”（pivot）。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。这个操作称为“分区操作”，分区操作结束后，基准元素所处的位置就是最终排序后它的位置 再对“基准”左右两边的子集不断重复第一步和第二步，直到所有子集只剩下一个元素为止。 源代码版本一1234567891011121314151617181920212223def quick_sort(self, ary): return qsort(ary, 0, len(ary)-1)def qsort(self, ary, start, end): # ary为原数组，其他为下标 if start &lt; end: # 这句不能忘！！！ left = start right = end key = ary[start] while left &lt; right: # 这里都没有等号，left=right是最后key赋值 while left &lt; right and ary[right] &gt;= key: # 核心 right -= 1 if left &lt; right: #说明打破while循环的原因是ary[right] &lt;= key ary[left] = ary[right] # 填坑 left += 1 # 换位继续 while left &lt; right and ary[left] &lt; key: left += 1 if left &lt; right: #说明打破while循环的原因是ary[left] &gt;= key ary[right] = ary[left] right -= 1 ary[left] = key #此时，left=right，用key来填坑 self.qsort(ary, start, left-1) # 注意这里的下标顺序 self.qsort(ary, left+1, end) return ary 版本二12345678910111213def quickSort(array): if len(array)&lt;2: return array else: base = array[0] # 元素，return时要变为列表 #小于等于基准值的元素组成的数组 less = [i for i in array[1:] if i&lt;=base] #大于基准值的元素组成的数组 greater = [i for i in array[1:] if i&gt; base] #将数组串起来 return quickSort(less)+[base]+quickSort(greater)print(quickSort([45, 26, 34, 2, 888, 54, 23, 45, 76, 2])) 堆排序 HeapSort介绍堆排序在 top K 问题中使用比较频繁。堆排序是采用二叉堆的数据结构来实现的，虽然实质上还是一维数组。堆（二叉堆）可以视为一棵完全的二叉树，完全二叉树的一个“优秀”的性质是，除了最底层之外，每一层都是满的，这使得堆可以利用数组来表示（普通的一般的二叉树通常用链表作为基本容器表示），每一个结点对应数组中的一个元素。 如下图，是一个堆和数组的相互关系，可看做堆的初始化 对于给定的某个结点的下标 i，可以很容易的计算出这个结点的父结点、孩子结点的下标： Parent(i) = floor(i/2)，i 的父节点下标 Left(i) = 2i，i 的左子节点下标 Right(i) = 2i + 1，i 的右子节点下标 二叉堆具有以下性质： 父节点的键值总是大于或等于（小于或等于）任何一个子节点的键值。 每个节点的左右子树都是一个二叉堆（都是最大堆或最小堆）。 步骤 构造最大堆（Build_Max_Heap）自底向上：若数组下标范围为0~n，考虑到单独一个元素是大根堆，则从下标n/2开始的元素均为大根堆。于是只要从n/2-1开始(最后一个非叶子节点开始)，分别与左孩子和右孩子比较大小，如果最大，则不用调整，否则和孩子中的值最大的一个交换位置，若交换之后还比此节点的孩子要小，继续向下交换（这里是自顶向下） 。并向前依次构造大根堆，这样就能保证，构造到某个节点时，它的左右子树都已经是大根堆。 堆排序（HeapSort）：由于堆是用数组模拟的。得到一个最大根堆后，数组内部并不是有序的。因此需要将堆化数组有序化。思想是总是移除根节点（用最后一个元素来填补空缺），并做最大堆调整的递归运算。第一次将heap[0]与heap[n-1]交换，再对heap[0…n-2]做最大堆调整。第二次将heap[0]与heap[n-2]交换，再对heap[0…n-3]做最大堆调整。重复该操作直至heap[0]和heap[1]交换。由于每次都是将最大的数并入到后面的有序区间，故操作完后整个数组就是有序的了。 最大堆调整（Max_Heapify）：该方法是提供给上述两个过程调用的。目的是将堆的末端子节点作调整，使得子节点永远小于父节点 。 构造最大堆：先自底向上，再自顶向下 调整最大堆：交换之后，被交换的节点从顶向下调整，调完继续交换，依次递归。 源代码12345678910111213141516171819202122232425262728def heap_sort(ary): n = len(ary) first = int(n/2-1) #最后一个非叶子节点 for start in range(first,-1,-1): #构建最大堆 max_heapify(ary,start,n-1) # range(n-1,0,-1)因为0时不用和顶点自己交换 for end in range(n-1,0,-1): #堆排，将最大跟堆转换成有序数组， ary[end],ary[0] = ary[0], ary[end] #将根节点元素与最后叶子节点进行互换，取出最大根节点元素，对剩余节点重新构建最大堆 max_heapify(ary,0,end-1) #因为end上面取的是n-1，故而这里直接放end-1，相当于忽略了最后最大根节点元素ary[n-1] return ary#最大堆调整：将堆的末端子节点作调整，使得子节点永远小于父节点#start为当前需要调整最大堆的位置，end为调整边界def max_heapify(ary,start,end): root = start while True: # 记住这一句 child = root * 2 + 1 #调整节点的子节点,这里要注意数值下标从0开始，左节点为root * 2 + 1，这里都是下标表示 if child &gt; end: # 左子树超过边界，右子树肯定也超了 break if child + 1 &lt;= end and ary[child] &lt; ary[child+1]: #两个都没超，选数值大的下标 child = child + 1 #取较大的子节点 # 满足父节点比子节点小才叫交换 if ary[root] &lt; ary[child]: # 子节点成为父节点；child为左子树或上一步较大的子节点 ary[root], ary[child] = ary[child], ary[root] #交换 root = child # 调整时候要自顶向下继续调整，不要忘了这一句 else: break 时空复杂度总结 时间复杂度 ＂快些以O(nlog2n)O(nlog2n)的速度归队＂ 即快，希，归，堆都是O(nlog2n)O(nlog2n)，其他都是O(n2)O(n2)，基数排序例外，是O(d(n+rd))O(d(n+rd)) 空间复杂度 快排O(log2n)O(log2n) 归并O(n)O(n) 基数O(rd)O(rd) 其他O(1)O(1) 稳定性 ＂心情不稳定，快些找一堆朋友聊天吧＂ 即不稳定的有：快，希，堆 其他性质 直接插入排序，初始基本有序情况下，是O(n)O(n) 冒泡排序，初始基本有序情况下，是O(n)O(n) 快排在初始状态越差的情况下算法效果越好． 堆排序适合记录数量比较大的时候，从n个记录中选择k个记录． 经过一趟排序，元素可以在它最终的位置的有：交换类的（冒泡，快排），选择类的（简单选择，堆） 比较次数与初始序列无关的是：简单选择与折半插入 排序趟数与原始序列有关的是：交换类的（冒泡和快排）]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>经典排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer解题_Python版]]></title>
    <url>%2F2018%2F09%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%E8%A7%A3%E9%A2%98_Python%E7%89%88%2F</url>
    <content type="text"><![CDATA[1.二维数组中的查找题目： 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 思路：遍历每一行，查找该元素是否在该行之中。 123456789# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here for i in array: if target in i: return True return False 2.替换空格题目： 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 思路：利用字符串中的replace直接替换即可。 12345# -*- coding:utf-8 -*-class Solution: # s 源字符串 def replaceSpace(self, s): return s.replace(' ','%20') 3.从尾到头打印链表题目：输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 思路：将链表中的值记录到list之中，然后进行翻转list。 1234567891011121314# -*- coding:utf-8 -*-class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): l = list() while listNode: l.append(listNode.val) listNode=listNode.next return l[::-1] 4.重建二叉树(flag)题目：输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 题解：首先前序遍历的第一个元素为二叉树的根结点，那么便能够在中序遍历之中找到根节点，那么在根结点左侧则是左子树；在根结点右侧，便是右子树。然后在递归遍历左子树和右子树。这里要注意一点，pre的左右子树分割长度与中序的左右子树分割长度一致。 12345678910111213141516171819# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回构造的TreeNode根节点 def reConstructBinaryTree(self, pre, tin): if len(pre)==0: return None if len(pre)==1: return TreeNode(pre[0]) else: flag = TreeNode(pre[0]) #pre的左右子树分割长度与中序的左右子树分割长度一致。 flag.left = self.reConstructBinaryTree(pre[1:tin.index(pre[0])+1],tin[:tin.index(pre[0])]) flag.right = self.reConstructBinaryTree(pre[tin.index(pre[0])+1:],tin[tin.index(pre[0])+1:]) return flag 5.用两个栈实现队列(flag)题目：用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 题解：申请两个栈Stack1和Stack2，Stack1当作输入，Stack2当作pop。当Stack2空的时候，将Stack1进行反转，并且输入到Stack2。 123456789101112131415# -*- coding:utf-8 -*-class Solution: def __init__(self): self.Stack1=[] self.Stack2=[] def push(self, node): # write code here self.Stack1.append(node) def pop(self): # return xx if self.Stack2==[]: while self.Stack1: self.Stack2.append(self.Stack1.pop()) return self.Stack2.pop() return self.Stack2.pop() 6.旋转数组的最小数字题目：把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 1234567# -*- coding:utf-8 -*-class Solution: def minNumberInRotateArray(self, rotateArray): if not rotateArray: return 0 else: return min(rotateArray) 7.斐波那契数列(flag)题目：大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39。 1234567891011# -*- coding:utf-8 -*-class Solution: def Fibonacci(self, n): if n==0: return 0 if n==1 or n==2: return 1 a,b=1,1 for i in range(n-2): a,b=b,a+b return b 8.跳台阶题目：一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 题解：ans[n]=ans[n-1]+ans[n-2] 12345678910111213# -*- coding:utf-8 -*-class Solution: def jumpFloor(self, number): if not number: return 0 if number==1: return 1 if number==2: return 2 a,b=1,2 for i in range(number-2): a,b=b,a+b return b 9.变态跳台阶题目：一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 题解：ans[n]=ans[n-1]+ans[n-2]+ans[n-3]+…+ans[n-n]，ans[n-1]=ans[n-2]+ans[n-3]+…+ans[n-n]，ans[n]=2*ans[n-1]。 123456789class Solution: def jumpFloorII(self, number): if not number: return 0 if number==1: return 1 if number==2: return 2 return 2**(number-1) 10.矩形覆盖题目：我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 题解：新增加的小矩阵竖着放，则方法与n-1时相同，新增加的小矩阵横着放，则方法与n-2时相同，于是f(n)=f(n-1)+f(n-2)。 1234567891011121314# -*- coding:utf-8 -*-class Solution: def rectCover(self, number): # write code here if not number: return 0 if number==1: return 1 if number==2: return 2 a,b=1,2 for _ in range(number-2): a,b=b,a+b return b 11.二进制中1的个数(flag)题目：输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 题解：每次进行右移一位，然后与1进行相与，如果是1则进行加1。 123456789101112131415161718192021# 向右移1位可以看成除以2，向左移一位可以看成乘以2。移动n位可以看成乘以或者除以2的n次方。# 负数原码（int整型用32位表示）所有位取反码然后+1得到补码；正数的补码为其自身class Solution: #转为字符串 def NumberOf1(self, n): # write code here if n==0: return 0 if n&gt;0: a=bin(n).count('1') return a else: a=bin(2**32+n).count('1') return a class Solution: #每次移一位，看此为是否为1，负数的表示内部已经是补码了 def NumberOf1(self, n): # write code here count = 0 for i in range(32): count += (n &gt;&gt; i) &amp; 1 return count 12.数值的整次方题目：给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 1234567891011# -*- coding:utf-8 -*-class Solution: def Power(self, base, exponent): # write code here ans=1 for i in range(0,abs(exponent)): ans=ans*base if exponent&gt;0: return ans else: return 1/ans 13.调整数组顺序使奇数位于偶数前面题目：输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 题解：申请奇数数组和偶数数组，分别存放奇数值和偶数值，数组相加便为结果。 123456789101112# -*- coding:utf-8 -*-class Solution: def reOrderArray(self, array): array=list(array) a=[] b=[] for i in array: if i%2==1: a.append(i) else: b.append(i) return a+b 14.链表中倒数第K个节点题目：输入一个链表，输出该链表中倒数第k个结点。 题解：快慢指针。 1234567891011121314151617181920212223# -*- coding:utf-8 -*-class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def FindKthToTail(self, head, k): l=[] count=0 if not head: return None a1=head for i in range(k): if not a1: return None else: a1=a1.next a2=head while a1: a1=a1.next a2=a2.next return a2 15.反转链表(flag)题目：输入一个链表，反转链表后，输出新链表的表头。 123456789101112131415161718192021# -*- coding:utf-8 -*-class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead: return None if not pHead.next: return pHead Pre=None Next=None while pHead: Next=pHead.next # 暂存当前节点的下一个节点信息 pHead.next=Pre # 断开链表, 反转节点, 这两句都是为了保护链表断开不丢失next的指向 Pre=pHead pHead=Next return Pre 16.合并两个排序的列表题目：输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 1234567891011121314151617181920212223242526272829303132333435# -*- coding:utf-8 -*-class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if not pHead1 and not pHead2: return None if not pHead1 or not pHead2: if not pHead1: return pHead2 else: return pHead1 merge=ListNode(0)# 新一个头结点数值为x的链表 p=merge #返回时的指向头结点的指针 while pHead1 and pHead2: if pHead1.val&lt;=pHead2.val: merge.next=pHead1 pHead1=pHead1.next else: merge.next=pHead2 pHead2=pHead2.next merge=merge.next while pHead1: merge.next=pHead1 pHead1=pHead1.next merge=merge.next while pHead2: merge.next=pHead2 pHead2=pHead2.next merge=merge.next return p.next 17.树的子结构(flag)题目：输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）。 题解：递归；或者将树转变为中序序列，然后转变为str类型，最后判断是否包含。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if not pRoot1 or not pRoot2: return False root1 = pRoot1 root2 = pRoot2 result = False if root1.val==root2.val: # HasSubtree条件出口，满足根节点相同才继续判断子树结构 result = self.hastree(root1,root2) if not result: result = self.HasSubtree(root1.left,root2) if not result: result = self.HasSubtree(root1.right,root2) return result def hastree(self,root1,root2): if root2==None: return True if root1==None: return False if root1.val==root2.val: return self.hastree(root1.left,root2.left) and self.hastree(root1.right,root2.right) if root1.val!=root2.val: return False 18.二叉树的镜像题目： 操作给定的二叉树，将其变换为源二叉树的镜像。 12345678910111213# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回镜像树的根节点 def Mirror(self, root): if root: root.left,root.right=root.right,root.left self.Mirror(root.left) self.Mirror(root.right) 19.顺时针打印矩阵(flag)题目: 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-class Solution: # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): rows=len(matrix) cols=len(matrix[0]) l=[] if rows==1 and cols==1: return [matrix[0][0]] left,right, up, down = 0,cols-1,0,rows-1 #这个是数组下标 while left&lt;=right and up&lt;=down: for i in range(left,right+1): #range函数接收参数从小到大，大的数值不计入 l.append(matrix[up][i]) for j in range(up+1,down+1): l.append(matrix[j][right]) if down-up&gt;=1: #相等时为剩余单行 for k in range(left,right)[::-1]: #这里注意逆序的数组下标 l.append(matrix[down][k]) if right-left&gt;=1: #相等时为剩余单列 for p in range(up+1,down)[::-1]: l.append(matrix[p][left]) up=up+1 down=down-1 left=left+1 right=right-1 return l 20.包含Min函数的栈(flag)题目：定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数。 1234567891011121314151617181920212223# -*- coding:utf-8 -*-class Solution: def __init__(self): self.stack=[] self.minstack=[] #pop()删除列表的最后一个元素,[-1]获取最后一个元素 def push(self, node): #最小栈存储 整个原栈的最小元素，若最小元素在原栈删除，则也要删除最小栈 if not self.minstack or node&lt;self.minstack[-1]: self.minstack.append(node) self.stack.append(node) def pop(self): if not self.stack: return None elif self.stack[-1]==self.minstack[-1]: self.minstack.pop() else: self.stack.pop() # write code here def top(self): return self.stack[-1] # write code here def min(self): return self.minstack[-1] # write code here 21.栈的压入弹出序列(flag)题目：输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）。 题解：构建压入和活动栈，只有处于压入栈栈顶或者活动栈内才可弹出；或者新构建一个中间栈，来模拟栈的输入和栈的输出，比对输入结果和输出结果是否相等。 123456789101112131415161718192021# -*- coding:utf-8 -*-class Solution: def IsPopOrder(self, pushV, popV): if not pushV: return None fixed=[] #压下去的辅助栈，处于栈顶可以出栈 left=pushV[:] #剩余的可活动栈，p元素位置之前的都要压入辅助栈 flag=True for p in popV: if p not in pushV: #还要判断pushV和popV元素不同的情况 return False if p in left: k=left.index(p) fixed=fixed+left[:k+1] left=left[k+1:] fixed.pop() elif fixed: #避免fixed[-1]越界 if p!=fixed[-1]: return False else:fixed.pop() return flag 22.从上往下打印二叉树题目：从上往下打印出二叉树的每个节点，同层节点从左至右打印。 思路：层次遍历，用队列。 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回从上到下每个节点值列表，例：[1,2,3] def PrintFromTopToBottom(self, root): # write code here res=[] value=[] if not root: return [] #返回空列表，而不是None else: res.append(root) value.append(root.val) while res: p=res.pop(0) # pop(0)才是第一个元素 if p.left: res.append(p.left) value.append(p.left.val) if p.right: res.append(p.right) value.append(p.right.val) return value 23.二叉树的后续遍历序列(flag)题目：输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 思路：二叉搜索树的特性是所有左子树值都小于中节点，所有右子树的值都大于中节点，递归遍历左子树和右子树的值。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# 后续遍历要满足 去除序列最后一个元素（根）后，将小于和大于这个元素直接分成两段，递归class Solution: def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False if len(sequence)==1: return True front=[] back=[] flag=0 # 辅助与分段的标记，第一个大于p元素之后的都放在back断 p=sequence.pop() for i in sequence: if i&lt;=p and flag==0: front.append(i) else: flag=1 back.append(i) if front and max(front)&gt;p: #front要满足所有元素都小于p return False if back and min(back)&lt;p: #back要满足所有元素都大于p return False LEFT=True #递归出口 RIGHT=True if front: LEFT=self.VerifySquenceOfBST(front) if back: RIGHT=self.VerifySquenceOfBST(back) return LEFT and RIGHT 24.二叉树中和为某一值的路径(flag)题目：输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前)。 12345678910111213141516171819202122232425262728# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回二维列表，内部每个列表表示找到的路径 def FindPath(self, root, expectNumber): # write code here # 这题不太会，记一下 if not root:#即是一开始条件语句，也是递归出口，若叶子节点不满足条件，left或者right返回空 return [] if root and not root.left and not root.right and root.val == expectNumber: return [[root.val]] #递归出口，即叶子节点满足条件，不会执行以下任何语句； res = [] #每次清空 # 先会一直先递归left，相当于深度优先搜索 left = self.FindPath(root.left, expectNumber-root.val) #left_left1_left11_right11_...right1 right = self.FindPath(root.right, expectNumber-root.val) # 遍历后的操作 for i in left+right: #是指里面的元素，[[root.val]]得到的元素是[root.val] res.append([root.val]+i) return res # 6 左图所示，left返回[[3]],right返回的res为[].append([2]+[1])=[[2,1]] # 3 2 最后一层返回[[6,3],[6,2,1]] # 1 25.复杂链表的复制(flag)题目：输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）。 思路： 1、遍历链表，复制每个结点，如复制结点A得到A1，将结点A1插到结点A后面（暂不处理随机节点）； 2、重新遍历链表，复制老结点的随机指针给新结点，如A1.random = A.random.next; 3、拆分链表，将链表拆分为原链表和复制后的链表 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-class RandomListNode: def __init__(self, x): self.label = x self.next = None self.random = Noneclass Solution: # 返回 RandomListNode def Clone(self, pHead): # write code here if not pHead: return pCur = pHead while pCur: node = RandomListNode(pCur.label) node.next = pCur.next pCur.next = node pCur = node.next pCur = pHead while pCur: if pCur.random: pCur.next.random = pCur.random pCur = pCur.next.next pCur = pHead cur = pHead.next h = cur while cur: #拆分 pCur.next = cur.next if pCur.next: cur.next = pCur.next.next pCur = pCur.next cur = cur.next return h 26.二叉搜索树与双向列表题目：输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 思路：中序遍历，然后添加一个pre指针 123456789101112131415161718192021# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def __init__(self): self.a=[] def Convert(self, pRootOfTree): # write code here if pRootOfTree==None: return self.Convert(pRootOfTree.left) self.a.append(pRootOfTree) self.Convert(pRootOfTree.right) for i in range(len(self.a)-1): self.a[i].right=self.a[i+1] self.a[i+1].left=self.a[i] return self.a[0] 27.字符串的排列(flag)题目：输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 输入：输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。 思路：用itertools.permutations;或者通过将固定每一位的字符，然后进行和后面的每个字符进行交换，得到所有结果集。 1234567891011# -*- coding:utf-8 -*-import itertoolsclass Solution: def Permutation(self, ss): if not ss: return [] return sorted(list(set(map(''.join, itertools.permutations(ss))))) # abc # itertools.permutations(ss)输出('a','b','c') ('a','c','b')...等 # map函数后为abc acb.. 28.数组中出现次数超过一般的数字题目：数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0 题解：判断是否有超过一半的元素，如果有则在数组中间的元素 1234567891011121314151617181920212223# -*- coding:utf-8 -*-class Solution: def MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 if len(numbers)==1: return numbers[0] l=len(numbers)//2 k=sorted(list(numbers)) #排序，满足条件处于中间位置的为最多元素 count=0 #不满足条件则遍历判断 此元素maxcount是否超过一半 maxcount=0 for i in range(len(k)-1): if k[i]==k[i+1]: count=count+1 if count&gt;maxcount: maxcount=count else: count=0 if maxcount&gt;=l: return k[l] else: return 0 29.最小的K个数题目：输入n个整数，找出其中最小的K个数，例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4。 12345678# -*- coding:utf-8 -*-class Solution: def GetLeastNumbers_Solution(self, tinput, k): # write code here if len(tinput)&lt;k: return [] res=sorted(tinput) return res[:k] 30.连续子数组的最大和(flag)题目：HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。你会不会被他忽悠住？(子向量的长度至少是1) 12345678910111213141516171819202122232425262728293031323334353637383940# -*- coding:utf-8 -*-class Solution: def FindGreatestSumOfSubArray(self, array): # write code here 只需要返回最大数，下标不管 # 思路：创建一个列表储存要加入的元素，当累积和小于0则清空前面一段 # 另外一个更简单的思路PART2 if len(array)==1: return array[0] if max(array)&lt;=0: #全是负数 return max(array) if min(array)&gt;=0: return sum(array) cum_max=0 maxnum=0 res=[] for k, i in enumerate(array): if cum_max+i&gt;=0 and max(array[k:])&gt;0: # 注意一点，当前数组后面全是负数，不能再加了 res.append(i) cum_max=cum_max+i else: #curmax=0 #此时可以记下标，这里不要求 cum_max=0 if sum(res)&gt;maxnum: maxnum=sum(res) res=[] return max(maxnum,sum(res)) #sum(res)是最后数组res一直加入元素而没有更新maxnum # ======================part2：========================== # class Solution: # def FindGreatestSumOfSubArray(self, array): # maxnum= float(-inf) # cum_max= 0 # for i in array: # if cum_max+i&lt;0: # cum_max=i #这一句是精华，什么都不作处理，下一个值当做cum_max # else: # cum_max=cum_max+i # if cum_max&gt;maxnum: # maxnum=cum_max # return maxnum 31.整数中1出现的次数题目：求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数） 12345678# -*- coding:utf-8 -*-class Solution: def NumberOf1Between1AndN_Solution(self, n): # write code here count=0 for i in range(1+n): count=count+str(i).count('1') return count 32.把数组排成最小的数(flag)题目：输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 思路：将数组转换成字符串之后，进行两两比较字符串的大小，比如3,32的大小由332和323确定，即3+32和32+3确定。 12345678910# -*- coding:utf-8 -*-class Solution: def PrintMinNumber(self, numbers): # write code here if not numbers: return '' cmp_def = lambda x1,x2: int(str(x1)+str(x2))-int(str(x2)+str(x1)) a=sorted(numbers,cmp=cmp_def) #sortef创建副本，sort原地 b=list(map(lambda x:str(x),a)) #列表数字转字符串 return ''.join(b) #jion反馈一个字符串 33.丑数(flag)题目：把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 思路：每一个丑数必然是由之前的某个丑数与2，3或5的乘积得到的，这样下一个丑数就用之前的丑数分别乘以2，3，5，找出这三这种最小的并且大于当前最大丑数的值，即为下一个要求的丑数。 12345678910111213141516171819202122# -*- coding:utf-8 -*-class Solution: def GetUglyNumber_Solution(self, index): # write code herei # 思路，创建一个数组存储丑数；创建三个丑数数组独立下标对应乘以235，最小为当前丑数 if index==0: return 0 if index==1: return 1 uglyarr=[1] a2,a3,a5=0,0,0 for i in range(index-1): n1,n2,n3=uglyarr[a2]*2,uglyarr[a3]*3,uglyarr[a5]*5 min_num=min(n1,n2,n3) uglyarr.append(min_num) if min_num==n1: a2=a2+1 if min_num==n2: a3=a3+1 if min_num==n3: a5=a5+1 return uglyarr[-1] 34.第一个只出现一次的字符题目：在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1。 123456789# -*- coding:utf-8 -*-class Solution: def FirstNotRepeatingChar(self, s): # write code here ss=list(s) for i,e in enumerate(ss): if s.count(e)==1: return i return -1 35.数组中的逆序对(Flag)题目描述：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007。 输入描述：题目保证输入的数组中没有的相同的数字。 12345678910111213141516171819202122232425262728293031323334353637383940414243'''/*归并排序的改进，把数据分成前后两个数组(递归分到每个数组仅有一个数据项)，合并数组，合并时，出现前面的数组值array[i]大于后面数组值array[j]时；则前面数组array[i]~array[mid]都是大于array[j]的，count += mid+1 - i参考剑指Offer，但是感觉剑指Offer归并过程少了一步拷贝过程。还有就是测试用例输出结果比较大，对每次返回的count mod(1000000007)求余*/'''# -*- coding:utf-8 -*-class Solution: def InversePairs(self, data): # write code here return self.inverseCount(data[:], 0, len(data)-1, data[:])%1000000007 def inverseCount(self, tmp, start, end, data): if end-start &lt;1: return 0 if end - start == 1: if data[start]&lt;=data[end]: return 0 mid = (start+end)//2 cnt = self.inverseCount(data, start, mid, tmp) + self.inverseCount(data, mid+1, end, tmp) # print(start, mid, end, cnt, data) i = start j = mid + 1 ind = start # 用于tmp的下标 while(i &lt;= mid and j &lt;= end): # tmp排序 if data[i] &lt;= data[j]: tmp[ind] = data[i] i += 1 else: tmp[ind] = data[j] cnt += mid - i + 1 j += 1 ind += 1 while(i&lt;=mid): tmp[ind] = data[i] i += 1 ind += 1 while(j&lt;=end): tmp[ind] = data[j] j += 1 ind += 1 return cnt 36.两个链表的第一个公共节点题目：输入两个链表，找出它们的第一个公共结点。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None#有些题的输入指针，没有val属性，只有nextclass Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None while pHead1: p2=pHead2 while p2: if pHead1==p2: return pHead1 p2=p2.next pHead1=pHead1.next return None 37.数字在排序数组中出现的次数题目：统计一个数字在排序数组中出现的次数。 123456789101112131415# -*- coding:utf-8 -*-class Solution: def GetNumberOfK(self, data, k): # write code here count=0 flag=1 if not data: return 0 for i in data: if i==k: count=count+1 flag=1 #设置处在相同阶段的标志 if count&gt;0 and flag!=1: #不在相同阶段就break break return count 38.二叉树的深度(flag)题目：输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 1234567891011121314# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def TreeDepth(self, pRoot): # write code here if not pRoot: return 0 left=self.TreeDepth(pRoot.left) #操作放在后面，想象后序遍历 right=self.TreeDepth(pRoot.right) return max(left,right)+1 39.平衡二叉树(flag)题目：输入一棵二叉树，判断该二叉树是否是平衡二叉树。 题解：平衡二叉树是左右子数的距离不能大于1，因此递归左右子树，判断子树距离是否大于1。用Maxdeep递归求深度 123456789101112131415161718192021# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def IsBalanced_Solution(self, pRoot): # write code here # 递归且有一个实现求深度的递归函数 if not pRoot: return True if abs(self.Maxdeep(pRoot.left)-self.Maxdeep(pRoot.right))&gt;1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) def Maxdeep(self,pRoot): if not pRoot: return 0 left = self.Maxdeep(pRoot.left) right = self.Maxdeep(pRoot.right) return max(left,right)+1 40.数组中只出现一次的数字题目：一个整型数组里除了两个数字之外，其他的数字都出现了偶数次。请写程序找出这两个只出现一次的数字。 题解：转为字符串；或者将数组中数转到set之中，然后利用dict存储每个数字出现的次数。 1234567891011121314151617181920212223242526272829303132333435# -*- coding:utf-8 -*-class Solution: # 返回[a,b] 其中ab是出现一次的两个数字 def FindNumsAppearOnce(self, array): # write code here if not array: return None if len(array)==1: return array[0] l=[] arr_str_list=list(map(lambda x:str(x),array)) arr_str=''.join(arr_str_list) #输入要为字符列表 for i,e in enumerate(arr_str_list): if arr_str.count(e)==1: l.append(i) ll=[] for i in l: ll.append(array[i]) return ll# ====================================class Solution: # 返回[a,b] 其中ab是出现一次的两个数字 def FindNumsAppearOnce(self, array): # write code here arrayset=set(array) dict=&#123;&#125; for num in arrayset: dict[num]=0 for i in range(0,len(array)): dict[array[i]]=dict[array[i]]+1 ans=[] for num in arrayset: if dict[num]==1: ans.append(num) return ans 41.和为S的连续正整数序列题目：小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck! 输出描述：输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序。 1234567891011121314151617181920# -*- coding:utf-8 -*-class Solution: def FindContinuousSequence(self, tsum): # write code here #若是100,只需要从1：50找就可以了，+2是考虑到下标0开始，让搜索范围大一些 bitsum=tsum//2+2 ori=list(range(1,bitsum)) res=[] for i in range(1,bitsum): l=[] summ=i #累积总和 for j in range(i+1,bitsum): summ=summ+j if summ==tsum: l=ori[i-1:j] #i,j看做下标返回满足条件的数组 if summ&gt;tsum: break if l: res.append(l) return res 42.和为S的两个数字(flag)题目：输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 输出描述：对应每个测试案例，输出两个数，小的先输出 12345678910111213141516171819202122232425# -*- coding:utf-8 -*-class Solution: def FindNumbersWithSum(self, array, tsum): # write code here # 返回列表类型的，若不满足条件则[] if len(array)&lt;=1 or min(array)&gt;tsum: return [] res=[] for i in range(len(array)): # 都是索引下标循环 for j in range(i+1,len(array)): if array[i]+array[j]==tsum: # 从头开始搜索这时候得到的应该是乘积最小的，可以直接退出外层循环 res.append(array[i]) res.append(array[j]) break if array[j]&gt;tsum: break else: # 上面循环正常结束才会执行，若上面循环执行break则这条语句不执行 continue break if res: return res else: return [] 43.左旋字符子串题目：汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 123456789# -*- coding:utf-8 -*-class Solution: def LeftRotateString(self, s, n): # write code here if len(s)&lt;n or not s: return '' if n==0: return s return s[n:]+s[:n] 44.反转单词顺序(flag)题目：牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 123456# -*- coding:utf-8 -*-class Solution: def ReverseSentence(self, s): # write code here ss=s.split(' ') return ' '.join(ss[::-1]) 45.扑克牌顺序题目：LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\小王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 1234567891011121314151617181920# -*- coding:utf-8 -*-class Solution: def IsContinuous(self, numbers): # write code here # 只需要判断没有0的数组里数值是唯一且max-min&lt;5即可 if len(numbers)&lt;5 and not numbers: return False grost_num=0 without_gro=[] for i in numbers: if i==0: grost_num+=1 elif i in without_gro and i!=0: #唯一 return False else: without_gro.append(i) if max(without_gro)-min(without_gro)&lt;5: return True else: return False 46.孩子们的圈圈(圈圈中最后剩下的数)(flag)题目：每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1)。 1234567891011121314# -*- coding:utf-8 -*-class Solution: def LastRemaining_Solution(self, n, m): # write code here # flag=0 # 把index想象为连续拼接数组 if not n or not m: return -1 num = list(range(n)) index = 0 while len(num)&gt;1: # 剩余两个都要继续执行 index=(index+m-1)%len(num) #index为上一次停的地方，加上m-1为重新第m-1个出列 num.pop(index) return num[0] 47.求1+2+3+…+n题目：求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 123456789# -*- coding:utf-8 -*-class Solution: def Sum_Solution(self, n): # write code here if not n: return None if n==1: return 1 return n+self.Sum_Solution(n-1) 48.不用加减乘除做加法题目：写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 思路：二进制异或进位。 12345678910111213141516171819202122232425262728293031# -*- coding:utf-8 -*-class Solution: def Add(self, num1, num2): # write code here # 位操作不懂 return sum([num1,num2]) # ============================'''首先看十进制是如何做的： 5+7=12，三步走第一步：相加各位的值，不算进位，得到2。第二步：计算进位值，得到10. 如果这一步的进位值为0，那么第一步得到的值就是最终结果。第三步：重复上述两步，只是相加的值变成上述两步的得到的结果2和10，得到12。同样我们可以用三步走的方式计算二进制值相加： 5-101，7-111 第一步：相加各位的值，不算进位，得到010，二进制每位相加就相当于各位做异或操作，101^111。第二步：计算进位值，得到1010，相当于各位做与操作得到101，再向左移一位得到1010，(101&amp;111)&lt;&lt;1。第三步重复上述两步， 各位相加 010^1010=1000，进位值为100=(010&amp;1010)&lt;&lt;1。 继续重复上述两步：1000^100 = 1100，进位值为0，跳出循环，1100为最终结果。'''class Solution: def Add(self, num1, num2): # write code here while num2!=0: sum=num1^num2 carry=(num1&amp;num2)&lt;&lt;1 num1=sum num2=carry return num1 49.把字符串转换成整数题目：将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 输入描述：输入一个字符串,包括数字字母符号,可以为空输出描述:如果是合法的数值表达则返回该数字，否则返回0。 1234567891011# -*- coding:utf-8 -*-class Solution: def StrToInt(self, s): # write code here l=[s] try: num=list(map(eval,l)) except Exception as e: return 0 else: return num[0] 50.数组中重复的数字题目：在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 思路：利用dict计算重复数字。 1234567891011# -*- coding:utf-8 -*-class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False def duplicate(self, numbers, duplication): # write code here for i in numbers: if numbers.count(i)&gt;1: duplication[0]=i return True return False 51.构建乘积数组题目:给定数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1];其中B中的元素B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]。不能使用除法。 注意：没有A[i] 12345678910111213141516# -*- coding:utf-8 -*-class Solution: def multiply(self, A): # write code here if not A: return None if len(A)==1: return None B=[None]*len(A) for i,e in enumerate(A): cumpower=1 for ii,ee in enumerate(A): if ii!=i: cumpower=cumpower*ee B[i]=cumpower return B 52.正则表达式匹配(flag)题目：请实现一个函数用来匹配包括’ , ‘和’ * ‘的正则表达式。模式中的字符’ , ‘表示任意一个字符，而’ * ‘表示它前面的字符可以出现任意次（包含0次）。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配。 思路: 当模式中的第二个字符不是 *时： 如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的。 如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回false。 当模式中的第二个字符是 *时： 如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。 如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式。 模式后移2字符，相当于 x被忽略。即模式串中与他前面的字符和字符串匹配0次。 字符串后移1字符，模式后移2字符。即模式串中*与他前面的字符和字符串匹配1次。 字符串后移1字符，模式不变，即继续匹配字符下一位，因为 可以匹配多位。即模式串中与他前面的字符和字符串匹配多次。 123456789101112131415161718# -*- coding:utf-8 -*-class Solution: # s, pattern都是字符串 # flag=0 def match(self, s, pattern): # write code here if (len(s) == 0 and len(pattern) == 0): return True if (len(s) &gt; 0 and len(pattern) == 0): return False if (len(pattern) &gt; 1 and pattern[1] == '*'): if (len(s) &gt; 0 and (s[0] == pattern[0] or pattern[0] == '.')): return (self.match(s, pattern[2:]) or self.match(s[1:], pattern[2:]) or self.match(s[1:], pattern)) else: return self.match(s, pattern[2:]) if (len(s) &gt; 0 and (pattern[0] == '.' or pattern[0] == s[0])): return self.match(s[1:], pattern[1:]) return False 53.表示数值的字符串题目：请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 1234567891011# -*- coding:utf-8 -*-class Solution: # s字符串 def isNumeric(self, s): # write code here l=[s] try: b=float(s) return True except Exception as e: return False 54.字符流中第一个不重复的字符(flag)题目：请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 输出描述：如果当前字符流没有存在出现一次的字符，返回#字符。 1234567891011121314151617181920# -*- coding:utf-8 -*-# flag=0class Solution: # 返回对应char def __init__(self): self.s='' self.dict=&#123;&#125; def FirstAppearingOnce(self): for i in self.s: if self.dict[i]==1: return i return '#' # write code here def Insert(self, char): # write code here self.s+=char if char in self.dict: self.dict[char]+=1 else: self.dict[char]=1 55.链表中环的入口节点(flag)题目：给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 思路：第一步，找环中相汇点。分别用p1，p2指向链表头部，p1每次走一步，p2每次走二步，直到p1==p2找到在环中的相汇点。第二步，找环的入口。接上步，当p1==p2时，p2所经过节点数为2x,p1所经过节点数为x,设环中有n个节点,p2比p1多走一圈有2x=n+x;n=x;可以看出p1实际走了一个环的步数，再让p2指向链表头部，p1位置不变，p1,p2每次走一步直到p1==p2; 此时p1指向环的入口：直线+小段环=整环，故p1再走整环-小段环到达起点。 12345678910111213141516171819202122# -*- coding:utf-8 -*-class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def EntryNodeOfLoop(self, pHead): # write code here fast,slow=pHead,pHead if fast and fast.next: fast=fast.next.next slow=slow.next while fast and fast.next and fast!=slow: fast=fast.next.next slow=slow.next if not fast.next: return None fast=pHead while fast!=slow: fast=fast.next slow=slow.next return fast 56.删除链表中重复的节点(flag)题目：在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留(全部删除)，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5。 12345678910111213141516171819202122232425# -*- coding:utf-8 -*-class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here result = ListNode(0) res = result tmp = pHead while tmp and tmp.next: if tmp.val == tmp.next.val: while tmp.next and tmp.val == tmp.next.val: tmp = tmp.next else: res.next = tmp #把整个tmp之后的链表都接上去了 res = res.next tmp = tmp.next res.next = tmp return result.next # 一开始res为&#123;-1&#125;，1和2不同，res.next = tmp得到&#123;-1，1,2,3,3,4,4,5&#125;，res和tmp指针往下# 第二次2和3不同，res.next = tmp得到&#123;-1，1，+，2,3,3,4,4,5&#125;，+号前为res指针位置# 第三次3和3相同，tmp指针到4的位置，下一次res.next=tmp得到&#123;-1,1,2，+，4,4,5&#125; 57. 二叉树中的下一个节点题目：给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 思路：分析二叉树的下一个节点，一共有以下情况：1.二叉树为空，则返回空；2.节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点；3.节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复之前的判断，返回结果。 12345678910111213141516171819202122232425# -*- coding:utf-8 -*-class TreeLinkNode: def __init__(self, x): self.val = x self.left = None self.right = None self.next = Noneclass Solution: def GetNext(self, pNode): # write code here if not pNode: return None if pNode.right: pNode=pNode.right while pNode.left: pNode=pNode.left return pNode else: #存在该节点在父节点右边，且一直递归，这时要找爷爷节点 型如"\" while pNode.next: if pNode.next.left==pNode: return pNode.next else: pNode=pNode.next return None 58.对称的二叉树(flag)题目：请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 思路：采用递归的方法来判断两数是否相同。 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None# 设计一个递归求issame的函数,issame(root1.left,root2.right)class Solution: def isSymmetrical(self, pRoot): if not pRoot: return True if not pRoot.left and not pRoot.right: return True if pRoot.left and pRoot.right: return self.issame(pRoot.left,pRoot.right) else: return False # write code here def issame(self, root1, root2): if not root1 and not root2: return True if root1 and root2: if root1.val==root2.val: return self.issame(root1.left,root2.right) and self.issame(root2.left,root1.right) else: return False 59.按之字形顺序打印二叉树(flag)题目：请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 123456789101112131415161718192021222324252627282930313233# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def Print(self, pRoot): # write code here res=[] if not pRoot: return [] level=[pRoot] reverseflag=False while level: levelvalue=[] nextlevel=[] for i in level: levelvalue.append(i.val) if i.left: nextlevel.append(i.left) if i.right: nextlevel.append(i.right) if reverseflag: levelvalue.reverse() if levelvalue: res.append(levelvalue) # for j in levelvalue: # res.append(j) reverseflag = not reverseflag level=nextlevel return res 60.把二叉树打印成多行题目：从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 12345678910111213141516171819202122232425262728# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Nonefrom collections import dequeclass Solution: # 返回二维列表[[1,2],[4,5]] def Print(self, pRoot): # write code here if not pRoot: return [] res=[] level = [pRoot] while level: levelvalue=[] nextlevel=[] for i in level: levelvalue.append(i.val) if i.left: nextlevel.append(i.left) if i.right: nextlevel.append(i.right) if levelvalue: res.append(levelvalue) level = nextlevel return res 61.序列化二叉树(flag)题目：请实现两个函数，分别用来序列化和反序列化二叉树。 思路：转变成前序遍历，空元素利用”#”代替，然后进行解序列。 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None#序列化指的是遍历二叉树为字符串；所谓反序列化指的是依据字符串重新构造成二叉树。#当在遍历二叉树时碰到Null指针时，这些Null指针被序列化为一个特殊的字符“#”,结点之间的数值用逗号隔开。class Solution: def Serialize(self, root): def Pre_Order(root): if root: result.append(str(root.val)) Pre_Order(root.left) Pre_Order(root.right) else: result.append('#') result = [] Pre_Order(root) return ','.join(result) # write code here def Deserialize(self, s): s = s.split(',') def Change(num): num[0] += 1 if num[0] &lt; len(s): if s[num[0]] == '#': return None root = TreeNode(int(s[num[0]])) root.left = Change(num) root.right = Change(num) return root else: return None num = [-1] return Change(num) 62.二叉搜索树中的第K个节点题目：给定一棵二叉搜索树，请找出其中的第k小的结点。例如（5，3，7，2，4，6，8）中，按结点数值大小顺序第三小结点的值为4。 思路：中序遍历后，返回第K个节点值。 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回对应节点TreeNode def __init__(self): self.res = [] def KthNode(self, pRoot, k): # write code here if not pRoot: return None #pnode=pRoot # 为啥要加这一句呢？ self.sorttree(pRoot) if len(self.res)&lt;k or k&lt;=0: return None else: return self.res[k-1] def sorttree(self,pRoot): # 中序排序 if not pRoot: return [] left = self.sorttree(pRoot.left) self.res.append(pRoot) right = self.sorttree(pRoot.right) 63.数据流中的中位数题目：如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 1234567891011121314# -*- coding:utf-8 -*-class Solution: def __init__(self): self.arr=[] def Insert(self, num): # write code here self.arr.append(num) def GetMedian(self,arr): #为啥这里要加 arr 作为输入,换为其他参数也行，必须要占位？ # write code here res=sorted(self.arr) if len(res)%2==0: return (res[len(res)//2-1]+res[len(res)//2])/2.0 else: return res[len(res)//2] 64.滑动窗口的最大值题目：给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}，{2,3,4,2,6,[2,5,1]}。 123456789101112# -*- coding:utf-8 -*-class Solution: def maxInWindows(self, num, size): # write code here if size&lt;=0: return [] if size==1: return num res=[] for i in range(len(num)-size+1): res.append(max(num[i:i+size])) return res 65.矩阵中的路径(flag)题目：请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 思路：当起点第一个字符相同时，开始进行递归搜索，设计搜索函数。 123456789101112131415161718192021222324# -*- coding:utf-8 -*-class Solution: def hasPath(self, matrix, rows, cols, path): # write code here for i in range(rows): for j in range(cols): if matrix[i*cols+j]==path[0]: if self.findpath(list(matrix),rows,cols,path[1:],i,j): return True return False def findpath(self, matrix, rows, cols, path ,i ,j): if not path: return True matrix[i*cols+j]='0' if j+1&lt;cols and matrix[i*cols+j+1]==path[0]: #j+1&lt;cols 这里是下标，不能等于 return self.findpath(matrix, rows, cols, path[1:],i,j+1) elif j-1&gt;=0 and matrix[i*cols+j-1]==path[0]: return self.findpath(matrix, rows, cols, path[1:],i,j-1) elif i+1&lt;rows and matrix[(i+1)*cols+j]==path[0]: return self.findpath(matrix, rows, cols, path[1:],i+1,j) elif i-1&gt;=0 and matrix[(i-1)*cols+j]==path[0]: return self.findpath(matrix, rows, cols, path[1:],i-1,j) else: return False 66.机器人的运动范围题目：地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 123456789101112131415161718# -*- coding:utf-8 -*-class Solution: def movingCount(self, threshold, rows, cols): # write code here # 回溯的出口 ： 所有self.findgrid回溯出口的不满足if条件就停止 matrix = [[0 for i in range(cols)] for j in range(rows)] count = self.findgrid(threshold,rows,cols,matrix,0,0) return count def findgrid(self,threshold,rows,cols,matrix,i,j): count = 0 if 0&lt;=i&lt;rows and 0&lt;=j&lt;cols and (sum(map(int, str(i) + str(j))) &lt;= threshold) and matrix[i][j]==0: matrix[i][j] = 1 count = 1 + self.findgrid(threshold,rows,cols,matrix,i,j-1)\ + self.findgrid(threshold,rows,cols,matrix,i,j+1)\ + self.findgrid(threshold,rows,cols,matrix,i+1,j)\ + self.findgrid(threshold,rows,cols,matrix,i-1,j) return count]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2018%2F08%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%93%E9%A2%98%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[注：这篇文章原文出处放在了下面的连接，原文已经写得非常清晰明了，这里有适当的修改和增加内容，写在这里是为了更好地查阅巩固。 戳我原文 朴素贝叶斯（Naive Bayes）是基于贝叶斯定理与特征条件假设的分类方法。 对于给定的训练数据集，首先基于特征条件独立假设学习输入、输出的联合分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。 朴素贝叶斯实现简单，学习与预测的效率都很高，是一种常用的方法。 朴素贝叶斯的学习与分类贝叶斯定理先看什么是条件概率 P(A|B)表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为 P\left(A|B\right)=\frac{P\left(AB\right)}{P\left(B\right)}贝叶斯定理便是基于条件概率，通过P(A|B)来求P(B|A)： P\left(B|A\right)=\frac{P\left(A|B\right)·P\left(B\right)}{P\left(A\right)}顺便提一下，上式中的分母，可以根据全概率公式分解为： P\left(A\right)=\sum_{i=1}^n{P\left(B_i\right)P\left(A|B_i\right)}特征条件独立假设这一部分开始朴素贝叶斯的理论推导，从中你会深刻地理解什么是特征条件独立假设。 给定训练数据集(X,Y)，其中每个样本X都包括nn维特征，即$x=(x_1,x_2,···,x_n)$，类标记集合含有K种类别，即$y=(y_1,y_2,···,y_k)$ 如果现在来了一个新样本x我们要怎么判断它的类别?从概率的角度来看，这个问题就是给定x，它属于哪个类别的概率更大。那么问题就转化为求解$P(y_1|x),P(y_2|x),P(y_k|x)$中最大的那个，即求后验概率最大的输出：$arg\underset{y_k}{\max}P\left(y_k|x\right)$ 。那$P(y_k|x)$怎么求解？答案就是贝叶斯定理 P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{P\left(x\right)}根据全概率公式，可以进一步分解上式中的分母： P\left(y_k|x\right)=\frac{P\left(x|y_k\right)·P\left(y_k\right)}{\sum_{i=1}^n{P\left(x|y_k\right)P\left(y_k\right)}} （公式1）先不管分母，分子中的$P(y_k)$是先验概率，根据训练集就可以简单地计算出来，而条件概率$P(x|y_k)=P(x_1,x_2,···,x_n|y_k)$它的参数规模是指数数量级别的，假设第$i$维特征$x_i$可取值的个数有$S_i$个，类别取值个数为k个，那么参数个数为$k\prod_{j=1}^n{S_j}$ 这显然是不可行的。针对这个问题，朴素贝叶斯算法对条件概率分布做了独立性的假设，通俗地讲就是说假设各个维度的特征 $x_1,x_2,···,x_n$互相独立，由于这是一个较强的假设，朴素贝叶斯算法也因此得名。在这个假设的前提上，条件概率可以转化为： P\left(x|y_i\right)=P\left(x_1,x_2,···,x_n|y_i\right)=\prod_{i=1}^n{P\left(x_i|y_i\right)} （公式2）这样参数规模就降到了$\sum_{i=1}^n{S_ik}$ 以上就是针对条件概率所作出的特征条件独立性假设，至此，先验概率$P(y_k)$和条件概率$P(x|y_k)$的求解问题就都解决了，那么我们是不是可以求解我们所需要的后验概率$P(y_k|x)$了？ 答案是肯定的。我们继续上面关于$P(y_k|x)$的推导，将公式2代入公式1中得到： P\left(y_k|x\right)=\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}于是朴素贝叶斯分类器可表示为： f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k|x\right)=arg\underset{y_k}{\max}\frac{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}{\sum_k{P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}}}因为对于所有的$y_k$，上式中的分母的值都是一样的（为什么？注意到全加符号就容易理解了），所以可以忽略分母部分，朴素贝叶斯分裂期最终表示为： f\left(x\right)=arg\underset{y_k}{\max}P\left(y_k\right)\prod_{i=1}^n{P\left(x_i|y_k\right)}朴素贝叶斯法的参数估计极大似然估计根据上述，可知朴素贝叶斯要学习的东西就是$P(Y=c_k)$和$P(X^{j}=a_{jl}|Y=c_k)$可以应用极大似然估计法估计相应的概率（简单讲，就是用样本来推断模型的参数，或者说是使得似然函数最大的参数） 先验概率$P(Y=c_k)$的极大似然估计是 P\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}}{N},\,\,k=1,2,···,K也就是用样本中$c_k$的出现次数除以样本容量。 设第$j$个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j2},···,a_{jl}}$，条件概率 $P(X^{j}=a_{jl}|Y=c_k)$的极大似然估计是： P\left(X^{\left(j\right)}=a_{jl}|Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}}{\sum_{i=1}^N{I\left(y_i=c_k\right)}}式中，$x_i^j$是第$i$个样本的第$j$个特征。 一个例题例题如下 贝叶斯估计极大似然估计有一个隐患，假设训练数据中没有出现某种参数与类别的组合怎么办？比如上例中当Y=1对应的$X^{(1)}$的取值只有1和2。这样可能会出现所要估计的概率值为0的情况，但是这不代表真实数据中就没有这样的组合。这时会影响到后验概率的计算结果，使分类产生偏差。解决办法是贝叶斯估计。 条件概率的贝叶斯估计： P_{\lambda}\left(X^{\left(j\right)}=a_{jl}\parallel Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(x_{i}^{\left(j\right)}=a_{jl},y_{i=}c_k\right)}+\lambda}{\sum_{i=1}^N{I\left(y_i=c_k\right)}+S_j\lambda}其中$\lambda≥0$，$S_j$表示第$x_j$个特征可能取值的个数。分子和分母分别比极大似然估计多了一点东西，其意义为在随机变量各个取值的频数上赋予一个正数$λ≥0$。当$λ=0$时就是极大似然估计。常取$λ=1$，这时称为拉普拉斯平滑。 先验概率的贝叶斯估计：（K为类别个数） P_{\lambda}\left(Y=c_k\right)=\frac{\sum_{i=1}^N{I\left(y_i=c_k\right)}+\lambda}{N+K\lambda}例题如下：套入公式计算即可 上诉说的是$X_j$为普通离散型的情况。如果$X_j$是稀疏的离散值，即各个特征的出现概率很低，那么可以假设$X_j$服从伯努利分布，即特征$X_j$出现记为1，不出现记为0。即我们只关注$X_j$是否出现，不关注$X_j$出现的次数，这样得到的$P\left(X^{\left(j\right)}=a_{jl}|Y=c_k\right)$是在是在样本类别$c_k$中$a_{jl}$出现的频率，公式如下所示。其中$a_{jl}$取值为0和1。 P(X_j=a_{jl}|Y=C_k)=P(X_j|Y=C_k)a_{jl}+(1-P(X_j|Y=C_k))(1-a_{jl})如果$X_j$是连续值，那么假设$X_j$的先验概率为高斯分布(正态分布)，这样假设$P\left(X^{\left(j\right)}=a_{jl}|Y=c_k\right)$的概率分布公式如下所示。其中$\mu_k$和$\sigma_{k}^2$是正态分布的期望和方差，$\mu_k$为样本类别$C_k$中，所有$X_j$的平均值，$\sigma_{k}^2$为样本类别$C_k$中，所有$X_j$的方差，$\mu_k$和$\sigma_{k}^2$可以通过极大似然估计求得。 P(X_j=a_{jl}|Y=C_k)=\frac{1}{\sqrt{2 \pi \sigma^2}}exp(-\frac{(a_{jl}- \mu_k)^2}{2\sigma_{k}^{2}})python代码实现朴素贝叶斯文档分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115# -*- coding: utf-8 -*-"""Created on 下午5:28 22 03 2017bayes algorithm: classify a words as good or bad [text classify]@author: plushunter"""from numpy import *class Naive_Bayes: def __init__(self): self._creteria = "NB" #创建不重复词集 def _creatVocabList(self,dataSet): vocabSet = set([]) # 创建一个空的SET for document in dataSet: vocabSet = vocabSet | set(document) # 并集 return list(vocabSet) # 返回不重复词表（SET的特性） #文档词集向量模型 def _setOfWordToVec(self,vocabList, inputSet): """ 功能:给定一行词向量inputSet，将其映射至词库向量vocabList，出现则标记为1，否则标记为0. """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 return returnVec #文档词袋模型 def _bagOfsetOfWordToVec(self,vocabList, inputSet): """ 功能：对每行词使用第二种统计策略，统计单个词的个数，然后映射到此库中 输出：一个n维向量，n为词库的长度，每个取值为单词出现的次数 """ returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 #更新此处代码 return returnVec def _trainNB0(self,trainMatrix, trainCategory): """ 输入：训练词矩阵trainMatrix与类别标签trainCategory,格式为Numpy矩阵格式 功能：计算条件概率p0Vect、p1Vect和类标签概率pAbusive """ numTrainDocs = len(trainMatrix)#样本个数 numWords = len(trainMatrix[0])#特征个数，此处为词库长度 pAbusive = sum(trainCategory) / float(numTrainDocs)#计算负样本出现概率（先验概率） p0Num = ones(numWords)#初始词的出现次数为1，以防条件概率为0，影响结果 p1Num = ones(numWords)#同上 p0Denom = 2.0#类标记为2，使用拉普拉斯平滑法, p1Denom = 2.0 #按类标记进行聚合各个词向量 for i in range(numTrainDocs): if trainCategory[i] == 0: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) else: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) p1Vect = log(p1Num / p1Denom)#计算给定类标记下，词库中出现某个单词的概率 p0Vect = log(p0Num / p0Denom)#取log对数，防止条件概率乘积过小而发生下溢 return p0Vect, p1Vect, pAbusive def _classifyNB(self,vec2Classify, p0Vec, p1Vec, pClass1): """ 该算法包含四个输入: vec2Classify表示待分类的样本在词库中的映射集合， p0Vec表示条件概率P(wi|c=0)P(wi|c=0)， p1Vec表示条件概率P(wi|c=1)P(wi|c=1)， pClass1表示类标签为1时的概率P(c=1)P(c=1)。 p1=ln[p(w1|c=1)p(w2|c=1)…p(wn|c=1)p(c=1)] p0=ln[p(w1|c=0)p(w2|c=0)…p(wn|c=0)p(c=0)] log取对数为防止向下溢出 功能:使用朴素贝叶斯进行分类,返回结果为0/1 """ p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1 - pClass1) if p1 &gt; p0: return 1 else: return 0 #test def testingNB(self,testSample): "step1：加载数据集与类标号" listOPosts, listClasses = loadDataSet() "step2：创建词库" vocabList = self._creatVocabList(listOPosts) "step3：计算每个样本在词库中出现的情况" trainMat = [] for postinDoc in listOPosts: trainMat.append(self._bagOfsetOfWordToVec(vocabList, postinDoc)) p0V, p1V, pAb = self._trainNB0(trainMat, listClasses) "step4：测试" thisDoc = array(self._bagOfsetOfWordToVec(vocabList, testSample)) result=self._classifyNB(thisDoc, p0V, p1V, pAb) print testSample, 'classified as:', result # return result#### 加载数据集def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return postingList, classVec#测试if __name__=="__main__": clf = Naive_Bayes() testEntry = [['love', 'my', 'girl', 'friend'], ['stupid', 'garbage'], ['Haha', 'I', 'really', "Love", "You"], ['This', 'is', "my", "dog"], ['maybe','stupid','worthless']] for item in testEntry: clf.testingNB(item) 使用朴素贝叶斯过滤垃圾邮件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-"""Created on 下午8:47 22 03 2017Email_Classify @author: plushunter """import reimport Bayesfrom numpy import *# mysent='This book is the best book on Python or M.L I have ever laid eyes upon.'# regEx = re.compile('\\W*')# listOfTokens=regEx.split(mysent)# tok=[tok.upper() for tok in listOfTokens if len(tok)&gt;0]# print tok## emailText=open('email/ham/6.txt').read()# listOfTokens=regEx.split(emailText)# print listOfTokensdef textParse(bigString): import re listOfTokens=re.split(r'\w*',bigString) return [tok.lower() for tok in listOfTokens if len(tok)&gt;2]def spamTest(): clf = Bayes.Naive_Bayes() docList=[] classList=[] fullText=[] for i in range(1,26): wordList=textParse(open('email/spam/%d.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList=textParse(open('email/ham/%i.txt'%i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList=clf._creatVocabList(docList) trainingSet=range(50);testSet=[] for i in range(10): randIndex=int(random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMatix=[];trainClasses=[] for docIndex in trainingSet: trainMatix.append(clf._bagOfsetOfWordToVec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam=clf._trainNB0(array(trainMatix),array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = clf._bagOfsetOfWordToVec(vocabList,docList[docIndex]) if clf._classifyNB(array(wordVector), p0V, p1V, pSpam)!=classList[docIndex]: errorCount+=1 print 'the error rate is :',float(errorCount)/len(testSet) 朴素贝叶斯优缺点优点 具有稳定的分类效率。 对缺失数据不敏感，算法也比较简单。 对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。 缺点 对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。 由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。 假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。]]></content>
      <categories>
        <category>机器学习专题</category>
      </categories>
      <tags>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大期望算法]]></title>
    <url>%2F2018%2F08%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%93%E9%A2%98%2F%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[注：这篇文章原文出处放在了下面的连接，有适当的修改和增加内容，写在这里是为了更好地查阅巩固。 戳我原文 EM算法简介最大期望算法（Expectation Maximization，简称EM算法）是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量。其主要思想就是通过迭代来建立完整数据的对数似然函数的期望界限，然后最大化不完整数据的对数似然函数。最大期望算法是一种迭代优化算法，其计算方法是每次迭代分为期望(E)步和最大(M)步。 EM算法实例假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。 我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。 P1=\frac{3+1+2}{15}=0.4 P2=\frac{2+3}{10}=0.5下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢? 此时我们加入隐含变量z，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2 我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2.例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2×0.2×0.2×0.8×0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7×0.7×0.7×0.3×0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。 我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到 P1=\frac{2+1+2}{15}=0.33 P2=\frac{3+3}{10}=0.6可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。 上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢? 但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。 利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率 z_1=\frac{0.00512}{0.00512+0.03087}=0.14相应的算出其他4轮的概率。 上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为E步。 按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14×3=0.42的概率为正，有0.14×2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为M步。 P1=\frac{4.22}{4.22+7.98}=0.35上面我们是通过迭代来得到P1和P2，结果更接近真实的P1和P2。 EM算法推导Jensen不等式Jensen不等式表述如下： 如果f是凸函数，x是随机变量，那么：$E[f(x)]\geq f(E[x])$ ,特别地，如果f是严格凸函数，那么当且仅当$P(x=E[x])=1$(也就是说x是常量)，$E[f(x)]=f(E[x])$ 通过下面这张图，我们可以加深理解： 上图中，函数f是凸函数，X是随机变量，有0.5的概率为a，有0.5的概率是b（就像抛硬币一样）。X的期望值就是a和b的中值了，图中可以看到$E[f(x)]\geq f(E[x])$成立. 极大似然估计EM算法推导过程中，会使用到极大似然估计参数。 极大似然估计是一种概率论在统计学的应用。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察结果，利用结果推出参数的大概值。极大似然估计建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 这里再给出求极大似然估计值的一般步骤： 1）写出似然函数； 2）对似然函数取对数，并整理； 3）求导数，令导数为0，得到似然方程； 4）解似然方程，得到的参数即为所求； EM算法推导对于m个样本观察数据$x=(x^{(1)},x^{(2)},x^{(3)},…,x^{(m)})$，找出样本的模型参数θ，极大化模型分布的对数似然函数如下所示 \theta =\arg \max_{\theta} \sum _{i=1}^{m}logP(x^{(i)};\theta)如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},z^{(3)},…,z^{(m)})$，此时我们极大化模型分布的对数似然函数如下 \theta =\arg \max_{\theta} \sum _{i=1}^{m}logP(x^{(i)};\theta) \\ \\ \\ =\arg \max_{\theta} \sum _{i=1}^{m}log\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\theta)上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入Jensen不等式。 我们再回到上述推导过程，得到如下方程式。 \sum _{i=1}^{m}log\sum _{z^{(i)}}P(x^{(i)},z^{(i)};\theta) \\ \\ \\ =\sum _{i=1}^{m}log\sum _{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})}\ \ \ \ \ \ \ (1) \\ \\ \\ \ge \sum _{i=1}^{m}\sum _{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})} \ \ \ \ \ \ \ (2)我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布$Q_i(z^{(i)})$，第二式用到Jensen不等式。 首先log函数是凹函数，那么E[f(X)]≤f(E[X])，也就是f(E(X))≥E(f(X))。其中$\sum _{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})}$ 是$\frac{P(x^{(i)},z^{(i);\theta})}{Q_i(z^{(i)})}$的数学期望，那么$log\sum _{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})}$ 便相当于f(E(X))，同时$\sum _{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})}$相当于E(f(X))，因此我们便得到上述方程式(1)(2)。 如果要满足Jensen不等式的等号，那么需要满足X为常量，即为 \frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c,\ c为常量那么稍加改变能够得到 c Q_i(z^{(i)})=P(x^{(i)},z^{(i)};\theta)\ ,c为常量 \sum_z c\ Q_i(z^{(i)})= \sum _z P(x^{(i)},z^{(i)};\theta)= c因此得到下列方程，其中方程(3)利用到条件概率公式。 Q_i(z^{(i)})=\frac{P(x^{(i)},z^{(i)};\theta)}{c}=\frac{P(x^{(i)},z^{(i)};\theta)}{\sum _z P(x^{(i)},z^{(i)};\theta)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\frac{P(x^{(i)},z^{(i)};\theta)}{ P(x^{(i)};\theta)}=P(z^{(i)}|x^{(i)};\theta) \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3)如果$Q_i(z^{(i)})=P(z^{(i)}|x^{(i)};\theta)$ 那么第(2)式就是我们隐藏数据的对数似然的下界。如果我们能极大化方程式(2)的下界，则也在尝试极大化我们的方程式(1)。即我们需要最大化下式 \arg \max _{\theta} \sum _{i=1}^{m}\sum _{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)},z^{(i)};\theta) }{Q_i(z^{(i)})}去掉上式中常数部分，则我们需要极大化的对数似然下界为 \arg \max _{\theta} \sum _{i=1}^{m}\sum _{z^{(i)}}Q_i(z^{(i)})[log P(x^{(i)},z^{(i)};\theta)-log {Q_i(z^{(i)})}] =\arg \max _{\theta} \sum _{i=1}^{m}\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\theta)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4)注意到上式中$Q_i(z^{(i)})$ 是一个分布，因此$\sum _{z^{(i)}}Q_i(z^{(i)})log P(x^{(i)},z^{(i)};\theta)$ 可以理解为$logP(x^{(i)},z^{(i)};\theta)$ 基于条件概率分布$Q_i(z^{(i)})$的期望，也就是我们EM算法中E步。极大化方程式(4)也就是我们EM算法中的M步。 到这里，我们推出了在固定参数θ后，使下界拉升的Q(z)的计算公式就是后验概率（条件概率），解决了Q(z)如何选择的问题。此步就是EM算法的E步，目的是建立L(θ)的下界。接下来的M步，目的是在给定Q(z)后，调整θ，从而极大化L(θ)的下界J（在固定Q(z)后，下界还可以调整的更大）。那么一般的EM算法的步骤如下： 第一步：初始化分布参数θ； 第二步：重复E步和M步直到收敛： E步：根据参数的初始值或上一次迭代的模型参数来计算出的因变量的后验概率（条件概率），其实就是隐变量的期望值，来作为隐变量的当前估计值： Q_i\left(z^{\left(i\right)}\right)=p\left(z^{\left(i\right)}|x^{\left(i\right)};\theta\right)M步：最大化似然函数从而获得新的参数值： \theta :=arg\underset{\theta}{\max}\sum_i{\sum_{z^{\left(i\right)}}{Q_i\left(z^{\left(i\right)}\right)\log\frac{p\left(x^{\left(i\right)},z^{\left(i\right)};\theta\right)}{Q_i\left(z^{\left(i\right)}\right)}}} EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。 Sklearn实现EM算法高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去这儿。下列代码来自于Sklearn官网GMM模块，利用高斯混合模型确定iris聚类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import matplotlib as mplimport matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasetsfrom sklearn.mixture import GaussianMixturefrom sklearn.model_selection import StratifiedKFoldcolors = ['navy', 'turquoise', 'darkorange']def make_ellipses(gmm, ax): for n, color in enumerate(colors): if gmm.covariance_type == 'full': covariances = gmm.covariances_[n][:2, :2] elif gmm.covariance_type == 'tied': covariances = gmm.covariances_[:2, :2] elif gmm.covariance_type == 'diag': covariances = np.diag(gmm.covariances_[n][:2]) elif gmm.covariance_type == 'spherical': covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n] v, w = np.linalg.eigh(covariances) u = w[0] / np.linalg.norm(w[0]) angle = np.arctan2(u[1], u[0]) angle = 180 * angle / np.pi # convert to degrees v = 2. * np.sqrt(2.) * np.sqrt(v) ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1], 180 + angle, color=color) ell.set_clip_box(ax.bbox) ell.set_alpha(0.5) ax.add_artist(ell)iris = datasets.load_iris()# Break up the dataset into non-overlapping training (75%)# and testing (25%) sets.skf = StratifiedKFold(n_splits=4)# Only take the first fold.train_index, test_index = next(iter(skf.split(iris.data, iris.target)))X_train = iris.data[train_index]y_train = iris.target[train_index]X_test = iris.data[test_index]y_test = iris.target[test_index]n_classes = len(np.unique(y_train))# Try GMMs using different types of covariances.estimators = dict((cov_type, GaussianMixture(n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0)) for cov_type in ['spherical', 'diag', 'tied', 'full'])n_estimators = len(estimators)plt.figure(figsize=(3 * n_estimators // 2, 6))plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05, left=.01, right=.99)for index, (name, estimator) in enumerate(estimators.items()): # Since we have class labels for the training data, we can # initialize the GMM parameters in a supervised manner. estimator.means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)]) # Train the other parameters using the EM algorithm. estimator.fit(X_train) h = plt.subplot(2, n_estimators // 2, index + 1) make_ellipses(estimator, h) for n, color in enumerate(colors): data = iris.data[iris.target == n] plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]) # Plot the test data with crosses for n, color in enumerate(colors): data = X_test[y_test == n] plt.scatter(data[:, 0], data[:, 1], marker='x', color=color) y_train_pred = estimator.predict(X_train) train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100 plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy, transform=h.transAxes) y_test_pred = estimator.predict(X_test) test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100 plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy, transform=h.transAxes) plt.xticks(()) plt.yticks(()) plt.title(name)plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))plt.show() EM算法优缺点优点 聚类。 算法计算结果稳定、准确。 EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。 缺点 对初始化数据敏感。 EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。 当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。]]></content>
      <categories>
        <category>机器学习专题</category>
      </categories>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浴室沉思（二）]]></title>
    <url>%2F2018%2F08%2F20%2F%E6%B5%B4%E5%AE%A4%E6%B2%89%E6%80%9D%2F%E6%B5%B4%E5%AE%A4%E6%B2%89%E6%80%9D%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1. 地球那么大，为什么月亮的倒影能够准确地投映在我家院子里的一口井里？ 2. 你可以通过逃跑解决肥胖问题。 3. 你看不见你的脖子，你也离不开它。 4. 现金是电子支付的离线缓存。 5. 给水果剥皮，就像是拆来自大地的礼物。 6. 酒店房间里收费的饮品就像是手机应用的内购。 7. 你不能想象一种不存在的颜色。 8. 火山：睡上一万年，生起床气，到处乱丢东西，再睡上一万年。 9. 如果你每天“带薪拉屎”十分钟，一年下来就能相当于休了个一周的“带薪拉屎假”。 10. 如果你的手臂可以拆卸，你也没办法自己把它们互换位置。。]]></content>
      <categories>
        <category>浴室沉思</category>
      </categories>
      <tags>
        <tag>showerthoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浴室沉思（一）]]></title>
    <url>%2F2018%2F08%2F20%2F%E6%B5%B4%E5%AE%A4%E6%B2%89%E6%80%9D%2F%E6%B5%B4%E5%AE%A4%E6%B2%89%E6%80%9D%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1. 总有一天，你会完全忘记你读到过这句话。 2. 音乐是颤抖的风。 3. 狗狗可能并不喜欢叼球回来，只是不想看到东西弄丢了。 4. 根据美国的法律，在合法地饮用第一杯酒之前，你要先绕太阳飞行21圈。 5. 人类殖民火星，就像是通过空气传播的病毒。 6. 因为脑海里的声音不需要换气，脑海里可以一直在尖叫。 7. 同义词没有同义词，但是反义词的反义词是同义词。 8. 闪电是有人躲在云里偷拍我们，但忘了关闪光灯。 9. 氧气有最严重的戒断症状。 10. 火车是一种在三维世界中建立在二维平面上只能一维移动的物体。]]></content>
      <categories>
        <category>浴室沉思</category>
      </categories>
      <tags>
        <tag>showerthoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浴室沉思101]]></title>
    <url>%2F2018%2F08%2F19%2F%E6%B5%B4%E5%AE%A4%E6%B2%89%E6%80%9D%2F%E6%B5%B4%E5%AE%A4%E6%B2%89%E6%80%9D101%2F</url>
    <content type="text"><![CDATA[原帖地址：戳我 欢迎来到r/showerthoughts, 我们很高兴在这里见到你，并且希望你玩得开心。这个帖子是一个关于showerthoughts 是什么/不是什么的简介。 什么是Showerthoughts? 简单来说，Showerthoughts 就是那些让庸常生活充满趣味的想法。这些想法往往是“另一个角度看待日常生活”的结果，而”另一个角度看待日常生活“往往又会让大家看到新的细节。这些想法可能十分可笑，也可能有些苦涩，可能会给你带来思想上的启迪，或者仅仅就是傻乎乎的而已。但他总应该是让人感到“啊，我怎么就没发现呢。“ Showerthoughts, 这个词指的是我们在日常生活中做无脑无聊时候事情时候产生的想法，并不一定要发生在浴室，也可以是在通勤中，在开车，或者是在等客服接你的电话的时候。 这里有一些Showerthoughts的例子： “你的胃觉得所有的土豆都是土豆泥。” “当人们提到穿越回过去的时候，总是担心会导致改变今天的世界；但是他们却没有觉得，今天做的一切都会改变未来的世界。“ ”如果你够蠢，所有的飞行物都是不明飞行物。” “C3PO的确是卢克天行者的哥哥。” “天鹅是一种聒噪，暴力，有进攻性的恐怖生物，同时也是爱情的象征。” 什么不是Showerthoughts？ 就像上面说的那样，Showerthoughts 应该让人从一个新的角度看待已经存在的现象或者事物。它不应该是一个个人意见或者观点，或者是一种产品的改进建议，或者是某些能快速通过Google 解决的问题。Showerthoughts 应该是一个命题，而不是一个假设。 R/showerthoughts 里面的内容应该文字流畅，内容新颖。如果某个post 过于泛滥，或者说文字水平太低，抑或者太过粗鲁，管理员将会将其移出。这些例子里的内容不是Showerthoughts： “淦！我要买些肥皂了！” （这是一条浴室观察，不是一条浴室沉思） “我讨厌开车的时候被别人加塞，但是看着别人被加塞还蛮开心的。“ （这是个人观点） “有人试过珠穆朗玛峰顶的味道嘛？“ （这已经很接近showerthoughts 了，但是这是一个疑问句，应该改写成一个陈述句。） “地球每一天都再心的地方” （文字不流畅，而且还有错别字） “有没有可能有一群蜜蜂真的有蜂群思维呢？“ （这是个假设，而且还是个文字游戏） 很多时候，我们很难找到一个完全原创，文字流畅，充满洞见的想法，不过这才让showerthoughts 变得好玩：它让你发现在你无意识的时候，你也是在思考的；而且这些想法也让你的无聊日常变得有趣起来。 为什么要把Showerthoughts加入到自己的博客当中？ Showerthoughts是我在Reddit里面最喜欢的一个版块，也是个人很喜欢的一种文化。看似简单的句子里蕴藏了许多奇思妙想，看待事物的角度时常让人耳目一新。看过许多有趣的Showerthoughts之后，偶尔也会自己进行创作，虽然质量不高，但都会发在饭否上面记录。但由于没有统一地方记录这些Showerthoughts，许多之前写的Showerthoughts已经很难找到了。所以，打算在博客里专门创建一个Showerthoughts的Tag，记录收藏有趣的Showerthoughts。 这些Showerthoughts绝大多数会在Reddit上面的r/showerthoughts版块中找到完整的原句，自己只搬运翻译一些感兴趣的Showerthoughts；另外还会摘录一些关注的饭友写的showerthoughts；最后也会自己偶尔写一些较低质量的Showerthoughts。 希望大家在码完代码疲惫之后，可以看到一些有趣的Showerthoughts，消除倦意，继续战斗。 注：微博上也有相关的Showerthoughts搬运博主，偶尔会转发一些网友有趣的Showerthoughts：浴室沉思]]></content>
      <categories>
        <category>浴室沉思</category>
      </categories>
      <tags>
        <tag>showerthoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浴室沉思101]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B%2Fkaggle_house_prices%2F</url>
    <content type="text"><![CDATA[思路 1、拆分数据集 ，验证集，测试集]]></content>
      <categories>
        <category>浴室沉思</category>
      </categories>
      <tags>
        <tag>showerthoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn 与 TensorFlow 机器学习实用指南（七）：降维]]></title>
    <url>%2F2018%2F08%2F07%2FSklearn%20%E4%B8%8E%20TensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%2FSklearn-%E4%B8%8E-TensorFlow-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E9%99%8D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[先来了解一张降维汇总图，降维的算法比较多，这里就只简单说MDS，PCA以及流行学习的Isomap和LLE。 多维缩放MDS：Multiple Dimensional Scaling低维嵌入：在很多时候， 人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维”嵌入” (embedding) . 图 10.2 给出 了 一个直观的例子. 原始高维空间中的样本点，在这个低维嵌入子空间中更容易进行学习。 MDS的目标是在降维的过程中将数据的dissimilarity(差异性)保持下来，也可以理解降维让高维空间中的距离关系与低维空间中距离关系保持不变。这里的距离用矩阵表示，N个样本的两两距离用矩阵D的每一项$dist_{ij}$ 表示，并且假设在低维空间中的距离是欧式距离。而降维后的数据表示为$Z_i$,那么就有 dist_{ij}^2 = \left | \mathbf{z_i-z_j} \right |^2 = \left | \mathbf{z_i} \right |^2 + \left | \mathbf{z_j} \right |^2 - 2\mathbf{z_i}\mathbf{z_j}^T令$B=ZZ^T$ ,右边的三项统一用内积矩阵B来表示$b_{ij}=z_iz_j^T$ ,所以有 dist_{ij}^2=b_{ii}+b_{jj}-2b_{ij} \quad(1)这时只要求出内积矩阵B即可求出降为后的矩阵Z（思路D-B-Z）。距离矩阵D去中心化之后(减去均值)，內积矩阵B的每一行每一列之和都是0，可以推导得出 其中 tr(.) 表示矩阵的迹(trace),$tr(E)=\sum_{i=1}^m\Vert z_i \Vert^2=\sum_{i=1}^mb_{ii}$,令 联立上(1)-(7),消除$b_{ii},b_{jj}$后，可以得到 b_{ij} = -\frac{1}{2}(dist_{ij}^2-dist_{i\cdot} - dist_{\cdot j}-dist_{\cdot\cdot}^2)​i⋅与⋅j是指某列或者某列总和，从而建立了距离矩阵D与内积矩阵B之间的关系.由此即可通过降维前后保持不变的距离矩阵 D 求取内积矩阵 B。对矩阵 B 做特征值分解(eigenvalue decomposition)，$B=VAV^T$，其中$A=diag(λ_1,λ_2,…λ_d)$为特征值构成的对角矩阵,$λ_1\geq λ_2\geq …\geq λ_d$，V 为特征向量矩阵.假定其中有d* 个非零特征值，它们构成对角矩阵$A=diag(λ_1,λ_2,…λ_{d})$,令 V*表示相应的特征向量矩阵，联立之前的$B=ZZ^T$,则最后Z可表达为 Z=A_*^{1/2}V_*^T在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能接近?而不必严格相等.此时可取 d’&lt;&lt; d 个最大特征值构成对角矩阵。 主成分分析PCA: Principal Component Analysis向量內积由$A⋅B=|A||B|cos(a)$, A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！ 基向量(x,y)实际上表示线性组合，$x(1,0)^T+y(0,1)^T$,不难证明所有二维向量都可以表示为这样的线性组合。此处（1，0）和（0，1）叫做二维空间中的一组基。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量.例如，(1,1)和(−1,1)也可以成为一组基。一般来说，我们希望基的模是1,实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$和$(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$ 基变换的矩阵表示：下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将(3,2)变换为新基上的坐标，就是用(3,2)与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换，其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标： 协方差矩阵及优化目标面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？ 现在问题来了：如果我们必须使用一维来表示上面这些数据，又希望尽量保留原始的信息，你要如何选择？ 通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。 那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。 我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即： Var(a)=\frac{1}{m}\sum^m_{i=1}(a_i-\mu)^2通过去中心化（字段所有数值减去字段均值），将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示： Var(a)=\frac{1}{m}\sum^m_{i=1}a_i^2于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大. 协方差对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。 如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。 数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则： Cov(a,b) = \frac{1}{m}\sum_{i=1}^ma_ib_i可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。 当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。 至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 协方差矩阵我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。 假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X： 然后我们用X乘以X的转置，并乘上系数1/m： 奇迹出现了！这个对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。 根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设$C=\frac{1}{m}XX^𝖳$ ，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。 协方差矩阵对角化根据上述推导，我们发现要达到优化条件，等价于将协方差对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系： 设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设$Y=PX$，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系： D = \frac{1}{m}YY^T\\ \quad =\frac{1}{m}(PX)(PX)^T\\ =\frac{1}{m}PXX^TP\\=PCP^T现在事情很明白了，我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^T$是一个对角矩阵，并且对角元素按从小到大依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。 现在所有焦点都聚焦在了协方差矩阵对角化问题上，由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质： 1）实对称矩阵不同特征值对应的特征向量必然正交。 2）设特征向量λ重数为r，则必然存在r个线性无关的特征向量对应于λ，因此可以将这r个特征向量单位正交化。 由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为$e_1,e_2,⋯,e_n$，我们将其按列组成矩阵： E = (e_1\ e_2 \ ··· \ e_n)则对协方差矩阵C有如下结论： E^TCE = \varLambda\ =\left[\begin{matrix} \lambda_1& & & \\ & \lambda_2& & \\ & & ···& \\ & & & \lambda_n\\ \end{matrix}\right]其中Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。 到这里，我们发现我们已经找到了需要的矩阵P：P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照Λ中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y 于是，只需对协方差矩阵$XX^T$进行特征值分解，将求得的特征值排序:$λ_1\geq λ_2\geq …\geq λ_d$,再再取前 d’ 个特征值对应的特征向量构成 $W=(w_1,w_2,..,w_d’)$.这就是主成分分析的解.要注意降维后低维空间的维数 d’ 通常是由用户事先指定。 简单回顾一下：给定原始数据矩阵X，其协方差矩阵$C=\frac{1}{M}XX^T$对角元素代表了方差，其余元素代表相关性。假定降维后的数据矩阵为Y，其协方差D会满足对角元素最大，其余元素为0，这样才会代表降维后的要求。设Y=PX，P代表要与矩阵相乘的基，有$D=PCP^T$的关系。这个等式的形式与实对称矩阵对角化一样，这时候只要找出C的特征值对应的特征向量，组合起来即为我们需要求得P。最后Y=PX得到降维后的数据矩阵。 PCA 仅需保留 W 与 样本 的均值向 量即可通过简单的向量减法和矩阵”向量乘法将新样本投影至低维空间中 . 显然，低维空间与原始高维空间必有不同，因为对应于最小的d-d’个特征值的特征 向量被舍弃了，这是降维导致的结果.但舍弃这部分信息往往是必要的- 一方面舍弃这部分信息之后能使样本的采样密度增大，这正是降维 的重要动机; 另一方面，当数据受到 噪声影响时， 最小的特征值所对应的特征 向量往往与噪声有关?将它们舍弃能在一定程度上起到去噪的效果. 流行学习算法Isomap：等度量映射等度量映射(Isometric Mapping，简称 Isomap) 的基本 出发点，是认为低维流Î~嵌入到 高维空 间之后，直接在高维空间 中计算直线距离具有误导性，因为高维空间中的直线距离在低维嵌入流形上是不可达的.如图 所示，低维嵌入流形上两点间的距离是”测地线” (geodesic)距离。 那么，如何计算测地线距离呢?这时我们可利用流形在局部上与 欧氏空间同胚这个性质，对每个点基于欧 氏距离找出其近邻点，然后就能建立一个近邻连接图，图中近邻点之间存在连接，而非近邻点之间不存在连接， 于是，计算两点之间测地线距离的问题就转变为计算近邻连接图上两点之间的最短路径问题. 在近邻连接图上计算两点间的最短路径?可采用著名的Dijkstra算法或Floyd算法，在得到任意两点的距离之后，就可通过MDS 方法来获得样本点在低维空间中的坐标。 流行学习算法LLE：局部线性嵌入LLE首先假设数据在较小的局部是线性的，也就是说，某一个数据可以由它邻域中的几个样本来线性表示。比如我们有一个样本x1,我们在它的原始高维邻域里用K-近邻思想找到和它最近的三个样本x2,x3,x4. 然后我们假设x1可以由x2,x3,x4线性表示，即 x_1 = w_{12}x_2 + w_{13}x_3 +w_{14}x_4其中，w12，w13，w14为权重系数。在我们通过LLE降维后，我们希望x1在低维空间对应的投影x′1和x2,x3,x4对应的投影x′2,x′3,x′4也尽量保持同样的线性关系，即 x_1' \approx w_{12}x_2' + w_{13}x_3' +w_{14}x_4'也就是说，投影前后线性关系的权重系数w12，w13，w14是尽量不变或者最小改变的。 从上面可以看出，线性关系只在样本的附近起作用，离样本远的样本对局部的线性关系没有影响，因此降维的复杂度降低了很多。 对于LLE算法，我们首先要确定邻域大小的选择，即我们需要多少个邻域样本来线性表示某个样本。假设这个值为k。我们可以通过和KNN一样的思想通过距离度量比如欧式距离来选择某样本的k个最近邻。 在寻找到某个样本的xi的k个最近邻之后我们就需要找到找到xi和这k个最近邻之间的线性关系，也就是要找到线性关系的权重系数。找线性关系，这显然是一个回归问题。假设我们有m个n维样本{x1,x2,…,xm},我们可以用均方差作为回归问题的损失函数：即： J(w) = \sum\limits_{i=1}^{m}||x_i-\sum\limits_{j=1}^{k}w_{ij}x_j||_2^2一般我们也会对权重系数$w_{ij}$做归一化的限制，即权重系数需要满足 \sum\limits_{j=1}^{k}w_{ij} = 1对于不在样本$x_i$邻域内的样本$x_j$，我们令对应的$w_{ij}=0$.也就是我们需要通过上面两个式子求出我们的权重系数。一般我们可以通过矩阵和拉格朗日子乘法来求解这个最优化问题。(这个推导就不写了） 最后得到 W_i = \frac{Z_i^{-1}1_k}{1_k^TZ_i^{-1}1_k}其中$W_i=(w_{i1},w_{i2},…w_{ik})^T$ ,矩阵$Z_i=(x_i−x_j)^T(x_i−x_j)$,其中$1_k$ 为k维全1向量。 在我们得到了高维的权重系数，那么我们希望这些权重系数对应的线性关系在降维后的低维一样得到保持。假设我们的n维样本集{x1,x2,…,xm}在低维的d维度对应投影为{y1,y2,…,ym}, 则我们希望保持线性关系，也就是希望对应的均方差损失函数最小，即最小化损失函数J(Y)如下： J(y) = \sum\limits_{i=1}^{m}||y_i-\sum\limits_{j=1}^{k}w_{ij}y_j||_2^2这个优化目标与之前的同形，唯一的区别是之前需要确定权重系数$w_i$，而现在是知道权重系数，需要确定的是$x_i$对应的低维空间坐标$y_i$。 令$M=(I-W)^T(I-W)$ ,则优化函数转变为最小化下式：$J(Y) = tr(Y^TMY)$,tr为迹函数。约束函数矩阵化为：$Y^TY=mI$ 上式可通过特征值分解求解:M 最小的 d’ 个特征值对应的特征向量组成的矩阵即为 $Z^T$.算法流程： 从图中可以看出，LLE算法主要分为三步，第一步是求K近邻的过程，这个过程使用了和KNN算法一样的求最近邻的方法。第二步，就是对每个样本求它在邻域里的K个近邻的线性关系，得到线性关系权重系数W，第三步就是利用权重系数来在低维里重构样本数据。 应用我们将会展示两种主要的降维方法：投影（projection）和流形学习（Manifold Learning），同时我们还会介绍三种流行的降维技术：主成分分析（PCA），核主成分分析（Kernel PCA）和局部线性嵌入（LLE）。 主成分分析（PCA）主成分分析（Principal Component Analysis）是目前为止最流行的降维算法。首先它找到接近数据集分布的超平面，然后将所有的数据都投影到这个超平面上。 保留（最大）方差在将训练集投影到较低维超平面之前，您首先需要选择正确的超平面。例如图左侧是一个简单的二维数据集，以及三个不同的轴（即一维超平面）。图右边是将数据集投影到每个轴上的结果。正如你所看到的，投影到实线上保留了最大方差，而在点线上的投影只保留了非常小的方差，投影到虚线上保留的方差则处于上述两者之间。 选择保持最大方差的轴看起来是合理的，因为它很可能比其他投影损失更少的信息。证明这种选择的另一种方法是，选择这个轴使得将原始数据集投影到该轴上的均方距离最小。这是就 PCA 背后的思想，相当简单。 主成分（Principle Componets）PCA 寻找训练集中可获得最大方差的轴。在上图中，它是一条实线。它还发现了一个与第一个轴正交的第二个轴，选择它可以获得最大的残差。在这个 2D 例子中，没有选择：就只有这条点线。但如果在一个更高维的数据集中，PCA 也可以找到与前两个轴正交的第三个轴，以及与数据集中维数相同的第四个轴，第五个轴等。 定义第i个轴的单位矢量被称为第i个主成分（PC）。在图中，第一个 PC 是c1，第二个 PC 是c2。在投影图中，前两个 PC 用平面中的正交箭头表示，第三个 PC 与上述 PC 形成的平面正交（指向上或下） 概述： 主成分的方向不稳定：如果您稍微打乱一下训练集并再次运行 PCA，则某些新 PC 可能会指向与原始 PC 方向相反。但是，它们通常仍位于同一轴线上。在某些情况下，一对 PC 甚至可能会旋转或交换，但它们定义的平面通常保持不变。 那么如何找到训练集的主成分呢？幸运的是，有一种称为奇异值分解（SVD）的标准矩阵分解技术，可以将训练集矩阵X分解为三个矩阵$U·Σ·V^T$的点积，其中$V^T$$包含我们想要的所有主成分，如下所示。 下面的 Python 代码使用了 Numpy 提供的svd()函数获得训练集的所有主成分，然后提取前两个 PC: 1234X_centered=X-X.mean(axis=0) # 中心化U,s,V=np.linalg.svd(X_centered)c1=V.T[:,0]c2=V.T[:,1] 警告：PCA 假定数据集以原点为中心。正如我们将看到的，Scikit-Learn 的PCA类负责为您的数据集中心化处理。但是，如果您自己实现 PCA（如前面的示例所示），或者如果您使用其他库，不要忘记首先要先对数据做中心化处理。 投影到d维空间：一旦确定了所有的主成分，你就可以通过将数据集投影到由前d个主成分构成的超平面上，从而将数据集的维数降至d维。选择这个超平面可以确保投影将保留尽可能多的方差。 为了将训练集投影到超平面上，可以简单地通过计算训练集矩阵X和Wd的点积，Wd定义为包含前d个主成分的矩阵（即由V^T的前d列组成的矩阵） 将训练集投影到d维空间的公式： X_{d-proj} = X \cdot W_d下面的 Python 代码将训练集投影到由前两个主成分定义的超平面上： 12W2=V.T[:,:2] # 降为2维X2D=X_centered.dot(W2) 使用 Scikit-LearnScikit-Learn 的 PCA 类使用 SVD 分解来实现，就像我们之前做的那样。以下代码应用 PCA 将数据集的维度降至两维（请注意，它会自动处理数据的中心化）： 1234from sklearn.decomposition import PCApca=PCA(n_components=2)X2D=pca.fit_transform(X) 将 PCA 转化器应用于数据集后，可以使用components_访问每一个主成分（注意，它返回以 PC 作为水平向量的矩阵，因此，如果我们想要获得第一个主成分则可以写成pca.components_.T[:,0]）。 方差解释率（Explained Variance Ratio）另一个非常有用的信息是每个主成分的方差解释率，可通过explained_variance_ratio_变量获得。它表示位于每个主成分轴上的数据集方差的比例。例如，让我们看下图中表示的三维数据集前两个分量的方差解释率： 12&gt;&gt;&gt; print(pca.explained_variance_ratio_)array([0.84248607, 0.14631839]) 这表明，84.2% 的数据集方差位于第一轴，14.6% 的方差位于第二轴。第三轴的这一比例不到1.2％，因此可以认为它可能没有包含什么信息 选择正确的维度通常我们倾向于选择加起来到方差解释率能够达到足够占比（例如 95%）的维度的数量，而不是任意选择要降低到的维度数量。当然，除非您正在为数据可视化而降低维度 — 在这种情况下，您通常希望将维度降低到 2 或 3。 下面的代码在不降维的情况下进行 PCA，然后计算出保留训练集方差 95% 所需的最小维数： 1234pca=PCA()pac.fit(X)cumsum=np.cumsum(pca.explained_variance_ratio_)d=np.argmax(cumsum&gt;=0.95)+1 你可以设置n_components = d并再次运行 PCA。但是，有一个更好的选择：不指定你想要保留的主成分个数，而是将n_components设置为 0.0 到 1.0 之间的浮点数，表明您希望保留的方差比率： 12pca=PCA(n_components=0.95)X_reduced=pca.fit_transform(X) 另一种选择是画出方差解释率关于维数的函数（简单地绘制cumsum）。曲线中通常会有一个肘部，方差解释率停止快速增长。您可以将其视为数据集的真正的维度。在这种情况下，您可以看到将维度降低到大约100个维度不会失去太多的可解释方差。 PCA 压缩显然，在降维之后，训练集占用的空间要少得多。例如，尝试将 PCA 应用于 MNIST 数据集，同时保留 95% 的方差。你应该发现每个实例只有 150 多个特征，而不是原来的 784 个特征。因此，尽管大部分方差都保留下来，但数据集现在还不到其原始大小的 20%！这是一个合理的压缩比率，您可以看到这可以如何极大地加快分类算法（如 SVM 分类器）的速度。 通过应用 PCA 投影的逆变换，也可以将缩小的数据集解压缩回 784 维。当然这并不会返回给你最原始的数据，因为投影丢失了一些信息（在5％的方差内），但它可能非常接近原始数据。原始数据和重构数据之间的均方距离（压缩然后解压缩）被称为重构误差（reconstruction error）。例如，下面的代码将 MNIST 数据集压缩到 154 维，然后使用inverse_transform()方法将其解压缩回 784 维。图 8-9 显示了原始训练集（左侧）的几位数字在压缩并解压缩后（右侧）的对应数字。您可以看到有轻微的图像质量降低，但数字仍然大部分完好无损。 123pca=PCA(n_components=154)X_mnist_reduced=pca.fit_transform(X_mnist)X_mnist_recovered=pca.inverse_transform(X_mnist_reduced) PCA逆变换公式，回退到原来的数据维度 X_{recovered} = X_{d-proj} \cdot W_d^T增量 PCA（Incremental PCA）先前 PCA 实现的一个问题是它需要在内存中处理整个训练集以便 SVD 算法运行。幸运的是，我们已经开发了增量 PCA（IPCA）算法：您可以将训练集分批，并一次只对一个批量使用 IPCA 算法。这对大型训练集非常有用，并且可以在线应用 PCA（即在新实例到达时即时运行）。 下面的代码将 MNIST 数据集分成 100 个小批量（使用 NumPy 的array_split()函数），并将它们提供给 Scikit-Learn 的IncrementalPCA类，以将 MNIST 数据集的维度降低到 154 维（就像以前一样）。请注意，您必须对每个最小批次调用partial_fit()方法，而不是对整个训练集使用fit()方法 1234567from sklearn.decomposition import IncrementalPCAn_batches=100inc_pca=IncrementalPCA(n_components=154)for X_batch in np.array_spplit(X_mnist,n_batches): #分100批次数据 inc_pca.partial_fit(X_batch) # 必须批次调用partial_fit()方法X_mnist_reduced=inc_pca.transform(X_mnist) 或者，您可以使用 NumPy 的memmap类，它允许您操作存储在磁盘上二进制文件中的大型数组，就好像它完全在内存中；该类仅在需要时加载内存中所需的数据。由于增量 PCA 类在任何时间内仅使用数组的一小部分，因此内存使用量仍受到控制。这可以调用通常的fit()方法，如下面的代码所示： 1234X_mm=np.memmap(filename,dtype='float32',mode='readonly',shape=(m,n))batch_size=m//n_batchesinc_pca=IncrementalPCA(n_components=154,batch_size=batch_size)inc_pca.fit(X_mm) 随机 PCA（Randomized PCA）Scikit-Learn 提供了另一种执行 PCA 的选择，称为随机 PCA。这是一种随机算法，可以快速找到前d个主成分的近似值。它的计算复杂度是O(m × d^2) + O(d^3)，而不是O(m × n^2) + O(n^3)，所以当d远小于n时，它比之前的算法快得多。 12rnd_pca=PCA(n_components=154,svd_solver='randomized')X_reduced=rnd_pca.fit_transform(X_mnist) 核 PCA（Kernel PCA）例如，下面的代码使用 Scikit-Learn 的KernelPCA类来执行带有 RBF 核的 kPCA 1234from sklearn.decomposition import KernelPCArbf_pca=KernelPCA(n_components=2,kernel='rbf',gamma=0.04)X_reduced=rbf_pca.fit_transform(X) 选择一种核并调整超参数由于 kPCA 是无监督学习算法，因此没有明显的性能指标可以帮助您选择最佳的核方法和超参数值。但是，降维通常是监督学习任务（例如分类）的准备步骤，因此您可以简单地使用网格搜索来选择可以让该任务达到最佳表现的核方法和超参数。例如，下面的代码创建了一个两步的流水线，首先使用 kPCA 将维度降至两维，然后应用 Logistic 回归进行分类。然后它使用Grid SearchCV为 kPCA 找到最佳的核和gamma值，以便在最后获得最佳的分类准确性： 1234567891011from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipelineclf = Pipeline([ ("kpca", KernelPCA(n_components=2)), ("log_reg", LogisticRegression())])param_grid = [&#123; "kpca__gamma": np.linspace(0.03, 0.05, 10), "kpca__kernel": ["rbf", "sigmoid"] &#125;]grid_search = GridSearchCV(clf, param_grid, cv=3)grid_search.fit(X, y) 你可以通过调用best_params_变量来查看使模型效果最好的核和超参数： 12&gt;&gt;&gt; print(grid_search.best_params_)&#123;'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'&#125; 另一种完全为非监督的方法，是选择产生最低重建误差的核和超参数。但是，重建并不像线性 PCA 那样容易。这里是原因：图 8-11 显示了原始瑞士卷 3D 数据集（左上角），并且使用 RBF 核应用 kPCA 后生成的二维数据集（右上角）。由于核技巧，这在数学上等同于使用特征映射φ将训练集映射到无限维特征空间（右下），然后使用线性 PCA 将变换的训练集投影到 2D。请注意，如果我们可以在缩减空间中对给定实例实现反向线性 PCA 步骤，则重构点将位于特征空间中，而不是位于原始空间中（例如，如图中由x表示的那样）。由于特征空间是无限维的，我们不能找出重建点，因此我们无法计算真实的重建误差。幸运的是，可以在原始空间中找到一个贴近重建点的点。这被称为重建前图像（reconstruction pre-image）。一旦你有这个前图像，你就可以测量其与原始实例的平方距离。然后，您可以选择最小化重建前图像错误的核和超参数。 您可能想知道如何进行这种重建。一种解决方案是训练一个监督回归模型，将预计实例作为训练集，并将原始实例作为训练目标。如果您设置了fit_inverse_transform = True，Scikit-Learn 将自动执行此操作，代码如下所示： 123rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433,fit_inverse_transform=True)X_reduced = rbf_pca.fit_transform(X)X_preimage = rbf_pca.inverse_transform(X_reduced) 概述：默认条件下，fit_inverse_transform = False并且KernelPCA没有inverse_tranfrom()方法。这种方法仅仅当fit_inverse_transform = True的情况下才会创建。 你可以计算重建前图像误差： 12&gt;&gt;&gt; from sklearn.metrics import mean_squared_error&gt;&gt;&gt; mean_squared_error(X, X_preimage) 32.786308795766132 现在你可以使用交叉验证的方格搜索来寻找可以最小化重建前图像误差的核方法和超参数。 局部线性嵌入LLE局部线性嵌入（Locally Linear Embedding）是另一种非常有效的非线性降维（NLDR）方法。这是一种流形学习技术，不依赖于像以前算法那样的投影。简而言之，LLE 首先测量每个训练实例与其最近邻（c.n.）之间的线性关系，然后寻找能最好地保留这些局部关系的训练集的低维表示（稍后会详细介绍） 。这使得它特别擅长展开扭曲的流形，尤其是在没有太多噪音的情况下。 例如，以下代码使用 Scikit-Learn 的LocallyLinearEmbedding类来展开瑞士卷。得到的二维数据集如图所示。正如您所看到的，瑞士卷被完全展开，实例之间的距离保存得很好。但是，距离不能在较大范围内保留的很好：展开的瑞士卷的左侧被挤压，而右侧的部分被拉长。尽管如此，LLE 在对流形建模方面做得非常好。 1234from sklearn.manifold import LocallyLinearEmbeddinglle=LocallyLinearEmbedding(n_components=2,n_neighbors=10)X_reduced=lle.fit_transform(X) 其他降维方法多维缩放（MDS）在尝试保持实例之间距离的同时降低了维度 Isomap 通过将每个实例连接到最近的邻居来创建图形，然后在尝试保持实例之间的测地距离时降低维度。 t-分布随机邻域嵌入（t-Distributed Stochastic Neighbor Embedding，t-SNE）可以用于降低维​​度，同时试图保持相似的实例临近并将不相似的实例分开。它主要用于可视化，尤其是用于可视化高维空间中的实例（例如，可以将MNIST图像降维到 2D 可视化）。 线性判别分析（Linear Discriminant Analysis，LDA）实际上是一种分类算法，但在训练过程中，它会学习类之间最有区别的轴，然后使用这些轴来定义用于投影数据的超平面。LDA 的好处是投影会尽可能地保持各个类之间距离，所以在运行另一种分类算法（如 SVM 分类器）之前，LDA 是很好的降维技术。 练习题 减少数据集维度的主要动机是什么？主要缺点是什么？ 什么是维度爆炸？ 一旦对某数据集降维，我们可能恢复它吗？如果可以，怎样做才能恢复？如果不可以，为什么？ PCA 可以用于降低一个高度非线性对数据集吗？ 假设你对一个 1000 维的数据集应用 PCA，同时设置方差解释率为 95%，你的最终数据集将会有多少维？ 在什么情况下你会使用普通的 PCA，增量 PCA，随机 PCA 和核 PCA？ 你该如何评价你的降维算法在你数据集上的表现？ 将两个不同的降维算法串联使用有意义吗？ 1、动机：为了加速后续训练算法（在某些情况下，它甚至可以消除噪声和冗余特征，使训练算法表现更好）； 通过可视化数据深入了解最重要的特征； 节省空间（压缩）。缺点：某些信息丢失，可能会降低后续训练算法的性能； 计算密集； 为机器学习管道增加了一些复杂性；转换后的功能通常难以解释。 2、在机器学习中，一个常见的表现是随机采样的高维向量通常非常稀疏，增加了过拟合的风险，并且在没有足够的训练数据的情况下很难识别数据中的模式。 3、一旦使用我们讨论过的算法减少了数据集的维数，几乎总是不可能完全逆转操作，因为在降维期间某些信息会丢失。 此外，虽然一些算法（例如PCA）具有可以重建与原始数据相对类似的数据集的简单反向变换过程，但是其他算法（例如T-SNE）则不然。 4、PCA可用于显着降低大多数数据集的维度，即使它们是高度非线性的，因为它至少可以消除无用的维度。 但是，如果没有无用的维度 - 例如，瑞士卷 - 那么使用PCA降低维数将失去太多信息。 你想要展开瑞士卷，而不是挤压它。 5、这是一个棘手的问题：它取决于数据集。 让我们看看两个极端的例子。 首先，假设数据集由几乎完全对齐的点组成。 在这种情况下，PCA可以将数据集减少到一维，同时仍然保留95％的方差。 现在想象一下，数据集由完全随机的点组成，分散在1000个维度周围。 在这种情况下，需要所有1,000个维度来保持95％的方差。 所以答案是，它取决于数据集，它可以是1到1,000之间的任何数字。 将解释的方差绘制为维数的函数是一种粗略了解数据集内在维度的方法。 6、常规PCA是默认值，但仅当数据集有足够内存时才有效。 当您需要在每次新实例到达时动态应用PCA，增量PCA对于在线任务很有用。 当您想要显着降低维度并且有足够内存时，随机PCA非常有用; 在这种情况下，它比普通PCA快得多。 最后，Kernel PCA对非线性数据集非常有用。 7、直观地说，如果从数据集中消除了大量维度而不会丢失太多信息，则降维算法表现良好。 衡量这种情况的一种方法是应用反向变换并测量重建误差。 但是，并非所有降维算法都提供逆向变换。 8、链接两个不同的降维算法绝对有意义。 一个常见的例子是使用PCA快速摆脱大量无用的维度，然后应用另一个慢得多的降维算法，如LLE。 这种两步法可能会产生与仅使用LLE相同的性能，但只需要很短的时间。]]></content>
      <categories>
        <category>Sklearn 与 TensorFlow 机器学习实用指南</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>PCA</tag>
        <tag>MDS</tag>
        <tag>LLE</tag>
        <tag>Isomap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn 与 TensorFlow 机器学习实用指南（六）：集成学习]]></title>
    <url>%2F2018%2F08%2F04%2FSklearn%20%E4%B8%8E%20TensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%2FSklearn-%E4%B8%8E-TensorFlow-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，比单一学习器获得显著优越的泛化性能。想要获得好的集成，个体学习器应”好而不同“，要保证准确性和多样性。要产生好而不同的个体学习器，恰是集成学习研究的核心 目前集成学习可分为两大类，即个体学习器之间有依赖关系，必须串行生成的序列化方法；以及个体学习器不存在强依赖关系，可同时生成的并行化方法。前者的代表是Boosting，最著名的是代表有Adaboost, GBDT和XGBOOST;后者的代表是Bagging和随机森林。对于学习器的结合策略有三大类：投票法（分类），平均法（连续数值），学习法（Stacking） BoostingAdaboost算法思想Adaboost提升方法是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注，于是，分类问题就被一系列的弱分类器“分而治之”。另外，对于弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率较大的弱分类器的权值，使其在表决中起较小的作用。(两个权重，一个是样本权重，另外一个是分类器的权重) AdaboostBoost的算法的框架如下图所示 算法流程具体算法流程如下图所示： step2. 预期迭代T轮 step4. 计算分类器$h_t$的误差率$\epsilon_t$ \epsilon_t=P\left(h_t\left(x\right)\ne y_i\right)=\frac{\sum_{i=1}^m{w_{ki}I\left(h_t\left(x_i\right)\ne y_i\right)}}{\sum_{i=1}^m{w_{ki}}}=\sum_{i=1}^m{w_{ki}I\left(G_m\left(x_i\right)\ne y_i\right)}这里$w_{ki}$表示第k轮（第k个分类器）中第i个实例的权重，$\sum_{i=1}^m{w_{ki}=1}$，I表示指示函数，代表满足条件的样本。这表明，误差率是被$h_t$分类错误的样本的权重之和。（这些样本的权重会在后面归一化） step5. 分类器比随机分类效果还差则停止 step6. 根据误差率计算分类器的权重，表示最终分类器的重要程度。由表达式可知，当误差率小于等于1/2时，$\alpha_k$大于等于0。并且$\alpha_k$随着误差率的减小而增大，意味着误差越小的分类器最后的重要程度越大。 step7. 更新样本权重，$Z_k$为归一化因子，把最后的全部样本权重求和即可。 w_{k+1,i}=\frac{w_{ki}}{Z_k}exp(-\alpha_k y_i G_k(x_i))Adaboost算法优缺点Adaboost优点 不容易发生过拟合。 Adaboost是一种有很高精度的分类器。 当使用简单分类器时，计算出的结果是可理解的。 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。 Adaboost缺点 训练时间过长。 执行效果依赖于弱分类器的选择。 对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。 GBTDGBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。并且GBDT每一棵树都是回归树CART.。由于GBDT的核心在与累加所有树的结果作为最终结果，而分类树得到的离散分类结果对于预测分类并不是这么的容易叠加。这是区别于分类树的一个显著特征（毕竟男+女=是男是女?，这样的运算是毫无道理的），GBDT在运行时就使用到了回归树的这个性质，它将累加所有树的结果作为最终结果。所以GBDT中的树都是回归树，而不是分类树，它用来做回归预测，当然回归树经过调整之后也能用来做分类。 这里要先介绍GBDT简单版本的提升树Boosting Decision Tree，后面再介绍GBDT。 提升树Boosting Decision Tree提升树(Boosting Decision Tree)由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策（之前CART只是拟合一颗完整的回归树）。回归树的构造在上一节已经介绍过了，不再赘述。 我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。提升树模型可以表示为决策树的加法模型： f_M\left(x\right)=\sum_{m=1}^M{T\left(x;\varTheta_m\right)}其中$T\left(x;\varTheta_m\right)$表示决策树；$\varTheta_m$为决策树的参数；M为树的个数。 提升树的过程如下，节点下所有点的均值作为该节点的预测值，例如左图的15与25。（这里是将特征分开处理并缩小了树的规模，若用CART可能会出现深度为3的回归树） 回归问题的提升树算法叙述如下： 对比初始的CART回归树与GBDT所生成的回归树，可以发现，最终的结果可能是相同的，那我们为什么还要使用GBDT呢？ 答案就是对模型过拟合的考虑。过拟合是指为了让训练集精度更高，学到了很多“仅在训练集上成立的规律”，导致换一个数据集后，当前规律的预测精度就不足以使人满意了。毕竟，在训练精度和实际精度（或测试精度）之间，后者才是我们想要真正得到的。 在上面这个例子中，初始的回归树为达到100%精度使用了3个特征（上网时长、时段、网购金额），但观察发现，分枝“上网时长&gt;1.1h”很显然过拟合了，不排除恰好A上网1.5h, B上网1小时，所以用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的。 而在GBDT中，两棵回归树仅使用了两个特征（购物金额与对百度知道的使用方式）就实现了100%的预测精度，其分枝依据更合乎逻辑（当然这里是相比较于上网时长特征而言），算法在运行中也体现了“如无必要，勿增实体”的奥卡姆剃刀原理 梯度提升决策树Gradient Boosting Decision TreeGBDT回归算法提升树利用加法模型与向前分布算法实现学习的优化过程，即是通过迭代得到一系列的弱分类器，进而通过不同的组合策略得到相应的强学习器。在GBDT的迭代中，假设前一轮得到的强学习器为$f_{t−1}(x)$ ，对应的损失函数则为$L(y,f_{t−1}(x))$ 。因此新一轮迭代的目的就是找到一个弱学习器$h_t(x)$ ，使得损失函$L(y,f_{t−1}(x)+h_t(x))$ 达到最小。因此问题的关键就在于对损失函数的度量，这也正是难点所在。当损失函数是平方损失和指数损失时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化没那么容易，如绝对值损失函数和Huber损失函数。常见的损失函数及其梯度如下表所示： 那我们怎么样才能找到一种通用的拟合方法呢？针对这一问题，Freidman提出了梯度提升算法：利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的扶梯度表示为，右下角的等式是求偏导后带入计算的 r_{ti}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{t-1}\;\,\left(x\right)}负梯度作为回归问题中提升树算法的残差的近似值（与其说负梯度作为残差的近似值，不如说残差是负梯度的一种特例，拟合一个回归树），这就是梯度提升决策树。假设样本数据为m，最大的迭代次数为T。其算法过程如下： step1. 初始化弱分类器，计算出使损失函数极小化的一个常数值c，此时树仅有一个根结点。c的均值可取样本y的均值。这个初始化得到的c将用于第一次计算负梯度$f(x_i)=f(t-1)=f(0)$的代入计算得到残差近似值$r_{ti}$。(第t代第i个样本) step2(a) .计算每个样本的$r_{ti}$ step2(b). 利用$(x_i,r_{ti})i=1,2,3,…,m$，我们可以拟合一颗CART回归树，得到第t棵回归树（注意回归树节点内的均值求法不再是我们想要的了），其对应的叶节点区域为$R_{tj},j=1,2,3,…,J$，其中J为叶子节点的个数。 step2(c). 接下来，针对每一个叶子节点中的样本，要拟合叶子结点最好的输出值$c_{tj}$(不再是简单的求节点均值)，使得求出的损失函数最小。回顾之前写的新一轮迭代的目的，这时候的输出值$c_{tj}$组成就是我们想要的第t棵弱学习器$h_t(x)$。其实就是在上一棵强学习器树稍加改变决策树中叶节点值，希望拟合的误差越来越小。 c_{tj}=\underset{c}{\underbrace{\arg\min}} \sum_{x_i \in R_{tj}}L(y_i,f_{t-1}(x_i)+c)这样我们便得到本轮的弱学习器决策树拟合函数 h_t(x)=\sum _{j=1} ^{J} c_{tj},I(x \in R_{tj}) step2(d). 更新强学习器，上一个强学习器+弱学习器 f_t(x)=f_{t-1}(x)+\sum_{j=1}^{J}c_{tj},I(x\in R_{tj}) step3. 得到输出的最后一轮的最终强学习器模型（最大迭代T轮） f(x)=f_T(x)=f_0(x)+\sum_{t=1}^{T}\sum_{j=1}^{J}c_{tj},I(x\in R_{tj})GBDT分类算法GBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。 二元GBDT分类算法对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为 L(y,f(x))=log(1+exp(-yf(x)))其中y∈{−1,1}。则此时的负梯度误差为 r_{ti}=-\left[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]_{f(x)=f_{t-1}(x)}=\frac{y_i}{1+exp(y_if(x_i))}对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 c_{tj}=\underset{c}{\underbrace{\arg\min}} \sum _{x_i\in R_{tj}}log(1+exp(-y_i(f_{t-1}(x_i)+c)))由于上式比较难优化，我们一般使用近似值代替 c_{tj}=\frac{\sum _{x_i\in R_{tj}}r_{ti}}{\sum _{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)}除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。 多元GBDT分类算法多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为 L(y,f(x))=-\sum_{k=1}^{K}y_k log(p_k(x))其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为 p_k(x)=\frac {exp(f_k(x))}{\sum _{l=1}^{K}exp(f_l(x))}集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为 r_{til}=-\left[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]_{f_k(x)=f_{t-1,l}(x)}=y_{il}-p_{t-1,l}(x_i)其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为 c_{tjl}=\underset{cjl}{\underbrace{\arg\min}} \sum_{i=1}^{m} \sum_{k=1}^{K}L(y_k,f_{t-1,l}(x))+\sum _{j=1}^{J}c_{jl},I(x_i\in R_{tj})由于上式比较难优化，我们用近似值代替 c_{tjl}=\frac{K-1}{K}=\frac{\sum_{x_i\in R_{tjl}}r_{til}}{\sum _{x_i\in R_{til}}|r_{til}|(1-|r_{til}|)}除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。 XGBoost数据建模中，经常采用Boosting方法通过将成百上千个分类准确率较低的树模型组合起来，成为一个准确率很高的预测模型。这个模型会不断地迭代，每次迭代就生成一颗新的树。但在数据集较复杂的时候，可能需要几千次迭代运算，这将造成巨大的计算瓶颈。 针对这个问题。华盛顿大学的陈天奇博士开发的XGBoost（eXtreme Gradient Boosting）基于C++通过多线程实现了回归树的并行构建，并在原有Gradient Boosting算法基础上加以改进，从而极大地提升了模型训练速度和预测精度. 梯度下降与牛顿法在机器学习任务中，需要最小化损失函数L(θ)，其中θ是要求解的模型参数。梯度下降法常用来求解这种无约束最优化问题，它是一种迭代方法：选取初值$θ^0$ ,不断迭代，更新θ的值，进行损失函数的极小化。 迭代公式：$θ^t = θ_{t-1}+△θ$ 梯度下降：将$L(θ^t)$ 在$θ^{t-1}$ 处进行一阶泰勒展开: L(θ^t)=L(θ^{t-1}+△θ)\approx L(θ^{t-1})+L'(θ^{t-1})△θ要使得$L(θ^t)&lt;L(θ^{t-1})$ ,可取$△θ=-\alpha L’(θ^{t-1})$ ，则$θ^t = θ^{t-1}-\alpha L’(θ^{t-1})$这里$\alpha$ 是步长，可通过line search确定，但一般直接赋一个小的数 牛顿法：将$L(θ^t)$ 在$θ^{t-1}$ 处进行二阶泰勒展开: L(θ^t)\approx L(θ^{t-1})+L'(θ^{t-1})△θ+L''(θ^{t-1})\frac{(△θ)^2}{2}可将一阶和二阶导数分别记为g 和 h,则: L(θ^t)\approx L(θ^{t-1})+g△θ+h\frac{(△θ)^2}{2}要使得$L(θ^t)$极小，即让$g△θ+h\frac{(△θ)^2}{2}$ 极小，可令其对△θ求偏导值为0，求得$△θ=-\frac{g}{h}$ ,故$θ^t = θ^{t-1}+△θ=θ^{t-1}-\frac{g}{h}$ ,将其推广到向量形式，有$θ^t = θ^{t-1}-H^{-1}g$ GBDT 在函数空间中利用梯度下降法进行优化XGBoost 在函数空间中用牛顿法进行优化 XGBoost的推导过程定义目标函数相比原始的GBDT，XGBoost的目标函数多了正则项，使得学习出来的模型更加不容易过拟合。有哪些指标可以衡量树的复杂度？树的深度，内部节点个数，叶子节点个数(T)，叶节点分数(w)…XGBoost采用的是：T代表叶子数量，w代表叶子预测权值 而XGBoost第t次迭代后，模型的预测等于前t-1次的模型预测加上第t棵树的预测 即每次迭代生成一棵新的回归树，从而使预测值不断逼近真实值（即进一步最小化目标函数）。每一次保留原来的模型不变，加入一个新的函数f到模型里面。其中$\hat{y}_i\left(t-1\right)$就是t-1轮的模型预测，$f_t{(x_i)}$为新t轮加入的预测函数。选取的$f_t{(x_i)}$必须使我们的目标函数尽量最大地降低（这里应用到了Boosting的基本思想，即当前的基学习器重点关注以前所有学习器犯错误的那些数据样本，以此来达到提升的效果） 此时目标损失函数可写作： 如果我们考虑平方误差作为损失函数，公式可改写为： 这里的化简没有看懂，感觉少了一项残差的平方。不过原等式$+f_t{(x_i)}$看做泰勒展开的$+\bigtriangleup x$ . 公式中$y_i,\widehat y_i$ 都是已知的，模型要学习的只有第t棵树$f_t$ .另外对于损失函数不是平方误差的情况，我们可以采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行下一步的计算。其中$g_i$和$h_i$为损失函数对$\widehat y_i^{t-1}$的一阶和二阶倒数。 这时候，所有东西都准备好了，最后我们怎么定义$f_t$呢？它可以代表一颗具有预测结果的树，即叶子节点有预测权重。我们定义w为树叶的权重序列，q为树的结构，那么q(x)代表样本x落在树叶的位置。 目标函数的最小化得到了目标函数，接下来是最关键的一步，在这种新的定义下，我们可以把目标函数进行如下改写，其中$I$被定义为每个叶子上面样本集合$I_j=\{i| q(x_i)=j\}$ 。$L(y_i,\widehat y_i^{t-1})$为真实值与前一个函数计算所得残差是已知的(我们都是在已知前一个树的情况下计算下一颗树的)，同时，在同一个叶子节点上的数的函数值是相同的，可以做合并，于是： 对上诉目标函数求导等于0，可以得到最后的结果： 得到这个目标函数，乍一看目标函数的计算与回归树的结构q函数没有什么关系，但是如果我们仔细回看目标函数的构成，就会发现其中$G_j$和$H_j$的取值是由第j个树叶上数据样本所决定的。而第jj个树上所具有的数据样本则是由树结构q函数决定的。也就是说，一旦回归树的结构q确定，那么相应的目标函数就能够根据上式计算出来。那么回归树的生成问题也就转换为找到一个最优的树结构q，使得它具有最小的目标函数。 计算求得的Obj目标函数代表了当指定一个树的结构的时候，目标函数上面最多减少多少。我们可以把它叫做结构分数（structure score）。可以把它认为是类似于基尼系数一样更加一般的对于树结构进行打分的函数。 当回归树的结构确定时，我们前面已经推导出其最优的叶节点分数以及对应的最小损失值，问题是怎么确定树的结构？才能让得到的结构分数最好，目标函数损失降低最大 。主要有以下两种方法 暴力枚举所有可能的树结构，选择损失值最小的 - NP难问题（树的结构有无穷种） 贪心法，每次尝试分裂一个叶节点，计算分裂前后的增益，选择增益最大的（主要） 确定树的结构（贪心法）分裂前后的增益怎么计算？ ID3算法采用信息增益 C4.5算法采用信息增益比 CART采用Gini系数 XGBoost采用上诉优化函数的打分 即每一次尝试区队已有的叶子加入一个分割。对于一个剧透的分割方案，我们可以获得的增益可以由如下公式计算得到： 这个公式形式上跟ID3算法（采用信息熵计算增益）或者CART算法（采用基尼指数计算增益） 是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的$\gamma$即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数$\lambda$，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。 但需要注意是：引入的分割不一定会使得情况变好，因为在引入分割的同时也引入新叶子的惩罚项。所以通常需要设定一个阈值，如果引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。此外在XGBoost的具体实践中，通常会设置树的深度来控制树的复杂度，避免单个树过于复杂带来的过拟合问题。 如何使用及参数 一些常见的问题1、机器学习算法中GBDT和XGBOOST的区别有哪些？ 2、为什么在实际的 kaggle 比赛中 gbdt 和 random forest 效果非常好？ 3、 为什么xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？ 这里就不花时间写了，可以参考知乎和一些博客文章。 Bagging和随机森林如果采样出的每个子集都完全不同，则每个基学习器只用到一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为了解决这个问题，我们可考虑使用相互有交叠的采样子集。 Bagging随机取出一个样本放入采样集中，再把该样本放回初始数据集。这样的自助采样过程还给 Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约 63.2%的样本，剩下约 36.8%的样本可用作验证集来对泛化性能进行包外估计(out-of-bag estimate)。Bagging通常对分类任务使用简单的投票法，对回归任务使用简单平均法。Bagging的算法描述如下所诉($D_{bs}$是自助采样产生的样本分布,输出采用投票法)： 随机森林随机森林(Random Forest)是Bagging的一个扩展变体。随机森林在以决策树为基学习器构建 Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性；而在随机森林中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令 k=d，则基决策树的构建与传统决策树相同；若令k=1,则是随机选择一个属性用于划分；一般情况下，推荐值 k=log2d.随机森林简单、容易实现、计算开销小. 随机森林的训练效率常优于 Bagging，因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的“随机型”决策树则只需考察一个属性子集。 由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，会像大数原理一样，抛硬币的次数接近无穷，其正反概率会越接近真实概率1/2。大部分的树会相互抵消，最后得到一个泛化较好的结果，从而得到一个好的预测结果。 学习器结合策略假定集成包含 T 个基学习器h1,h2,…ht,其中hi在示例x上的输出为hi(x)下面介绍几种对hi进行结合的常见策略。 投票法 1）硬投票 2）相对多数投票 3）加权投票 平均法 1）简单平均法 2）加权平均法（较好） 学习法 1）Stacking Stacking当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。Stacking是学习法的典型代表。这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习(metalearner)。 Stacking先从初始数据集训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记仍被当作样例标记。Stacking的算法描述如下，这里假定初级学习器使用不同的学习算法产生，即初级集成是异质的（初级学习器也可是同质的）。 train数据是初级训练器5折交叉得到的输出，test是初级训练器预测test后的均值 应用投票分类像我们之前讨论的一样，我们会在一个项目快结束的时候使用集成算法，一旦你建立了一些好的分类器，就把他们合并为一个更好的分类器。事实上，在机器学习竞赛中获得胜利的算法经常会包含一些集成方法。 接下来的代码创建和训练了在 sklearn 中的投票分类器。这个分类器由三个不同的分类器组成（训练集是第五章中的 moons 数据集）： 123456789&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier &gt;&gt;&gt; from sklearn.ensemble import VotingClassifier &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression &gt;&gt;&gt; from sklearn.svm import SVC&gt;&gt;&gt; log_clf = LogisticRegression() &gt;&gt;&gt; rnd_clf = RandomForestClassifier() &gt;&gt;&gt; svm_clf = SVC()&gt;&gt;&gt; voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), &gt;&gt;&gt; ('svc', svm_clf)],voting='hard') &gt;&gt;&gt; voting_clf.fit(X_train, y_train) 让我们看一下在测试集上的准确率： 123456789&gt;&gt;&gt; from sklearn.metrics import accuracy_score &gt;&gt;&gt; for clf in (log_clf, rnd_clf, svm_clf, voting_clf): &gt;&gt;&gt; clf.fit(X_train, y_train) &gt;&gt;&gt; y_pred = clf.predict(X_test) &gt;&gt;&gt; print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) LogisticRegression 0.864 RandomForestClassifier 0.872 SVC 0.888 VotingClassifier 0.896 你看！投票分类器比其他单独的分类器表现的都要好。 如果所有的分类器都能够预测类别的概率（例如他们有一个predict_proba()方法），那么你就可以让 sklearn 以最高的类概率来预测这个类，平均在所有的分类器上。这种方式叫做软投票。他经常比硬投票表现的更好，因为它给予高自信的投票更大的权重。你可以通过把voting=”hard”设置为voting=”soft”来保证分类器可以预测类别概率。然而这不是 SVC 类的分类器默认的选项，所以你需要把它的probability hyperparameter设置为True（这会使 SVC 使用交叉验证去预测类别概率，其降低了训练速度，但会添加predict_proba()方法）。如果你修改了之前的代码去使用软投票，你会发现投票分类器正确率高达 91% Bagging 和 Pasting就像之前讲到的，可以通过使用不同的训练算法去得到一些不同的分类器。另一种方法就是对每一个分类器都使用相同的训练算法，但是在不同的训练集上去训练它们。有放回采样被称为装袋（Bagging，是 bootstrap aggregating 的缩写）。无放回采样称为粘贴（pasting） 换句话说，Bagging 和 Pasting 都允许在多个分类器上对训练集进行多次采样，但只有 Bagging 允许对同一种分类器上对训练集进行进行多次采样。 当所有的分类器被训练后，集成可以通过对所有分类器结果的简单聚合来对新的实例进行预测。聚合函数通常对分类是统计模式（例如硬投票分类器）或者对回归是平均。每一个单独的分类器在如果在原始训练集上都是高偏差，但是聚合降低了偏差和方差。通常情况下，集成的结果是有一个相似的偏差，但是对比与在原始训练集上的单一分类器来讲有更小的方差。 sklearn 中的 Bagging 和 Pastingsklearn 为 Bagging 和 Pasting 提供了一个简单的API：BaggingClassifier类（或者对于回归可以是BaggingRegressor。接下来的代码训练了一个 500 个决策树分类器的集成，每一个都是在数据集上有放回采样 100 个训练实例下进行训练（这是 Bagging 的例子，如果你想尝试 Pasting，就设置bootstrap=False）。n_jobs参数告诉 sklearn 用于训练和预测所需要 CPU 核的数量。（-1 代表着 sklearn 会使用所有空闲核）： 12345&gt;&gt;&gt;from sklearn.ensemble import BaggingClassifier &gt;&gt;&gt;from sklearn.tree import DecisionTreeClassifier&gt;&gt;&gt;bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1) &gt;&gt;&gt;bag_clf.fit(X_train, y_train) &gt;&gt;&gt;y_pred = bag_clf.predict(X_test) 如果基分类器可以预测类别概率（例如它拥有predict_proba()方法），那么BaggingClassifier会自动的运行软投票，这是决策树分类器的情况。 下图对比了单一决策树的决策边界和 Bagging 集成 500 个树的决策边界，两者都在 moons 数据集上训练。正如你所看到的，集成的分类比起单一决策树的分类产生情况更好：集成有一个可比较的偏差但是有一个较小的方差（它在训练集上的错误数目大致相同，但决策边界较不规则）。 Bootstrap 在每个预测器被训练的子集中引入了更多的分集，所以 Bagging 结束时的偏差比 Pasting 更高，但这也意味着预测因子最终变得不相关，从而减少了集合的方差。总体而言，Bagging 通常会导致更好的模型，这就解释了为什么它通常是首选的。然而，如果你有空闲时间和 CPU 功率，可以使用交叉验证来评估 Bagging 和 Pasting 哪一个更好。 Out-of-Bag 评价对于 Bagging 来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。BaggingClassifier默认采样。BaggingClassifier默认是有放回的采样m个实例 （bootstrap=True），其中m是训练集的大小，这意味着平均下来只有63%的训练实例被每个分类器采样，剩下的37%个没有被采样的训练实例就叫做 Out-of-Bag 实例。注意对于每一个的分类器它们的 37% 不是相同的。 因为在训练中分类器从来没有看到过 oob 实例，所以它可以在这些实例上进行评估，而不需要单独的验证集或交叉验证。你可以拿出每一个分类器的 oob 来评估集成本身。 在 sklearn 中，你可以在训练后需要创建一个BaggingClassifier来自动评估时设置oob_score=True来自动评估。接下来的代码展示了这个操作。评估结果通过变量oob_score_来显示： 1234&gt;&gt;&gt; bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,bootstrap=True, n_jobs=-1, oob_score=True)&gt;&gt;&gt; bag_clf.fit(X_train, y_train) &gt;&gt;&gt; bag_clf.oob_score_ 0.93066666666666664 根据这个 obb 评估，BaggingClassifier可以再测试集上达到93.1%的准确率，让我们修改一下： 1234&gt;&gt;&gt; from sklearn.metrics import accuracy_score &gt;&gt;&gt; y_pred = bag_clf.predict(X_test) &gt;&gt;&gt; accuracy_score(y_test, y_pred) 0.93600000000000005 我们在测试集上得到了 93.6% 的准确率，足够接近了！ 对于每个训练实例 oob 决策函数也可通过oob_decision_function_变量来展示。在这种情况下（当基决策器有predict_proba()时）决策函数会对每个训练实例返回类别概率。例如，oob 评估预测第二个训练实例有 60.6% 的概率属于正类（39.4% 属于负类）： 123&gt;&gt;&gt; bag_clf.oob_decision_function_ array([[ 0., 1.], [ 0.60588235, 0.39411765],[ 1., 0. ], ... [ 1. , 0. ],[ 0., 1.],[ 0.48958333, 0.51041667]]) 随机森林正如我们所讨论的，随机森林是决策树的一种集成，通常是通过 bagging 方法（有时是 pasting 方法）进行训练，通常用max_samples设置为训练集的大小。与建立一个BaggingClassifier然后把它放入 DecisionTreeClassifier 相反，你可以使用更方便的也是对决策树优化够的RandomForestClassifier（对于回归是RandomForestRegressor）。接下来的代码训练了带有 500 个树（每个被限制为 16 叶子结点）的决策森林，使用所有空闲的 CPU 核： 1234&gt;&gt;&gt;from sklearn.ensemble import RandomForestClassifier&gt;&gt;&gt;rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) &gt;&gt;&gt;rnd_clf.fit(X_train, y_train)&gt;&gt;&gt;y_pred_rf = rnd_clf.predict(X_test) 除了一些例外，RandomForestClassifier使用DecisionTreeClassifier的所有超参数（决定数怎么生长），把BaggingClassifier的超参数加起来来控制集成本身 随机森林算法在树生长时引入了额外的随机；与在节点分裂时需要找到最好分裂特征相反，它在一个随机的特征集中找最好的特征。它导致了树的差异性，并且再一次用高偏差换低方差，总的来说是一个更好的模型。以下是BaggingClassifier大致相当于之前的randomforestclassifier： 1&gt;&gt;&gt;bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter="random", max_leaf_nodes=16),n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1) 极端随机树当你在随机森林上生长树时，在每个结点分裂时只考虑随机特征集上的特征（正如之前讨论过的一样）。相比于找到更好的特征我们可以通过使用对特征使用随机阈值使树更加随机（像规则决策树一样）。 这种极端随机的树被简称为 Extremely Randomized Trees（极端随机树），或者更简单的称为 Extra-Tree。再一次用高偏差换低方差。它还使得 Extra-Tree 比规则的随机森林更快地训练，因为在每个节点上找到每个特征的最佳阈值是生长树最耗时的任务之一。 你可以使用 sklearn 的ExtraTreesClassifier来创建一个 Extra-Tree 分类器。他的 API 跟RandomForestClassifier是相同的，相似的， ExtraTreesRegressor 跟RandomForestRegressor也是相同的 API。 我们很难去分辨ExtraTreesClassifier和RandomForestClassifier到底哪个更好。通常情况下是通过交叉验证来比较它们（使用网格搜索调整超参数） 特征重要度最后，如果你观察一个单一决策树，重要的特征会出现在更靠近根部的位置，而不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。sklearn 在训练后会自动计算每个特征的重要度。你可以通过feature_importances_变量来查看结果。例如如下代码在 iris 数据集（第四章介绍）上训练了一个RandomForestClassifier模型，然后输出了每个特征的重要性。看来，最重要的特征是花瓣长度（44%）和宽度（42%），而萼片长度和宽度相对比较是不重要的（分别为 11% 和 2%）： 12345678910&gt;&gt;&gt; from sklearn.datasets import load_iris &gt;&gt;&gt; iris = load_iris() &gt;&gt;&gt; rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1) &gt;&gt;&gt; rnd_clf.fit(iris["data"], iris["target"]) &gt;&gt;&gt; for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_): &gt;&gt;&gt; print(name, score) sepal length (cm) 0.112492250999sepal width (cm) 0.0231192882825 petal length (cm) 0.441030464364 petal width (cm) 0.423357996355 相似的，如果你在 MNIST 数据及上训练随机森林分类器（在第三章上介绍），然后画出每个像素的重要性，你可以得到下图 随机森林可以非常方便快速得了解哪些特征实际上是重要的，特别是你需要进行特征选择的时候 提升下图显示连续五次预测的 moons 数据集的决策边界（在本例中，每一个分类器都是高度正则化带有 RBF 核的 SVM）。第一个分类器误分类了很多实例，所以它们的权重被提升了。第二个分类器因此对这些误分类的实例分类效果更好，以此类推。右边的图代表了除了学习率减半外（误分类实例权重每次迭代上升一半）相同的预测序列（误分类的样本权重提升速率即学习率）。你可以看出，序列学习技术与梯度下降很相似，除了调整单个预测因子的参数以最小化代价函数之外，AdaBoost 增加了集合的预测器，逐渐使其更好。 klearn 通常使用 Adaboost 的多分类版本 SAMME（这就代表了 分段加建模使用多类指数损失函数）。如果只有两类别，那么 SAMME 是与 Adaboost 相同的。如果分类器可以预测类别概率（例如如果它们有predict_proba()），如果 sklearn 可以使用 SAMME 叫做SAMME.R的变量（R 代表“REAL”），这种依赖于类别概率的通常比依赖于分类器的更好。 接下来的代码训练了使用 sklearn 的AdaBoostClassifier基于 200 个决策树桩 Adaboost 分类器（正如你说期待的，对于回归也有AdaBoostRegressor）。一个决策树桩是max_depth=1的决策树-换句话说，是一个单一的决策节点加上两个叶子结点。这就是AdaBoostClassifier的默认基分类器： 123&gt;&gt;&gt;from sklearn.ensemble import AdaBoostClassifier&gt;&gt;&gt;ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,algorithm="SAMME.R", learning_rate=0.5) &gt;&gt;&gt;ada_clf.fit(X_train, y_train) 如果你的 Adaboost 集成过拟合了训练集，你可以尝试减少基分类器的数量或者对基分类器使用更强的正则化。 梯度提升另一个非常著名的提升算法是梯度提升。与 Adaboost 一样，梯度提升也是通过向集成中逐步增加分类器运行的，每一个分类器都修正之前的分类结果。然而，它并不像 Adaboost 那样每一次迭代都更改实例的权重，这个方法是去使用新的分类器去拟合前面分类器预测的残差 。 让我们通过一个使用决策树当做基分类器的简单的回归例子（回归当然也可以使用梯度提升）。这被叫做梯度提升回归树（GBRT，Gradient Tree Boosting 或者 Gradient Boosted Regression Trees）。首先我们用DecisionTreeRegressor去拟合训练集（例如一个有噪二次训练集）： 123&gt;&gt;&gt;from sklearn.tree import DecisionTreeRegressor &gt;&gt;&gt;tree_reg1 = DecisionTreeRegressor(max_depth=2) &gt;&gt;&gt;tree_reg1.fit(X, y) 现在在第一个分类器的残差上训练第二个分类器： 123&gt;&gt;&gt;y2 = y - tree_reg1.predict(X) &gt;&gt;&gt;tree_reg2 = DecisionTreeRegressor(max_depth=2) &gt;&gt;&gt;tree_reg2.fit(X, y2) 随后在第二个分类器的残差上训练第三个分类器： 123&gt;&gt;&gt;y3 = y2 - tree_reg1.predict(X) &gt;&gt;&gt;tree_reg3 = DecisionTreeRegressor(max_depth=2) &gt;&gt;&gt;tree_reg3.fit(X, y3) 现在我们有了一个包含三个回归器的集成。它可以通过集成所有树的预测来在一个新的实例上进行预测。 1&gt;&gt;&gt;y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)) 下图左栏展示了这三个树的预测，在右栏展示了集成的预测。在第一行，集成只有一个树，所以它与第一个树的预测相似。在第二行，一个新的树在第一个树的残差上进行训练。在右边栏可以看出集成的预测等于前两个树预测的和。相同的，在第三行另一个树在第二个数的残差上训练。你可以看到集成的预测会变的更好。 我们可以使用 sklean 中的GradientBoostingRegressor来训练 GBRT 集成。与RandomForestClassifier相似，它也有超参数去控制决策树的生长（例如max_depth，min_samples_leaf等等），也有超参数去控制集成训练，例如基分类器的数量（n_estimators）。接下来的代码创建了与之前相同的集成： 123&gt;&gt;&gt;from sklearn.ensemble import GradientBoostingRegressor&gt;&gt;&gt;gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) &gt;&gt;&gt;gbrt.fit(X, y) 超参数learning_rate 确立了每个树的贡献。如果你把它设置为一个很小的树，例如 0.1，在集成中就需要更多的树去拟合训练集，但预测通常会更好。这个正则化技术叫做 shrinkage。下图 展示了两个在低学习率上训练的 GBRT 集成：其中左面是一个没有足够树去拟合训练集的树，右面是有过多的树过拟合训练集的树。 为了找到树的最优数量，你可以使用早停技术。最简单使用这个技术的方法就是使用staged_predict()：它在训练的每个阶段（用一棵树，两棵树等）返回一个迭代器。加下来的代码用 120 个树训练了一个 GBRT 集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用 GBRT 树的最优数量训练另一个集成 123456789101112&gt;&gt;&gt;import numpy as np &gt;&gt;&gt;from sklearn.model_selection import train_test_split&gt;&gt;&gt;from sklearn.metrics import mean_squared_error&gt;&gt;&gt;X_train, X_val, y_train, y_val = train_test_split(X, y)&gt;&gt;&gt;gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120) &gt;&gt;&gt;gbrt.fit(X_train, y_train)&gt;&gt;&gt;errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)] &gt;&gt;&gt;bst_n_estimators = np.argmin(errors)&gt;&gt;&gt;gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) &gt;&gt;&gt;gbrt_best.fit(X_train, y_train) 验证错误在图的左面展示，最优模型预测被展示在右面 你也可以早早的停止训练来实现早停（与先在一大堆树中训练，然后再回头去找最优数目相反）。你可以通过设置warm_start=True来实现 ，这使得当fit()方法被调用时 sklearn 保留现有树，并允许增量训练。接下来的代码在当一行中的五次迭代验证错误没有改善时会停止训练： 123456789101112131415&gt;&gt;&gt;gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)min_val_error = float("inf") error_going_up = 0 for n_estimators in range(1, 120): gbrt.n_estimators = n_estimators gbrt.fit(X_train, y_train) y_pred = gbrt.predict(X_val) val_error = mean_squared_error(y_val, y_pred) if val_error &lt; min_val_error: min_val_error = val_error error_going_up = 0 else: error_going_up += 1 if error_going_up == 5: break # early stopping GradientBoostingRegressor也支持指定用于训练每棵树的训练实例比例的超参数subsample。例如如果subsample=0.25，那么每个树都会在 25% 随机选择的训练实例上训练。你现在也能猜出来，这也是个高偏差换低方差的作用。它同样也加速了训练。这个技术叫做随机梯度提升。 也可能对其他损失函数使用梯度提升。这是由损失超参数控制（见 sklearn 文档）。 Stacking本章讨论的最后一个集成方法叫做 Stacking（stacked generalization 的缩写）。这个算法基于一个简单的想法：不使用琐碎的函数（如硬投票）来聚合集合中所有分类器的预测，我们为什么不训练一个模型来执行这个聚合？图 展示了这样一个在新的回归实例上预测的集成。底部三个分类器每一个都有不同的值（3.1，2.7 和 2.9），然后最后一个分类器（叫做 blender 或者 meta learner ）把这三个分类器的结果当做输入然后做出最终决策（3.0） 练习题 如果你在相同训练集上训练 5 个不同的模型，它们都有 95% 的准确率，那么你是否可以通过组合这个模型来得到更好的结果？如果可以那怎么做呢？如果不可以请给出理由。 软投票和硬投票分类器之间有什么区别？ 是否有可能通过分配多个服务器来加速 bagging 集成系统的训练？pasting 集成，boosting 集成，随机森林，或 stacking 集成怎么样？ out-of-bag 评价的好处是什么？ 是什么使 极端随机树Extra-Tree 比规则随机森林更随机呢？这个额外的随机有什么帮助呢？那这个 Extra-Tree 比规则随机森林谁更快呢？ 如果你的 Adaboost 模型欠拟合，那么你需要怎么调整超参数？ 如果你的梯度提升过拟合，那么你应该调高还是调低学习率呢？ 1、只要模型间多样性较大，组合成一个集合模型，会起到一定的效果。 2、硬投票分类器只计算集成中每个分类器的投票数，并选择得票最多的类。 软投票分类器计算每个类的平均估计概率，并选择具有最高概率的类。 软投票使概率大的类别权重更高，通常表现更好，但只有在可以估计类概率的分类器时才有效（例如，对于Scikit-Learn中的SVM分类器，您必须设置probability = True）。 3、 bagging，pasting，随机森林是可以的。boosting 集成因为学习器需要基于先前的学习器构建，因此训练是连续的，故不适合。至于stacking 集成，给定层中的所有预测变量彼此独立，因此可以在多个服务器上并行训练它们。 但是，一层中的预测变量只能在前一层中的预测变量都经过训练后才能进行训练。 4、未被训练过得实例可以当做验证集 5、在随机森林中，只考虑特征的随机子集并在每个节点处进行分割。 对于Extra-Trees也是如此，但它们更进一步：不是像常规决策树一样搜索最佳阈值，而是为每个特征使用随机阈值。 这种额外的随机性就像一种正则化的形式：如果随机森林过度拟合训练数据，极端随机树可能表现更好。 此外，由于Extra-Trees不会搜索最佳阈值，因此它们比随机森林训练要快得多。 然而，在做出预测时，它们既不比随机森林更快也不慢。 6、如果您的AdaBoost集合欠拟合，可以尝试增加学习器的数量或减少基本学习器的正则化超参数。 也可以尝试稍微提高学习率。 7、如果您的Gradient Boosting过拟合，应该尝试降低学习率。 你也可以使用早期停止法。]]></content>
      <categories>
        <category>Sklearn 与 TensorFlow 机器学习实用指南</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>CART</tag>
        <tag>Adaboost</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
        <tag>Bagging</tag>
        <tag>随机森林</tag>
        <tag>Stacking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn 与 TensorFlow 机器学习实用指南（五）：决策树]]></title>
    <url>%2F2018%2F07%2F23%2FSklearn%20%E4%B8%8E%20TensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%2FSklearn-%E4%B8%8E-TensorFlow-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[一些基本概念决策树简述决策树（Decision Tree）是数据挖掘中一种基本的分类和回归方法，它呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是if−thenif−then规则的集合。决策树模型的主要优点是模型具有可读性，分类速度快。在学习时，利用训练数据，根据损失函数最小化原则建立决策树模型；而在预测时，对新的数据，利用决策树模型进行分类。主要的决策树算法有ID3算法、C4.5算法和CART算法。一个决策树的学习过程包括三个步骤：特征选择、决策树的生成以及决策树的修剪。 信息熵信息熵是衡量样本纯度的一种指标，嘉定当前样本集合D中第k类样本所占的比例为$p_k(k=1,2,…,|y|)$,则D的信息熵定义为 Ent(D)=-\sum_{k=1}^{\vert y\vert}p_klog_2{p_k}Ent(D)的值越小，则D的纯度越高。 以周志华西瓜书P76的西瓜数据集中的17个样本数据为例子。 显然，标签类别|y|=2.其中正例占$p_1$=8/17，反例占$p_2$=9/17。于是根节点的信息熵为 Ent(D)=-\sum_{k=1}^2p_klog_2^{p_k}=-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})=0.998条件熵$Ent(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望 Ent\left( Y|X \right) =\sum_{i=1}^n{p_iEnt\left( Y|X=x_i \right)}特征选择信息增益信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。特征A对训练数据集D的信息增益Gain(D,a)定义为集合D的经验熵$Ent(D)$与特征A给定条件下D的经验条件熵$Ent(Y | X)$之差，即假定离散属性a有n个可能的取值${a^1,a^2,…a^n}$，若使用属性a的取值来对样本集合划分，则会产生n个分支节点子集$D_1,D_2,..,Dn$，$|D_i|$ 为$D_i$的样本个数。(考虑不同的分支节点所包含的样本数不同，用$\frac{|D^i|}{|D|}$代替期望概率$p_i$，即样本越多的分支节点的影响越大) Gain(D,a)=Ent(D)-\sum_{i=1}^n\frac{|D^i|}{|D|}End(D^i)以西瓜数据集的“色泽”属性为例，它有3个可能的取值{青绿，乌黑，浅白}。若使用该属性对D角线划分，则可以得到三个子集，分布记为$D^1$(色泽=青绿)={1,4,6,10,13,17}，正反例率分别为$p_1=\frac{3}{6}$，$p_2=\frac{3}{6}$；$D^2$(色泽=乌黑)={2,3,7,8,9,15}，正反例率分别为$p_1=\frac{4}{6}$，$p_2=\frac{2}{6}$ ；$D^3$(色泽=乌黑)={5,11,12,14,16}，正反例率分别为$p_1=\frac{1}{5}$，$p_2=\frac{4}{5}$。 首先，根据信息熵公式可计算出“色泽”划分之后所获得的3个分支节点的信息熵为 Ent(D^1)=-(\frac{3}{6}log_2\frac{3}{6}+\frac{3}{6}log_2\frac{3}{6})=1.000 \\ \\ \\ Ent(D^2)=-(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})=0.918 \\ \\ \\ Ent(D^3)=-(\frac{1}{5}log_2\frac{1}{5}+\frac{4}{5}log_2\frac{4}{5})=0.722之后根据上面公式可计算出属性“色泽”的信息增益为 Gain(D,色泽)=Ent(D)-\sum_{i=1}^3\frac{|D^i|}{|D|}Ent(D^i) \\ \\ \\ =0.998-((\frac{6}{17}\times1.000+\frac{6}{17}\times0.918+\frac{5}{17}\times0.722)=0.109一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。其中ID3决策树就是以信息增益为准则来选择划分属性。根据得到的属性最大信息增益来划分结果示例如下所示。之后再根据子集样本进一步划分，直到只有一个样本个体或者样本个体都为同一类标签为止。 若我们把样本编号1-17也作为一个划分属性，则根据信息增益公式可计算出它的信息增益为0.998，远大于其他候选划分属性。这很容易理解，“编号”将产生17个分支，每个分支仅包含一个样本，这些分支节点纯度已打最大。然而这些侧介绍显然不具有泛化能力。 所以实际上，信息增益准则对可取数目较多的属性有所偏好；为了减少这种偏好，著名的C4.5决策树算法不直接采用信息增益，而是使用增益率来做最优划分属性。 增益率增益率定义为其信息增益Gain(D，a)与训练数据集D关于特征a样本数的信息熵之比，增益率的数学定义为 Gain_{ratio}(D,a)=\frac{Gain(D,a)}{IV(a)} IV(a)=-\sum_{v=1}^n\frac{|D^i|}{|D|}log_2\frac{|D^i|}{|D|}（注意子集的划分是属性a样本的可取类别数目n，而不是标签类别|y|了啊），IV(a)可以成为属性a的内在信息，若属性a的可取数目越大，则IV(a)的值通常越大。这样就可以一定程度平衡信息增益对可取数目较多的属性的偏好。以“色泽”为例 IV(色泽)=-\frac{6}{17}log_2\frac{6}{17}-\frac{6}{17}log_2\frac{6}{17}-\frac{5}{17}log_2\frac{5}{17}=1.580另外，需要注意的是，增益率准则对可取数值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率的最高的。 基尼指数数据集D的纯度可以用基尼指数来度量，Gini（D）越小，则数据集D的纯度越高。假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼系数定义为 Gini\left( p \right) =\sum_{k=1}^K{p_k\left( 1-p_k \right) =1-\sum_{k=1}^K{p_k^2}}根据基尼指数定义，可以得到样本集合D的基尼指数，其中$D_k$表示数据集D中属于第k类的样本子集 Gini(D)=1-\sum_{k=1}^{K}\left(\frac{|D_k|}{|D|} \right)^2若样本集合D根据特征A是否取某一可能值a被分割成$D_1$和$D_2$两部分，即 D_1=\left\{ \left( x,y \right) \in D|A\left( x \right) =0 \right\} \mathrm{，}D_2=D-D_1则在特征A的条件下，集合D的基尼指数定义为 Gini\left( D,A \right) =\frac{|D_1|}{|D|}Gini\left( D_1 \right) +\frac{|D_2|}{|D|}Gini\left( D_2 \right)基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。 \min_{i\epsilon A}(Gain\_Gini(D,A)) \min_{A\epsilon Attribute}(\min_{i\epsilon A}(Gain\_Gini(D,A)))剪枝剪枝是决策树学习算法对付“过拟合”的主要手段。在决策树学习中，为了尽可能正确分类样本，节点划分过程不断重复，有时会造成决策树分支过多，这时有时候把自身特点当做所有数据都具有的一般性质而导致过拟合，因此，可以通过主动去掉一些分支来降低过拟合的风险 基本策略有预剪枝和后剪枝，预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分当前节点标记为叶节点；后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的字数替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点。 CART决策树的生成 ID3算法和C4.5按照信息增益和增益率最大的特征作为节点特征，然后递归构建。比较简单，还要注意这两种方法都容易过拟合。这里主要讲一下CART分类与回归树。 分类树与回归树（classification and regression tree，CART）模型（Breiman）由特征选择、树生成及剪枝组成，既可用于分类也可用于回归。CART算法采用二分递归分割的技术将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。 如果待预测分类是离散型数据，则CART生成分类决策树。 如果待预测分类是连续性数据，则CART生成回归决策树。 CART分类树对分类树用基尼系数（Gini index）最小化准则，进行特征选择，生成二叉树。 具体算法步骤如下： 1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为”是”或者“否”将D分割为D1和D2两部分，计算其基尼系数。 2）在所有可能的特征A以及他们所有可能的切分点a中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 3）对两个子结点递归地调用上述两个步骤，直至满足停止条件。 4）生成CART决策树 以生物特征分类数据为例 针对上述离散型数据，按照体温为恒温和非恒温(多类别属性也按’非’分成两类别)进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算以体温为划分特征D1,D2的基尼指数。 Gini(D_1)=1-[ (\frac{5}{7})^2+(\frac{2}{7})^2]=\frac{20}{49} Gini(D_2)=1-[ (\frac{3}{8})^2+(\frac{3}{8})^2+(\frac{2}{8})^2]=\frac{42}{64}然后计算得到特征体温下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分 Gain\_Gini(D,体温)=\frac{7}{15}*\frac{20}{49}+\frac{8}{15}*\frac{42}{64}在所有可能的特征A以及他们所有可能的切分点a中的二分划分中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点，依次递归。 CART回归树回归树衡量最好的标准不再是最大熵，而是最小化均方差。而且在每个节点（不一定是叶子节点）都会得一个预测值，这个预测值可为所有样本的平均值。我们利用最小二乘回归树生成算法来生成回归树f(x)，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。 回归树算法流程：j为选定的某个特征属性，s为在这个特征上的切分数值（需要遍历所有特征和切分点来选到最小化均方差），R1R2为切分的样本，C1C2为切分区域样本中的特征均值。 实例详解： 考虑如上所示的连续性变量，根据给定的数据点，考虑1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5共9个自定均分的切分点。对各切分点依次求出R1,R2,c1,c2及m(s)，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示 c_1=\frac{1}{N_m}\sum_{x_i\epsilon R_m(j,s)}y_i=\frac{1}{1}\sum_{x_i\epsilon R_1(1,1.5)}5.56=5.56 c_2=\frac{1}{N_m}\sum_{x_i\epsilon R_m(j,s)}y_i=\frac{1}{9}\sum_{x_i\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50 m(s)=\min_{j,s}[\min_{c_1}\sum _{x_i\epsilon R_i(j,s)}(y_i-c_1)^2+\min_{c_2}\sum _{x_i\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72依次改变(j,s)对，可以得到s及m(s)的计算结果，如下表所示。 当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。回归树T1(x)为 T_1(x)=\begin{cases} & 6.24,x]]></content>
      <categories>
        <category>Sklearn 与 TensorFlow 机器学习实用指南</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
        <tag>CART</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn 与 TensorFlow 机器学习实用指南（四）：支持向量机]]></title>
    <url>%2F2018%2F07%2F18%2FSklearn%20%E4%B8%8E%20TensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%2FSklearn-%E4%B8%8E-TensorFlow-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[推导支持向量机条件描述样本分类Y∈{+1， -1}是样本的标签，分别代表两个不同的类。这里我们需要用这些样本去训练学习一个线性分类器（超平面）：$f(x)=sgn(w^Tx + b)$，也就是$w^Tx + b$大于0的时候，输出+1，小于0的时候，输出-1。sgn()表示取符号。而$g(x) =w^Tx + b=0$就是我们要寻找的分类超平面 也就是，对于任何一个正样本$y_i$=+1，它都要处于超平面的一边，也就是要保证：$y= w^Tx + b&gt;0$。对于任何一个负样本$y_i=-1$，它都要处于超平面的另一边，也就是要保证：$y = w^Tx + b&lt;0$这两个约束，其实可以合并成同一个式子：$y_i (w^Tx_i+b)\geq0$ 最大间隔点到直线距离距离引理：设直线 L 的方程为Ax+By+C=0，点 P 的坐标为（Xo，Yo），则点 P 到直线 L 的距离为 \frac{\vert Ax_0+By_0+c\vert}{\sqrt{A^2+B^2}}间隔:间隔表示距离划分超平面最近的样本到划分超平面距离的两倍,即($x_i$为离直线最近的样本i) \gamma=2\min_i \frac{1}{\Vert w \Vert}\vert w^Tx_i+b \vert约束条件所以，线性支持向量机的目标是找到一组合适的参数(w, b), 在满足分割样本的条件下，使得上诉间隔最大 \max_{w,b} \min_i \frac{2}{\Vert w \Vert}\vert w^Tx_i+b \vert \\ \\ \\s.t. \quad y_i(w^T+b)>0, i=1,2,...,m上面的优化问题十分复杂, 难以处理. 为了能在现实中应用, 我们希望能对其做一些简化, 使其变为可以求解的, 经典的凸二次规划 (QP) 问题 凸二次规划的优化问题是指目标函数是凸二次函数, 约束是线性约束的一类优化问题 支持向量机的缩放引理：若（w* ,b*）是上面优化问题的解，那么对任何的r&gt;0,(rw*,rb*)仍是该问题的解。 由于对 (w, b) 的放缩不影响解, 为了简化优化问题,我们约束 (w, b) 使得最短距离固定为1. \min_i \quad \vert w^Tx_i+b \vert = 1所以最后我们的优化目标可以等价为下面优化目标： \min_{w,b} \quad \frac{1}{2}w^Tw \\ \\ \\s.t. \quad y_i(w^T+b)\geq1, i=1,2,...,m这是一个凸二次规划问题，除了用解决QP问题的常规方法之外，还可以应用拉格朗日对偶性，通过求解对偶问题得到最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一是对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。 拉格朗日函数与对偶形式拉格朗日函数对于优化问题 \min_{u} \quad f(u) \\ \\ \\s.t. \quad g_i(u)\leq 0, i=1,2,...,m \\ \\ \\ \qquad \ \ \ h_j(u)=0, j=1,2,...,n \qquad（6）定义其拉格朗日函数为 L(u,\alpha,\beta):=f(u)+\sum_{i=1}^m\alpha_ig_i(u)+\sum_{j=1}^n\beta_jh_j(u)\\ \\ \\s.t.\quad \alpha_i\geq0公式6的优化问题等价于 \min \limits_{u} \max \limits_{\alpha,\beta} \quad L(u,\alpha,\beta) \\ \\ \\ s.t. \alpha_i\geq0, \ \ i=1,2,...,m \qquad(8)其证明过程如下 其中，当$g_i$不满足约束时，即$g_i(u)&gt;0$,我们可以取$\alpha_i=\infty $;当$h_j$不满足约束时, 即$h_j(u)\neq 0$, 我们可以取$\beta_j=sign(h_j(u))\infty$，使得$\beta_jh_j(u)=\infty$. 当u满足约束时，由于$\alpha_i\geq 0,g_i(u)\leq 0$,则$\alpha_ig_i(u) \leq 0$.因此$\alpha_ig_i(u)$的最大值为0. KKT条件和对偶形式公式8描述的优化问题在最优值必须满足如下条件(KKT完整条件放后面) 主问题可行：$g_i(u)\leq 0,h_i(u)=0$; 对偶问题可行：$\alpha_i \geq 0$; 互补松弛：$\alpha_i g_i(u)=0$. 主问题可行和对偶问题可行是公式6和公式8描述的优化问题的约束项。松弛互补是在主问题和对偶问题都可行的条件下的最大值 在对偶问题里，公式6描述的优化问题，其等价形式公式8称为主问题，其对偶问题为(最大最小条件互换) \max \limits_{\alpha,\beta} \min \limits_{u} \quad L(u,\alpha,\beta) \\ \\ \\ s.t. \alpha_i\geq0, \ \ i=1,2,...,m 引理一：对偶问题是主 (primal) 问题的下界（最大值里取最小值肯定会大于最小值里取最大值） \max \limits_{\alpha,\beta} \min \limits_{u} \quad L(u,\alpha,\beta)\leq \min \limits_{u} \max \limits_{\alpha,\beta} \quad L(u,\alpha,\beta) 引理二(Slater条件)：当主问题为凸优化问题，即$f$和$g_i$ 为凸函数，$h_j$为仿射函数(类似于kx+b的一类函数)，且可行域中至少有一点使不等式约束严格成立时，对偶问题等价于原问题。(支持向量机的优化函数和约束函数都为凸函数) 线性支持向量机线性支持向量机的拉格朗日函数为 L(w,b,\alpha):=\frac{1}{2}w^tw+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b))其对偶问题为（并非主问题） \max \limits_{\alpha}\min\limits_{w,b}\frac{1}{2}w^Tw+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b)) \\ \\ \\ s.t.\quad \alpha_i\geq0,\quad i=1,2,...,m. \qquad(12)因为其内层的(w,b)属于无约束优化问题，我们可以通过令偏导等于0的方法得到(w,b)的最优值 \frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^m{a_iy_ix_i} \frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^m{a_iy_i=0}将上面公式带入(12)消去(w,b)即可得(变为min是因为化简过程有个负号，转为求最小) L\left( w,b,a \right) =\frac{1}{2}\sum_{i=1}^m{\sum_{j=1}^m{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^m{a_iy_i\left( \left( \sum_{j=1}^m{a_jy_jx_j} \right) ·x_i+b \right) +\sum_{i=1}^m{a_i}}}} \\ \\ \\ =-\frac{1}{2}\sum_{i=1}^m{\sum_{j=1}^m{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^m{a_i}}}故最终化简得到线性支持向量机对偶型 \min \limits_{\alpha} \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum_{i=1}^m\alpha_i \\ \\ \\ s.t.\sum_{i=1}^m\alpha_iy_i=0, \\ \\ \\ \alpha_i\geq 0,\quad i=1,2,...,m.上诉为线性支持向量机的最终化简对偶问题，其优化模型还要满足以下的KKT条件 主问题可行：$1-y_i(w^Tx_i+b)\leq0$ 对偶问题可行：$\alpha_i\geq0$ 互补松弛：$\alpha_i(1-y_i(w^Tx_i+b))=0$ 根据KKT条件，我们可以得到两个非常重要的结论（这么多证明得出两个结论，不容易啊） 结论一：落在间隔边界上的支持向量，其对应的样本的对偶变量$\alpha_i&gt;0$ 证明：由KKT可知，根据$\alpha_i(1-y_i(w^Tx_i+b))=0$ 当;$\alpha_i&gt;0$时，有$1-y_i(w^Tx_i+b)=0$ ，即$y_i(w^Tx_i+b)=1$ 结论二：支持向量机的参数(w,b)仅仅由支持向量决定，与其他样本无关 证明：由于对偶变量$\alpha_i&gt;0$对应的样本是支持向量， w=\sum_{i=1}^m\alpha_iy_ix_i=\sum_{i:\alpha_i=0}^m0\cdot y_ix_i+\sum_{i:\alpha_i>0}^m\alpha_iy_ix_i=\sum_{i\in SV}\alpha_iy_ix_i其中SV代表所有支持向量的集合.b可以由互补松弛算出，对于某一支持向量$x_s$ 及其标记$y_s$，由于$y_s(w^Tx_s+b)=1$, 则有 b=y_s-w^Tx_s=y_s-\sum_{i\in SV}\alpha_iy_ix_i^Tx_s根据对偶公式(16)先计算最优解$\alpha_1,\alpha_2,…,\alpha_m$， 然后可以得到w和b，这样我们就可以写出分类超平面$w^\ast\cdot x+b^\ast=0$ 和分类决策函数$f(x)=sign(w^\ast·x+b^\ast)$ 在实践中，为了得到对b更稳健的估计,通常使用对所有支持向量求解得到b的平均值 软间隔支持向量机对每个样本点引进一个松弛变量$\xi \geqslant 0 $，使函数间隔加上松弛变量大于等于1.这样，约束条件变成 y_i\left( w·x_i+b \right) \geqslant 1-\xi _i当然，如果我们允许$\xi \geqslant 0 $任意大的话，那任意的超平面都是符合条件的了。这里我们引入松弛变量$\xi$，松弛变量值越大，当样本违背约束的程度越大，当松弛变量为0时，样本分类正确。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi \geqslant 0 $的总和也要最小，优化最大间隔和违背约束程度越小，目标函数由原来的$\frac{1}{2}||w||^2$变成 \frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}这里，$C&gt;0$称为惩罚参数，一般事先由应用问题决定，用于权衡优化间隔和少量样本违背大间隔约束这两个目标，C越大时(看重这一项)对误分类的惩罚增大，，有更多的样本满足大间隔约束。C值小时对误分类的惩罚减小，允许有一些样本不满足大间隔约束.。最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量最小间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。 则有软间隔支持向量机基本型：软间隔支持向量机旨在找到一组合适的参数 (w, b), 使得 \underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^m{\xi _i} \\ \\ \\ s.t.\ \ y_i\left( w^T·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,m \\ \\ \\ \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}m \qquad(21)可证明w的解是唯一的，但b的解不唯一，b的解存在于一个区间。 用之前的方法将限制加入到目标函数中，得到如下原始最优化问题的拉格朗日函数(不等式为大于转为负号形式)： L\left( w,b,\xi ,\alpha,\beta \right) =\frac{1}{2}||w||^2+C\sum_{i=1}^m{\xi _i-\sum_{i=1}^m{\alpha_i\left( y_i\left( w^T·x_i+b \right) -1+\xi _i \right) -\sum_{i=1}^m{\beta_i\xi _i}}} \\ \\ \\ s.t.\alpha\geq0 ,\beta\geq0.其对偶问题为 \max_\limits{\alpha,\beta}\min_\limits{w,b,\xi} L\left( \alpha,\beta,w,b,\xi \right) \qquad(23)首先求拉格朗日函数针对$w,b,\xi$求极小 \frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^m{a_iy_ix_i} \\ \\ \\ \frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^m{\alpha_iy_i=0} \\ \\ \\ \frac{\partial L}{\partial \xi _i}=0\Rightarrow C-\alpha_i-\beta_i=0，i=1,2,3···,m \qquad(24)其中，有$\alpha\geq0,\beta\geq0$。因为$\beta_i=C-\alpha_i\geq0$，不失一般性，我们可以约束$0\leq\alpha_i\leq C$，从而去掉变量$\beta_i$.将这些求解变量带入拉格朗日对偶型即公式(23)，得到和原来一样的目标函数，唯一的区别就是现在拉格朗日乘子$\alpha$多了一个上限C。 最终化简的软间隔支持向量机对偶型问题(软间隔支持向量机的对偶问题等价于找到一组合适的$\alpha$使得) \underset{\alpha}{\min}\ \ \frac{1}{2}\sum_{i=1}^m{\sum_{j=1}^m{\alpha_i\alpha_jy_iy_j⟨x_i·x_j ⟩-\sum_{i=1}^m{\alpha_i}}} \\ \\ \\ s.t.\ \sum_{i=1}^m{a_iy_i=0} \\ \\ \\ 0\le a_i\le C\ ,\ \ i=1,2,···,m上诉为最终化简的软间隔支持向量机对偶型问题，其优化模型还要满足以下的KKT条件 主问题可行：$1-\xi_i-y_i(w^Tx_i+b)\leq0$,$-\xi\leq0$; 对偶问题可行：$\alpha_i\geq0$, $\beta_i\geq0$; 互补松弛：$\alpha_i(1-\xi_i-y_i(w^Tx_i+b))=0$,$\beta_i\xi_i=0$. 根据KKT条件，我们也可以得到两个结论： 结论一：软间隔支持向量机中, 支持向量落在最大间隔边界, 内部, 或被错误分类的样本 证明：由软间隔支持向量机的 KKT 条件可知，$\alpha_i(1-\xi_i-y_i(w^Tx_i+b))=0$且$\beta_i\xi_i=0$。当$\alpha_i&gt;0$时，$1-\xi_i-y_i(w^Tx_i+b)=0$，进一步可以分为两种情况： 第一：$0&lt;\alpha_i&lt;’C$， 此时$\beta_i$=C-$\alpha_i$&gt;0(公式24的偏导约束)。因此$\xi_i$=0，即样本恰好落在最大间隔边界上。 第二：$\alpha_i=C$， 此时$\beta_i=C-\alpha_i=0$. 若$\xi$ &lt;1’则分类正确，该样本落在最大间隔内部(间隔边界与分离超平面之间)；若$\xi$=1，该样本在分隔超面上。若$\xi$&gt;0样本位于分离超平面误分的一侧。 结论二：支持向量机的参数 (w, b) 仅由支持向量决定,与其他样本无关。 证明：和线性支持向量机证明方式相同. 求解w，b的方式也与线性支持向量机相同。注意：对任一适合条件0&lt;a&lt;C都可求得一个$b^∗$，即这些点正好是位于分隔边界上的点来求出b点的。但是由于原始问题对b的求解并不唯一，所以实际计算时可以取在所有符合条件的样本点上的平均值。 w^*=\sum_{i=1}^N{a_{i}^{*}y_ix_i}选择a的一个分量的一个分量$a_i$适合约束条件适合约束条件0&lt;$a_i$&lt;C,计算 b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i⟨x_i·x_j ⟩}Hinge损失函数线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数.下标”+”表示以下取正值的函数z=max(0，z)： \sum_i^{m}[1-y_i(w·x_i+b)]_++\lambda||w||^2目标函数的第一项是经验损失或经验风险，函数$L(y·(w·x+b))=[1-y(w·x+b)]_+$ ,称为合页损失函数（hinge loss function）.这就是说，当样本点$(x_i,y_i)$被分类正确且函数间隔（确信度）$y_i(w\cdot x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w\cdot x_i+b)$。目标函数目标函数的第二项是系数的L2范数，是正则项。下面要证明上面的公式等价于线性支持向量机原始最优化问题，即(公式21) 证明：先令$[1-y_i(w·x_i+b)]_+=\xi_i$， 则可以写成$\xi_i=\max(0, 1-y_i(w·x_i+b) )$ 。当样本满足约束分类正确时，$y_i(w\cdot x_i+b)&gt;1$， 有$1-y_i(w\cdot x_i+b)\leq0$，得$\xi_i=0$; 当当样本不满足约束时分类错误时， 有$\xi_i=1-y_i(w\cdot x_i+b)$ 故公式21的两个约束条件满足，其最优问题可以写作 \underset{w,b}{min}\sum_{i=1}^m\xi_i+\lambda||w||^2若取$\lambda =\frac{1}{2C}$， 则$\underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)$ 与原始最优问题等价 图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。 图中虚线显示的是感知机的损失函数，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求。 核函数线性可分问题：既然在原始的特征空间$R^d$不是线性可分的, 支持向量机希望通过一个映射 $\phi$ :$R^d$ → $R^{\widetilde{d}}$,使得数据在新的空间 $R^{\widetilde{d}}$ 是线性可分的. 令 $\phi$(x)代表将样本 x 映射到 $R^{\widetilde{d}}$中的特征向量,参数 w 的维数也要相应变为 $\widetilde{d}$维. 在上面的支持向量机对偶公式中，注意到，被映射到高维的特征向量总是以成对内积的形式存在，即 $\phi (x_i)^T\phi(x_j)$ ,如果先计算特征在$R^{\widetilde{d}}$ 空间的映射, 再计算内积, 复杂度是O($\widetilde{d}$) ,当特征被映射到非常高维的空间, 甚至是无穷维空间时, 这将会是沉重的存储和计算负担. 核技巧旨在将特征映射和内积这两步运算压缩为一步, 并且使复杂度由O($\widetilde{d}$)降为O($d$),即, 核技巧希望构造一个核函数 $\kappa (x_i,x_j) $ ,使得 \kappa(x_i,x_j)=\phi (x_i)^T\phi(x_j)并且，$\kappa (x_i,x_j) $ 的计算复杂度是 O($d$) . 应用软间隔分类如果我们严格地规定所有的数据都不在“街道”上，都在正确地两边，称为硬间隔分类，硬间隔分类有两个问题，第一，只对线性可分的数据起作用，第二，对异常点敏感。 为了避免上述的问题，我们更倾向于使用更加软性的模型。目的在保持“街道”尽可能大和避免间隔违规（例如：数据点出现在“街道”中央或者甚至在错误的一边）之间找到一个良好的平衡。这就是软间隔分类。 在 Scikit-Learn 库的 SVM 类，你可以用C超参数（惩罚系数）来控制这种平衡：较小的C会导致更宽的“街道”，但更多的间隔违规。下图显示了在非线性可分隔的数据集上，两个软间隔SVM分类器的判定边界。左边图中，使用了较大的C值，导致更少的间隔违规，但是间隔较小。右边的图，使用了较小的C值，间隔变大了，但是许多数据点出现在了“街道”上。然而，第二个分类器似乎泛化地更好：事实上，在这个训练数据集上减少了预测错误，因为实际上大部分的间隔违规点出现在了判定边界正确的一侧 如果你的 SVM 模型过拟合，你可以尝试通过减小超参数C去调整(街道更宽) SVM 特别适合复杂的分类，而中小型的数据集分类中很少用到。另外，SVM 对特征缩放比较敏感，要先做数据缩放处理。 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型（使用LinearSVC类，超参数C=1，hinge 损失函数）来检测 Virginica 鸢尾花。 123456789101112131415161718192021import numpy as npfrom sklearn import datasetsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import LinearSVCiris = datasets.load_iris()X = iris["data"][:, (2, 3)] # petal length, petal widthy = (iris["target"] == 2).astype(np.float64) # Iris-Virginicasvm_clf = Pipeline(( ("scaler", StandardScaler()), ("linear_svc", LinearSVC(C=1, loss="hinge")), ))svm_clf.fit(X_scaled, y)Then, as usual, you can use the model to make predictions:&gt;&gt;&gt; svm_clf.predict([[5.5, 1.7]])array([ 1.]) 不同于 Logistic 回归分类器，SVM 分类器不会输出每个类别的概率。 SVM的分类器为SVC类，回归则为SVR类。 作为一种选择，你可以在 SVC 类，使用SVC(kernel=&quot;linear&quot;, C=1)，但是它比较慢，尤其在较大的训练集上，所以一般不被推荐。另一个选择是使用SGDClassifier类，即SGDClassifier(loss=&quot;hinge&quot;, alpha=1/(m*C))。它应用了随机梯度下降（SGD 见第四章）来训练一个线性 SVM 分类器。尽管它不会和LinearSVC一样快速收敛，但是对于处理那些不适合放在内存的大数据集是非常有用的，或者处理在线分类任务同样有用。 LinearSVC要使偏置项规范化，首先你应该集中训练集减去它的平均数，完成数据中心化。如果你使用了StandardScaler，那么它会自动处理。此外，确保你设置loss参数为hinge，因为它不是默认值。最后，为了得到更好的效果，你需要将dual参数设置为False，除非特征数比样本量多。 常用损失函数 铰链损失（Hinge Loss）：主要用于支持向量机（SVM） 中， 二分类SVM等于Hinge损失+ L2正则化。 互熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中； 平方损失（Square Loss）：主要是最小二乘法（OLS）中； 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中； Hinge loss 的叫法来源于其损失函数的图形，为一个折线，通用的函数表达式为： L(m_i) = max(0,1-m_i(w)) =\max(0, 1-y\tilde{y}), 其中y=\pm 1表示如果被正确分类，损失是0，否则损失就是 $1-m_i(w)​$ 在机器学习中，Hing 可以用来解 间距最大化 的问题，最有代表性的就是SVM 问题，最初的SVM 优化函数如下： \underset{w,\zeta}{argmin} \frac{1}{2}||w||^2+ C\sum_i \zeta_i \\ st.\quad \forall y_iw^Tx_i \geq 1- \zeta_i \\ \zeta_i \geq 0将约束项进行变形，则为： \zeta_i \geq 1-y_iw^Tx_i则损失函数可以进一步写为： \begin{equation}\begin{split}J(w)&=\frac{1}{2}||w||^2 + C\sum_i max(0,1-y_iw^Tx_i) \\ &= \frac{1}{2}||w||^2 + C\sum_i max(0,1-m_i(w)) \\ &= \frac{1}{2}||w||^2 + C\sum_i L_{Hinge}(m_i) \end{split}\end{equation}因此， SVM 的损失函数可以看作是 L2-norm 和 Hinge loss 之和。 非线性支持向量机分类创建多项式特征1234567891011from sklearn.datasets import make_moonsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturespolynomial_svm_clf = Pipeline(( ("poly_features", PolynomialFeatures(degree=3)), ("scaler", StandardScaler()), ("svm_clf", LinearSVC(C=10, loss="hinge")) ))polynomial_svm_clf.fit(X, y) 多项式核添加多项式特征很容易实现，不仅仅在 SVM，在各种机器学习算法都有不错的表现，但是低次数的多项式不能处理非常复杂的数据集，而高次数的多项式却产生了大量的特征，会使模型变得慢. 幸运的是，当你使用 SVM 时，你可以运用一个被称为“核技巧”（kernel trick）的神奇数学技巧。它可以取得就像你添加了许多多项式，甚至有高次数的多项式，一样好的结果。所以不会大量特征导致的组合爆炸，因为你并没有增加任何特征。超参数coef0控制了高阶多项式与低阶多项式对模型的影响。 123456from sklearn.svm import SVCpoly_kernel_svm_clf = Pipeline(( ("scaler", StandardScaler()), ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5)) ))poly_kernel_svm_clf.fit(X, y) 增加相似特征另一种解决非线性问题的方法是使用相似函数（similarity funtion）计算每个样本与特定地标（landmark）的相似度。例如，让我们来看看前面讨论过的一维数据集，并在x1=-2和x1=1之间增加两个地标。接下来，我们定义一个相似函数，即高斯径向基函数（Gaussian Radial Basis Function，RBF），设置γ = 0.3。 RBF:\quad \phi_{\gamma}(x, \ell) = exp(-\gamma \|x - \ell \|^2)它是个从 0 到 1 的钟型函数，值为 0 的离地标很远，值为 1 的在地标上。现在我们准备计算新特征。例如，我们看一下样本x0=-1：它距离第一个地标距离(x1=-2)是 1，距离第二个地标(x1=1)是 2。因此它的新特征为x2=exp((-0.3 × 1)^2)≈0.74和x3=exp((-0.3 × 2)^2)≈0.30。右边的图显示了特征转换后的数据集（删除了原始特征），正如你看到的，它现在是线性可分了。 你可能想知道如何选择地标。最简单的方法是在数据集中的每一个样本的位置创建地标。这将产生更多的维度从而增加了转换后数据集是线性可分的可能性。但缺点是，m个样本，n个特征的训练集被转换成了m个实例，m个特征的训练集（假设你删除了原始特征）。这样一来，如果你的训练集非常大，你最终会得到同样大的特征 高斯 RBF 核就像多项式特征法一样，相似特征法对各种机器学习算法同样也有不错的表现。但是在所有额外特征上的计算成本可能很高，特别是在大规模的训练集上。然而，“核” 技巧再一次显现了它在 SVM 上的神奇之处：高斯核让你可以获得同样好的结果成为可能，就像你在相似特征法添加了许多相似特征一样，但事实上，你并不需要在RBF添加它们。我们使用 SVC 类的高斯 RBF 核来检验一下。 12345rbf_kernel_svm_clf = Pipeline(( ("scaler", StandardScaler()), ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001)) ))rbf_kernel_svm_clf.fit(X, y) 下图显示了用不同的超参数gamma (γ)和C训练的模型。增大γ使钟型曲线更窄，导致每个样本的影响范围变得更小：即判定边界最终变得更不规则，在单个样本周围环绕。相反的，较小的γ值使钟型曲线更宽，样本有更大的影响范围，判定边界最终则更加平滑。所以γ是可调整的超参数：如果你的模型过拟合，你应该减小γ值，若欠拟合，则增大γ（与超参数C相似）。 还有其他的核函数，但很少使用。例如，一些核函数是专门用于特定的数据结构。在对文本文档或者 DNA 序列进行分类时，有时会使用字符串核（String kernels）（例如，使用 SSK 核（string subsequence kernel）或者基于编辑距离（Levenshtein distance）的核函数） 这么多可供选择的核函数，你如何决定使用哪一个？一般来说，你应该先尝试线性核函数（记住LinearSVC比SVC(kernel=”linear”)要快得多），尤其是当训练集很大或者有大量的特征的情况下。如果训练集不太大，你也可以尝试高斯径向基核（Gaussian RBF Kernel），它在大多数情况下都很有效。如果你有空闲的时间和计算能力，你还可以使用交叉验证和网格搜索来试验其他的核函数，特别是有专门用于你的训练集数据结构的核函数 计算复杂性LinearSVC类基于liblinear库，它实现了线性 SVM 的优化算法。它并不支持核技巧，但是它样本和特征的数量几乎是线性的：训练时间复杂度大约为O(m × n)。 如果你要非常高的精度，这个算法需要花费更多时间。这是由容差值超参数ϵ（在 Scikit-learn 称为tol）控制的。大多数分类任务中，使用默认容差值的效果是已经可以满足一般要求。 SVC 类基于libsvm库，它实现了支持核技巧的算法。训练时间复杂度通常介于O(m^2 × n)和O(m^3 × n)之间。不幸的是，这意味着当训练样本变大时，它将变得极其慢（例如，成千上万个样本）。这个算法对于复杂但小型或中等数量的数据集表现是完美的。然而，它能对特征数量很好的缩放，尤其对稀疏特征来说（sparse features）（即每个样本都有一些非零特征）。在这个情况下，算法对每个样本的非零特征的平均数量进行大概的缩放。下表对 Scikit-learn 的 SVM 分类模型进行比较。 SVM 回归正如我们之前提到的，SVM 算法应用广泛：不仅仅支持线性和非线性的分类任务，还支持线性和非线性的回归任务。技巧在于逆转我们的目标：限制间隔违规的情况下，不是试图在两个类别之间找到尽可能大的“街道”（即间隔）。SVM 回归任务是限制间隔违规情况下，尽量放置更多的样本在“街道”上。“街道”的宽度由超参数ϵ控制。下图显示了在一些随机生成的线性数据上，两个线性 SVM 回归模型的训练情况。一个有较大的间隔（ϵ=1.5），另一个间隔较小（ϵ=0.5） 添加更多的数据样本在间隔之内并不会影响模型的预测，因此，这个模型认为是不敏感的（ϵ-insensitive）。 你可以使用 Scikit-Learn 的LinearSVR类去实现线性 SVM 回归。 123from sklearn.svm import LinearSVRsvm_reg = LinearSVR(epsilon=1.5)svm_reg.fit(X, y) 处理非线性回归任务，你可以使用核化的 SVM 模型。比如，图显示了在随机二次方的训练集，使用二次方多项式核函数的 SVM 回归。左图是较小的正则化（即更大的C值），右图则是更大的正则化（即小的C值）。 下面的代码使用了 Scikit-Learn 的SVR类（支持核技巧）。在回归任务上，SVR类和SVC类是一样的，并且LinearSVR是和LinearSVC等价。LinearSVR类和训练集的大小成线性（就像LinearSVC类），当训练集变大，SVR会变的很慢（就像SVC类） 1234from sklearn.svm import SVRsvm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)svm_poly_reg.fit(X, y) SVM 也可以用来做异常值检测，详情见 Scikit-Learn 文档 补充拉格朗日子乘与KKT等式约束问题 可以得到一个重要的结论：▽f(x)一定与▽h(x)平行，故其关系可以写成▽f(x) =λ ▽h(x). 扩展到高维度等式：f(x)为目标优化函数，hi(x)为约束等式，其表达式恒等于0。F(x)为等价的拉格朗日子乘函数。 计算 F 对x与$\lambda$ 的偏导数并令其为零，可得最优解的必要条件： 其中第一式为定常方程式(stationary equation)，第二式为约束条件。也就是之前已经证明过得结果 故等式约束要满足的条件为 不等式约束问题 这两种情况的最佳解具有两种不同的必要条件 内部解：h(x)&lt;0，不满足h(x)的等式约束，故不在边界上，在可行域的内部。在约束条件无效的情形下， h(x)不起作用，约束优化问题退化为无约束优化问题，因此驻点(最优解)满足▽f(x)=0且$\lambda$=0 ($\lambda$为0才能消去F(x)的约束项)。 边界解：在约束条件有效的情形下，约束不等式变成等式约束，有h(x)=0。和等式约束做相同的处理。另外，存在$\lambda$ 使得$\bigtriangledown f=-\lambda \bigtriangledown g$，但这里$\lambda$ 的正负号是有其意义的。因为我们希望最小化 f，梯度$\bigtriangledown f$ (函数 f在点x的最陡上升方向)应该指向可行域的内部(因为你的最优解最小值是在边界取得的)，但 $\bigtriangledown g$指向可行域的的外部(即 g(x)&gt;0的区域，因为你的约束是小于等于0)，因此$\lambda \geq0$，称为对偶可行性(dual feasibility)。 所以，不论是内部解或边界解，$\lambda h(x)=0$恒成立(不同情况总有一个为0)，称为互补松弛性(complementary slackness)。整合上述两种情况，最佳解的必要条件包括拉格朗日函数常定方程式、主问题可行，对偶可行，和互补松弛。 对偶性一个优化问题，通过求出它的 dual problem ，在只有 weak duality 成立的情况下，我们至少可以得到原始问题的一个下界。而如果 strong duality 成立，则可以直接求解 dual problem 来解决原始问题，就如同经典的 SVM 的求解过程一样。有可能 dual problem 比 primal problem 更容易求解，或者 dual problem 有一些优良的结构（例如 SVM 中通过 dual problem 我们可以将问题表示成数据的内积形式从而使得 kernel trick 的应用成为可能）。此外，还有一些情况会同时求解 dual 和 primal problem ，比如在迭代求解的过程中，通过判断 duality gap 的大小，可以得出一个有效的迭代停止条件. 支持向量机的其他变体Prob SVM.Prob SVM. 对数几率回归可以估计出样本属于正类的概率, 而支持向量机只能判断样本属于正类或负类,无法得到概率.Prob SVM 先训练一个支持向量机, 得到参数(w,b)。再令$s_i:=y_iw^Tx_i+b$，将${(s_1,y_1),(s_2,y_2),…,(s_m,y_m)}$当做新的训练数据训练一个对数几率回归模型，得到参数$(\theta_1,\theta_0)$.因此，ProbSVM 的假设函数为 h(x):=sigm(\theta_1(w^Tx_i+b)+\theta_0)对数几率回归模型可以认为是对训练得到的支持向量机的微调, 包括尺度 (对应$\theta_1)$和平移(对应$\theta_0$). 多分类支持向量机支持向量机也可以扩展到多分类问题中. 对于 K 分类问题, 多分类支持向量机有 K组参数 ${(w_1,b_1),(w_2,b_2),…,(w_K,b_K)}$并希望模型对于属于正确标记的结果以 1 的间隔高于其他类的结果, 形式化如下 \min \limits_{W,b} \frac{1}{m}\sum_{i=1}^m\sum_{k=1}^Kmax(0,(w_{yi}^Tx_i+b_{yi})-(w_{k}^Tx_i+b_{k}+1)+\frac{2}{\lambda}\sum_{k=1}^Kw_k^Tw_k支持向量回归 (SVR)支持向量回归 (SVR). 经典回归模型的损失函数度量了模型的预测 $h(x_i)$与$y_i$的差别, 支持向量回归能够容忍$h(x_i)$与$y_i$之间小于$\epsilon$ 的偏差.令$s:=y-w^Tx+b$，我们定义$\epsilon$不敏感损失为 支持向量机和LR的异同SVM与LR的相同点 LR和SVM都是分类算法，都是监督学习算法。 如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？ LR和SVM都是判别模型。判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别。 LR和SVM在学术界和工业界都广为人知并且应用广泛。 SVM与LR的不同点1.损失函数 SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。即支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。 影响SVM决策面的样本点只有少数的支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果 2.核技巧 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。 这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM转化为对偶问题后，只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的），这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。 3.正则项 根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。 4.异常值 两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。 5.normalization 两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？ 因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。 练习题 支持向量机背后的基本思想是什么 什么是支持向量 当使用 SVM 时，为什么标准化输入很重要？ 分类一个样本时，SVM 分类器能够输出一个置信值吗？概率呢？ 在一个有数百万训练样本和数百特征的训练集上，你是否应该使用 SVM 原始形式或对偶形式来训练一个模型？ 假设你用 RBF 核来训练一个 SVM 分类器，如果对训练集欠拟合：你应该增大或者减小γ吗？调整参数C呢？ 1、支持向量机背后的基本目标是在训练实例中分隔两个类的决策边界之间具有最大可能的间隔。 当执行软间隔分类时，SVM在完全分离两个类和具有尽可能宽的街道之间搜索折衷。 另一个关键思想是在训练非线性数据集时使用核技巧。 2、在训练SVM之后，支持向量是位于“街道”上的任何实例，包括其边界。 决策边界完全由支持向量决定。 任何不是支持向量的实例（即街道外）都没有任何影响; 你可以删除它们，添加更多实例或移动它们，只要它们离开街道它们就不会影响决策边界。 计算预测仅涉及支持向量，而不是整个训练集。 3、SVM尝试适应类之间最大可能的“街道”，因此如果训练集未缩放，SVM将倾向于忽略数值小的特征。 4、SVM分类器可以输出测试实例与决策边界之间的距离，您可以将其用作置信度分数。 但是，这个分数不能直接转换为类概率的估计。 如果在Scikit-Learn中创建SVM时设置probability = True，则在训练之后，它将使用SVM分数的Logistic回归校准概率（通过对训练数据进行额外的5折交叉验证进行训练）。 这会将predict_proba（）和predict_log_proba（）方法添加到SVM。 5、此问题仅适用于线性SVM，因为进行核化kernelized需要用到对偶形式(对偶问题就是为了可以进行核化)。 SVM问题的原始形式的计算复杂度与训练实例m的数量成比例，而对偶形式的计算复杂度与m2和m3之间的数量成比例。 所以如果有数百万个实例，你肯定应该使用原始形式，因为对偶形式会太慢。 6、如果使用RBF内核训练的SVM分类器不适合训练集，则可能存在正则化强度过高。 要减少它，您需要增加gamma或C（或两者）。]]></content>
      <categories>
        <category>Sklearn 与 TensorFlow 机器学习实用指南</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn 与 TensorFlow 机器学习实用指南（三）：回归]]></title>
    <url>%2F2018%2F07%2F14%2FSklearn%20%E4%B8%8E%20TensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%2FSklearn-%E4%B8%8E-TensorFlow-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归：线性回归模型 \hat{y} = h _{\theta} (\mathbf{x})= \theta^T \cdot \mathbf{x} = \theta _{0} + \theta _{1}x _{1}+\theta _{2}x _{2}+\dots+\theta _{n}x _{n} x 为每个样例中特征值的向量形式，包括 $x_1$ 到 $x_n$ ，而且$x_0$ 恒为1。矩阵点乘，相同规模对应相乘。 损失函数 MSE (\mathbf{X},h _{\theta}) = \frac{1}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}^2矩阵形式一行为一个实例 \left(\begin{matrix} 1& x_{1}^{\left(1\right)}& x_{1}^{\left(2\right)}& ···& x_{1}^{\left(n\right)}\\ 1& x_{2}^{\left(1\right)}& x_{2}^{\left(2\right)}& ···& x_{2}^{\left(n\right)}\\ ···& ···& ···& & \\ 1& x_{m}^{\left(1\right)}& x_{m}^{\left(2\right)}& ···& x_{m}^{\left(n\right)}\\ \end{matrix}\right)优化线性回归损失函数的两种方法：1）最小二乘法原理的正态方程 2）梯度下降 正态方程 \frac{\partial }{\partial \theta}MSE(\theta) = \frac{1}{m}(2X^TX\theta-2X^Ty) \hat{\theta} = ({\mathbf{X}}^T\cdot\mathbf{X})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y} 这里的$X^TX$要满足满秩矩阵，然而，现实大多数任务不会满足这个条件。好像只有线性回归能用正态方程求解。优点是一次计算；缺点是矩阵的逆计算慢，尤其是特征数量很多的情况下就更糟糕了，但是一旦你得到了线性回归模型（通过解正态方程或者其他的算法），进行预测是非常快的。 梯度下降批量梯度下降批量梯度下降：使用梯度下降的过程中，你需要计算每一个θj 下代价函数的梯度代价函数的偏导数: 利用公式2对θj求导，其余 θ看做常数。 \frac{\partial }{\partial \theta_j}MSE(\theta)=\frac{2}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}{x_j}^{(i)}更新： \theta :=\theta_j-\lambda\frac{\partial }{\partial \theta_j}MSE(\theta)为了避免单独计算每一个梯度，你也可以使用下面的公式来一起计算它们。梯度向量记为$\nabla_{\theta}MSE(\theta) $ ，其包含了代价函数所有的偏导数(每个模型参数只出现一次)。利用正态方程最后的推导即可） 在这个方程中每一步计算时都包含了整个训练集X ，这也是为什么这个算法称为批量梯度下降：每一次训练过程都使用所有的的训练数据。因此，在大数据集上，其会变得相当的慢（但是我们接下来将会介绍更快的梯度下降算法）。然而，梯度下降的运算规模和特征的数量成正比。训练一个数千数量特征的线性回归模型使用梯度下降要比使用正态方程快的多. 更新： \theta^{(next\ step)}=\theta - \eta\nabla_{\theta}MSE(\theta)我们来看一下这个算法的应用： 12345678910111213141516import numpy as npeta = 0.03n_iterations = 15000m = 100 # 样本数目，X = 2 * np.random.rand(100, 1) # 产生100行1列的0~2的数值y = 4 + 3 * X + np.random.randn(100, 1)# np.c_表示按列操作拼接，np.r_表示按行操作拼接X_b = np.c_[np.ones((100, 1)), X] # x0 = 1theta = np.random.randn(2, 1)for iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients # 最后输出的是所有系数矩阵&gt;&gt;&gt; thetaarray([[4.11509616],[2.87011339]]) # 理论θ_0=4, θ_3, 由于噪声会有点误差 梯度下降的一些要点： 应该确保所有的特征有着相近的尺度范围（例如：使用Scikit_Learn的 StandardScaler类） 学习率$\lambda $ 要自适应 随机梯度下降批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样例。 虽然随机性可以很好的跳过局部最优值，但同时它却不能达到最小值。解决这个难题的一个办法是逐渐降低学习率。开始时，走的每一步较大（这有助于快速前进同时跳过局部最小值），然后变得越来越小，从而使算法到达全局最小值。 这个过程被称为模拟退火 下面的代码使用一个简单的learning schedule来实现随机梯度下降： 123456789101112131415161718192021n_epochs = 50 t0, t1 = 5, 50 #learning_schedule的超参数def learning_schedule(t): return t0 / (t + t1) # np.random.randn返回2行1列符合标准正态分布的数；# np.random.rand返回[0,1）的随机数；# randint返回范围内的整数theta = np.random.randn(2,1)for epoch in range(n_epochs): for i in range(m): random_index = np.random.randint(m) # m个样本随机选一个样本 xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2 * xi.T.dot(xi.dot(theta)-yi) # 单个个体视为批量 eta = learning_schedule(epoch * m + i) # 根据迭代情况调整学习速率 theta = theta - eta * gradiens&gt;&gt;&gt; thetaarray([[3.96100095],[3.0580351 ]]) 通过使用Scikit-Learn完成线性回归的随机梯度下降，你需要使用SGDRegressor类，这个类默认优化的是均方差代价函数。下面的代码迭代了50代，其学习率eta为0.1，使用默认的learning schedule(与前面的不一样)，同时也没有添加任何正则项（penalty = None）： 12345# 因为这个函数需要的y是一个行向量，所以压扁;# 另外，numpy.flatten() 与 numpy.ravel()将多维数组降位一维，前者会进行拷贝处理from sklearn.linear_model import SGDRegressorsgd_reg = SGDRregressor(n_iter=50, penalty=None, eta=0.1)sgd_reg.fit(X,y.ravel()) 结果很接近正态方程的解 12&gt;&gt;&gt; sgd_reg.intercept_, sgd_reg.coef_(array([4.18380366]),array([2.74205299])) 小批量梯度下降小批量梯度下降中，它则使用一个随机的小型实例集，小批量梯度下降在参数空间上的表现比随机梯度下降要好的多，尤其在有大量的小型实例集时，主要利用了矩阵运算的硬件优化 也看一下这个算法的应用 12345678910111213141516171819202122232425theta_path_mgd = []n_iterations = 50minibatch_size = 20np.random.seed(42)theta = np.random.randn(2,1) # random initializationt0, t1 = 200, 1000def learning_schedule(t): return t0 / (t + t1)t = 0for epoch in range(n_iterations): shuffled_indices = np.random.permutation(m) X_b_shuffled = X_b[shuffled_indices] # 打乱所有样本顺序 y_shuffled = y[shuffled_indices] for i in range(0, m, minibatch_size): t += 1 xi = X_b_shuffled[i:i+minibatch_size] yi = y_shuffled[i:i+minibatch_size] gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(t) theta = theta - eta * gradients theta_path_mgd.append(theta) 多项式回归如果你的数据实际上比简单的直线更复杂呢？ 令人惊讶的是，你依然可以使用线性模型来拟合非线性数据。 一个简单的方法是对每个特征进行加权后作为新的特征，然后训练一个线性模型在这个扩展的特征集。 这种方法称为多项式回归。 于是，我们使用Scikit-Learning的PolynomialFeatures类进行训练数据集的转换，让训练集中每个特征的平方（2次多项式）作为新特征（在这种情况下，仅存在一个特征）： 1234567&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; poly_features = PolynomialFeatures(degree=2,include_bias=False)&gt;&gt;&gt; X_poly = poly_features.fit_transform(X) # 转换特征，包含原始特征和二次项特征&gt;&gt;&gt; X[0]array([-0.75275929])&gt;&gt;&gt; X_poly[0]array([-0.75275929, 0.56664654]) 现在包含原始特X并加上了这个特征的平方X^2。现在你可以在这个扩展训练集上使用LinearRegression模型进行拟合 12345&gt;&gt;&gt; lin_reg = LinearRegression()&gt;&gt;&gt; lin_reg.fit(X_poly, y)&gt;&gt;&gt; lin_reg.intercept_, lin_reg.coef_(array([ 1.78134581]), array([[ 0.93366893, 0.56456263]]))# 模型预测函数y=0.56*x_1^2+0.93*x_1+1.78 请注意，当存在多个特征时，多项式回归能够找出特征之间的关系（这是普通线性回归模型无法做到的）。 这是因为LinearRegression会自动添加当前阶数下特征的所有组合。例如，如果有两个特征a,b，使用3阶（degree=3）的LinearRegression时，不仅仅只有a2,a3,b2,同时也会有它们的其他组合项ab,a2b,ab2。 学习曲线我们可以使用交叉验证来估计一个模型的泛化能力。如果一个模型在训练集上表现良好，通过交叉验证指标却得出其泛化能力很差，那么你的模型就是过拟合了。如果在这两方面都表现不好，那么它就是欠拟合了。这种方法可以告诉我们，你的模型是太复杂了还是太简单了。 另一种方法是观察学习曲线：画出模型在训练集上的表现，同时画出以训练集规模为自变量的训练集函数。为了得到图像，需要在训练集的不同规模子集上进行多次训练。下面的代码定义了一个函数，用来画出给定训练集后的模型学习曲线： 1234567891011121314from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitdef plot_learning_curves(model, X, y): X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) train_errors, val_errors = [], [] for m in range(1, len(X_train)): # 根据样本规模画出模型的表现 model.fit(X_train[:m], y_train[:m]) y_train_predict = model.predict(X_train[:m]) y_val_predict = model.predict(X_val) train_errors.append(mean_squared_error(y_train_predict, y_train[:m])) val_errors.append(mean_squared_error(y_val_predict, y_val))plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train") # 训练损失plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val") # 验证损失 我们一起看一下简单线性回归模型的学习曲线 12lin_reg = LinearRegression()plot_learning_curves(lin_reg, X, y) 上面的曲线表现了一个典型的欠拟合模型，两条曲线都到达高原地带并趋于稳定，并且最后两条曲线非常接近，同时误差值非常大。 如果你的模型在训练集上是欠拟合的，添加更多的样例是没用的。你需要使用一个更复杂的模型或者找到更好的特征。 现在让我们看一个在相同数据上10阶多项式模型拟合的学习曲线 12345678from sklearn.pipeline import Pipelinepolynomial_regression = Pipeline(( ("poly_features", PolynomialFeatures(degree=10, include_bias=False)), ("sgd_reg", LinearRegression()),))plot_learning_curves(polynomial_regression, X, y) 在训练集上，误差要比线性回归模型低的多。 图中的两条曲线之间有间隔，这意味模型在训练集上的表现要比验证集上好的多，这也是模型过拟合的显著特点。当然，如果你使用了更大的训练数据，这两条曲线最后会非常的接近。 改善模型过拟合的一种方法是提供更多的训练数据，直到训练误差和验证误差相等 在统计和机器学习领域有个重要的理论：一个模型的泛化误差由三个不同误差的和决定： 偏差：泛化误差的这部分误差是由于错误的假设决定的。例如实际是一个二次模型，你却假设了一个线性模型。一个高偏差的模型最容易出现欠拟合。 方差：这部分误差是由于模型对训练数据的微小变化较为敏感，一个多自由度的模型更容易有高的方差（例如一个高阶多项式模型），因此会导致模型过拟合。 不可约误差：这部分误差是由于数据本身的噪声决定的。降低这部分误差的唯一方法就是进行数据清洗（例如：修复数据源，修复坏的传感器，识别和剔除异常值）。 下图依次为欠拟合，过拟合，较合适。 正则化范数器学习中有几个常用的范数，分别是： $L_1$−范数：$\Vert x\Vert_1 =\sum_{i=1}^n\vert x_i\vert$ $L_2$−范数：$\Vert x\Vert_ 2=(\sum_{i=1}^n\vert x_i^2\vert)^{\frac{1}{2}}$ $L_p$−范数：$\Vert x\Vert_p =(\sum_{i=1}^n\vert x_i^p\vert)^{\frac{1}{p}}$ $L_∞$−范数：$\Vert x\Vert_∞=lim_{p→∞}(\sum_{i=1}^n\vert x_i^p\vert)^{\frac{1}{p}}$ 岭回归(Ridge)岭回归（也称为Tikhonov正则化）是线性回归的正则化版，是L2正则的基础，注意到这个正则项只有在训练过程中才会被加到代价函数。当得到完成训练的模型后，我们应该使用没有正则化的测量方法去评价模型的表现。 一般情况下，训练过程使用的代价函数和测试过程使用的评价函数不一样样的。除了正则化，还有一个不同：训练时的代价函数应该在优化过程中易于求导，而在测试过程中，评价函数更应该接近最后的客观表现。一个好的例子：在分类训练中我们使用对数损失（马上我们会讨论它）作为代价函数，但是我们却使用精确率/召回率来作为它的评价函数。 岭回归代价函数: J(\theta)=MSE(\theta)+\alpha\frac{1}{2}\sum\limits_{i=1}^n\theta_i^2超参数α 决定了你想正则化这个模型的强度,正则化强度越大，模型会越简单。如果α=0 那此时的岭回归便变为了线性回归。如果α 非常的大，所有的权重最后都接近与零，最后结果将是一条穿过数据平均值的水平直线 值得注意的是偏差 $\theta_0$是没有被正则化的（累加运算的开始是 i=1而不是i=0）。如我定义$w$作为特征的权重向量($\theta_1$到$\theta_n$)，那么正则项可以简写成$\frac{1}{2} (\Vert w\Vert_2)^2$, 其中$\Vert \cdot \Vert_2$ 表示权重向量的L2范数。对于梯度下降来说仅仅在均方差梯度向量加上一项$\alpha w$ ,加上$\alpha\theta$是$1/2∗\alpha∗\theta^2$求偏导的结果 在使用岭回归前，对数据进行放缩（可以使用StandardScaler）是非常重要的,算法对于输入特征的数值尺度（scale）非常敏感。大多数的正则化模型都是这样的。 对线性回归来说，对于岭回归，我们可以使用封闭方程去计算，也可以使用梯度下降去处理. 岭回归的封闭方程的解 令 MES(\theta)=(X\theta-y)^T(X\theta-y)+\lambda \theta^T\theta \frac{\partial }{\partial \theta}MSE(\theta) =X^TX\theta-X^Ty +\alpha\theta=0求出 \hat{\theta} =({\mathbf{X}}^T\cdot\mathbf{X}+\alpha\mathbf{I})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y} 矩阵$I$是是一个除了左上角有一个0的n×n的单位矩阵，这个0代表偏差项。偏差$\theta_0$不被正则化的。 下面是如何使用 Scikit-Learn 来进行封闭方程的求解（使用 Cholesky 法进行矩阵分解对上面公式进行变形）: 12345&gt;&gt;&gt; from sklearn.linear_model import Ridge&gt;&gt;&gt; ridge_reg = Ridge(alpha=1, solver="cholesky")&gt;&gt;&gt; ridge_reg.fit(X, y)&gt;&gt;&gt; ridge_reg.predict([[1.5]])array([[ 1.55071465]] 使用随机梯度法进行求解： 1234&gt;&gt;&gt; sgd_reg = SGDRegressor(penalty="l2")&gt;&gt;&gt; sgd_reg.fit(X, y.ravel())&gt;&gt;&gt; sgd_reg.predict([[1.5]])array([[ 1.13500145]]) penalty参数指的是正则项的惩罚类型。指定“l2”表明你要在损失函数上添加一项：权重向量 L2范数平方的一半，这就是简单的岭回归。 Lasso 回归Lasso 回归（也称 Least Absolute Shrinkage，或者 Selection Operator Regression）是另一种正则化版的线性回归：L1正则的基础，就像岭回归那样，它也在损失函数上添加了一个正则化项，但是它使用权重向量的L1范数而不是权重向量L2范数的一半。 Lasso回归的代价函数: J(\theta)=MSE(\theta)+\alpha\sum\limits_{i=1}^n\left|\theta_i \right| Lasso回归的一个重要特征是它倾向于完全消除最不重要的特征的权重（即将它们设置为零） 下面是一个使用Lasso类的小Scikit-Learn示例。你也可以使用SGDRegressor(penalty=”l1”)来代替它 12345&gt;&gt;&gt; from sklearn.linear_model import Lasso&gt;&gt;&gt; lasso_reg = Lasso(alpha=0.1)&gt;&gt;&gt; lasso_reg.fit(X, y)&gt;&gt;&gt; lasso_reg.predict([[1.5]])array([ 1.53788174] 弹性网络(ElasticNet)弹性网络介于Ridge回归和Lasso回归之间。它的正则项是Ridge回归和Lasso回归正则项的简单混合，同时你可以控制它们的混合率r，当r=0时，弹性网络就是Ridge回归，当r=1时，其就是Lasso回归 弹性网络代价函数： J(\theta)=MSE(\theta)+r\alpha\sum\limits_{i=1}^n\left|\theta_i \right|+\frac{1-r}{2}\alpha\sum\limits_{i=1}^n\theta_i^2那么我们该如何选择线性回归，岭回归，Lasso回归，弹性网络呢？一般来说有一点正则项的表现更好，因此通常你应该避免使用简单的线性回归。岭回归是一个很好的首选项，但是如果你的特征仅有少数是真正有用的，你应该选择Lasso和弹性网络。就像我们讨论的那样，它两能够将无用特征的权重降为零。一般来说，弹性网络的表现要比Lasso好，因为当特征数量比样例的数量大的时候，或者特征之间有很强的相关性时，Lasso可能会表现的不规律。下面是一个使用Scikit-Learn 弹性网络ElasticNet（l1_ratio指的就是混合率r）的简单样例: 12345&gt;&gt;&gt; from sklearn.linear_model import ElasticNet&gt;&gt;&gt; elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)&gt;&gt;&gt; elastic_net.fit(X, y)&gt;&gt;&gt; elastic_net.predict([[1.5]])array([ 1.54333232]) 正则化的作用那为什么正则化能起作用呢？首先L0范数（元素非零个数，严格上来说不能算是范数）和L1范数都可以实现权重稀疏。L1范数和L0范数可以实现稀疏，L1范数是L0范数的最优凸近似，L1因具有比L0更好的优化求解特性而被广泛应用。L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。 L1范数的主要作用的实现稀疏特征，那么L2范数可以起什么样作用呢？ 执行 L2 正则化对模型具有以下影响 使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布。 使权重值接近于 0（但并非正好为 0） L2 正则化可能会导致对于某些信息缺乏的特征，模型会学到适中的权重。L2 正则化降低较大权重的程度高于降低较小权重的程度。随着权重越来越接近于 0.0，L2 将权重“推”向 0.0 的力度越来越弱。L2 正则化会使相似度高(存在噪点)两个特征的权重几乎相同。按照我自己的理解，不同的权重会有不同程度的拟合效果，权重较小，低阶的w控制曲线的整体走势，权重较大，高阶的w控制曲线的局部形态，以此类推。这样看来L2正则项的作用就很明显了，要改变预测曲线的整体细节走势肯地会造成损失函数的不满，但是把曲线的形态熨平似乎并没有什么不妥，会降低过拟合的风险。 L2除了能防止过拟合，提升模型的泛化能力。还有另外的一点好处：优化计算。 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果一个系统是ill-conditioned的，它的输出结果就不要太相信了。 然而，如果当我们的样本X的数目比每个样本的维度还要小的时候，矩阵XTX将会不是满秩的，也就是XTX会变得不可逆，所以w*就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。 但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了： \hat{\theta} =({\mathbf{X}}^T\cdot\mathbf{X}+\alpha\mathbf{I})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y} 这里面，专业点的描述是：要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。考虑没有规则项的时候，也就是λ=0的情况，如果矩阵XTX的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善condition number。 早期停止法（Early Stopping）随着训练的进行，算法一直学习，它在训练集上的预测误差（RMSE）自然而然的下降。然而一段时间后，验证误差停止下降，并开始上升。这意味着模型在训练集上开始出现过拟合。一旦验证错误达到最小值，便提早停止训练. 随机梯度和小批量梯度下降不是平滑曲线，你可能很难知道它是否达到最小值。 一种解决方案是，只有在验证误差高于最小值一段时间后（你确信该模型不会变得更好了），才停止，之后将模型参数回滚到验证误差最小值。 下面是一个早期停止法的基础应用 1234567891011121314from sklearn.base import clonesgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,learning_rate="constant", eta0=0.0005)minimum_val_error = float("inf")best_epoch = Nonebest_model = Nonefor epoch in range(1000): sgd_reg.fit(X_train_poly_scaled, y_train) # 训练多项式的新特征，拟合非线性 y_val_predict = sgd_reg.predict(X_val_poly_scaled) val_error = mean_squared_error(y_val_predict, y_val) if val_error &lt; minimum_val_error: minimum_val_error = val_error best_epoch = epoch best_model = clone(sgd_reg) 注意：当warm_start=True时，调用fit()方法后，训练会从停下来的地方继续，而不是从头重新开始 逻辑回归逻辑回归会生成一个介于 0 到 1 之间（不包括 0 和 1）的概率值，而不是确切地预测结果是 0 还是 1。以用于检测垃圾邮件的逻辑回归模型为例。如果此模型推断某一特定电子邮件的值为 0.932，则意味着该电子邮件是垃圾邮件的概率为 93.2%。更准确地说，这意味着在无限训练样本的极限情况下，模型预测其值为 0.932 的这组样本实际上有 93.2% 是垃圾邮件，其余的 6.8% 不是垃圾邮件。 逻辑回归模型的概率估计（向量形式）： \hat{p}=h_\theta(\mathbf{x})=\sigma(\theta^T \cdot \mathbf{x})Logistic函数（也称为logit），用σ() 表示，其是一个sigmoid函数（图像呈S型），它的输出是一个介于0和1之间的数字逻辑函数(S函数) \sigma(t)=\frac{1}{1+exp(-t)}Logistic函数（也称为logit），用σ() 表示，其是一个sigmoid函数（图像呈S型），它的输出是一个介于0和1之间的数字逻辑函数(S函数) 逻辑回归预测模型(σ() 概率输出以0.5作为二分类门槛): 单个样例的代价函数: 这个代价函数是合理的，因为当t接近0时，-log(t)变得非常大，所以如果模型估计一个正例概率接近于0，那么代价函数将会很大，同时如果模型估计一个负例的概率接近1，那么代价函数同样会很大。 另一方面，当t接近于1时， -log(t)接近0，所以如果模型估计一个正例概率接近于0，那么代价函数接近于0，同时如果模型估计一个负例的概率接近0，那么代价函数同样会接近于0， 这正是我们想的.（简单来说,y=1时，概率p越接近1损失越小；相反y=0时，概率p越接近0时损失越小） 整个训练集的代价函数只是所有训练实例的平均值。可以用一个表达式（你可以很容易证明）来统一表示，称为对数损失 逻辑回归的代价函数（对数损失）： J(\theta)=-\frac{1}{m}\sum\limits_{i=1}^m\left[y^{(i)}log\left(\hat{p}^{(i)}\right)+\left(1-y^{(i)}\right)log\left(1-\hat{p}^{(i)}\right)\right]但是这个代价函数对于求解最小化代价函数的θ 是没有公式解的（没有等价的正态方程）。 但好消息是，这个代价函数是凸的，所以梯度下降（或任何其他优化算法）一定能够找到全局最小值（如果学习速率不是太大，并且你等待足够长的时间）。下面公式给出了代价函数关于第j个模型参数θj 的偏导数。 逻辑回归代价函数的偏导数: \frac{\partial}{\partial \theta_j}J(\theta_j)=\frac{1}{m} \sum\limits_{i=1}^m{\left(\sigma\left(\theta^T \cdot \mathbf{x}^{(i)}\right)-y^{(i)}\right)}{x_j}^{(i)}这个公式首先计算每个样例的预测误差，然后误差项乘以第j项特征值，最后求出所有训练样例的平均值。 一旦你有了包含所有的偏导数的梯度向量，你便可以在梯度向量上使用批量梯度下降算法。 也就是说：你已经知道如何训练Logistic回归模型。 对于随机梯度下降，你当然只需要每一次使用一个实例，对于小批量梯度下降，你将每一次使用一个小型实例集。 决策边界我们使用鸢尾花数据集来分析Logistic回归。 这是一个著名的数据集，其中包含150朵三种不同的鸢尾花的萼片和花瓣的长度和宽度。这三种鸢尾花为：Setosa，Versicolor，Virginica 让我们尝试建立一个分类器，仅仅使用花瓣的宽度特征来**识别Virginica**，首先让我们加载数据： 123456&gt;&gt;&gt; from sklearn import datasets&gt;&gt;&gt; iris = datasets.load_iris()&gt;&gt;&gt; list(iris.keys())['data', 'target_names', 'feature_names', 'target', 'DESCR']&gt;&gt;&gt; X = iris["data"][:, 3:] # petal width&gt;&gt;&gt; y = (iris["target"] == 2).astype(np.int) 接下来，我们训练一个逻辑回归模型： 1234from sklearn.linear_model import LogisticRegressionlog_reg = LogisticRegression()log_reg.fit(X, y) # 训练模型 我们来看看模型估计的花瓣宽度从0到3厘米的概率估计 1234X_new = np.linspace(0, 3, 1000).reshape(-1, 1) # 构造花瓣宽度从0到3厘米的所有特征y_proba = log_reg.predict_proba(X_new) # 预测概率plt.plot(X_new, y_proba[:, 1], "g-", label="Iris-Virginica")plt.plot(X_new, y_proba[:, 0], "b--", label="Not Iris-Virginica" Virginica花的花瓣宽度（用三角形表示）在1.4厘米到2.5厘米之间，而其他种类的花（由正方形表示）通常具有较小的花瓣宽度，范围从0.1厘米到1.8厘米。注意，它们之间会有一些重叠。在大约2厘米以上时，分类器非常肯定这朵花是Virginica花（分类器此时输出一个非常高的概率值），而在1厘米以下时，它非常肯定这朵花不是Virginica花（不是Virginica花有非常高的概率）。在这两个极端之间，分类器是不确定的。但是，如果你使用它进行预测（使用predict()方法而不是predict_proba()方法），它将返回一个最可能的结果。因此，在1.6厘米左右存在一个决策边界，这时两类情况出现的概率都等于50％：如果花瓣宽度大于1.6厘米，则分类器将预测该花是Virginica，否则预测它不是（即使它有可能错了）： 12&gt;&gt;&gt; log_reg.predict([[1.7], [1.5]])array([1, 0]) 下图的线性决策边界表示相同的数据集，但是这次使用了两个特征进行判断：花瓣的宽度和长度。 一旦训练完毕，Logistic回归分类器就可以根据这两个特征来估计一朵花是Virginica的可能性。 虚线表示这时两类情况出现的概率都等于50％：这是模型的决策边界。 请注意，它是一个线性边界。每条平行线都代表一个分类标准下的两两个不同类的概率，从15％（左下角）到90％（右上角）。越过右上角分界线的点都有超过90％的概率是Virginica花 就像其他线性模型，逻辑回归模型也可以ℓ1或者ℓ2 惩罚使用进行正则化。Scikit-Learn默认添加了ℓ2 惩罚 在Scikit-Learn的LogisticRegression模型中控制正则化强度的超参数不是α （与其他线性模型一样），而是是它的逆：C. C的值越大，模型正则化强度越低 Softmax回归Logistic回归模型可以直接推广到支持多类别分类，不必组合和训练多个二分类器， 其称为Softmax回归或多类别Logistic回归. 这个想法很简单：当给定一个实例x 时，Softmax回归模型首先计算k类的分数sk(x) ，然后将分数应用在Softmax函数（也称为归一化指数）上，估计出每类的概率。 计算sk(x) 的公式看起来很熟悉，因为它就像线性回归预测的公式一样 k类的Softmax得分: $s_k(x)=θ^T⋅x$ 注意，每个类都有自己独一无二的参数向量θk 。 所有这些向量通常作为行放在参数矩阵Θ 中 一旦你计算了样例x 的每一类的得分，你便可以通过Softmax函数估计出样例属于第k类的概率p^k ：通过计算e的sk(x) 次方，然后对它们进行归一化（除以所有分子的总和）。 和Logistic回归分类器一样，Softmax回归分类器将估计概率最高（它只是得分最高的类）的那类作为预测结果，如公式4-21所示 Softmax回归分类器一次只能预测一个类（即它是多类的，但不是多输出的），因此它只能用于判断互斥的类别，如不同类型的植物。 你不能用它来识别一张照片中的多个人。 现在我们知道这个模型如何估计概率并进行预测，接下来将介绍如何训练。我们的目标是建立一个模型在目标类别上有着较高的概率（因此其他类别的概率较低），最小化公式4-22可以达到这个目标，其表示了当前模型的代价函数，称为交叉熵，当模型对目标类得出了一个较低的概率，其会惩罚这个模型。 交叉熵通常用于衡量待测类别与目标类别的匹配程度（我们将在后面的章节中多次使用它） 交叉熵熵的本质是香农信息量$log\frac{1}{p}$的期望。信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小（猜题次数、编码长度等），就是用交叉熵来衡量的。 现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布。按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为$H(p)=\sum \limits_{i=1}^n p(i)\cdot log\frac{1}{p(i)}$ 。如果使用错误分布q来表示来自真实分布p的平均编码长度，则应该是$H(p,q)=\sum\limits_{i=1}^n p(i)\cdot log\frac{1}{q(i)}$ 。因为用q来编码的样本来自分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。当q为真实分布p时，交叉熵达到最小值1，否则将会大于1。我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“相对熵”：$D(p\Vert q)=H(p,q)-H(p)=\sum\limits_{i=1}^n p(i)\cdot log\frac{p(i)}{q(i)}$ ,其又被称为KL散度(Kullback–Leibler divergence，KLD)。它表示两个概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小，特别地，若2者相同则熵为0。 另外，通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。所以他们得到的相对效果是一样程度的。当然也有特殊情况，彼时两者须区别对待。 上面这个公式由公式4-22求导得到，过程和逻辑回归损失函数一样，只不过将每个类别都纳入计算而已，当k=2则计算正负两类，与逻辑回归一模一样。现在你可以计算每一类的梯度向量，然后使用梯度下降（或者其他的优化算法）找到使得代价函数达到最小值的参数矩阵Θ。 让我们使用Softmax回归对三种鸢尾花进行分类。当你使用LogisticRregression对模型进行训练时，Scikit_Learn默认使用的是一对多模型，但是你可以设置multi_class参数为“multinomial”来把它改变为Softmax回归。你还必须指定一个支持Softmax回归的求解器，例如“lbfgs”求解器（有关更多详细信息，请参阅Scikit-Learn的文档）。其默认使用ℓ12 正则化，你可以使用超参数C控制它。 12345X = iris["data"][:, (2, 3)] # petal length, petal widthy = iris["target"]softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10)softmax_reg.fit(X, y) 所以下次你发现一个花瓣长为5厘米，宽为2厘米的鸢尾花时，你可以问你的模型你它是哪一类鸢尾花，它会回答94.2％是Virginica花（第二类），或者5.8％是其他鸢尾花 1234&gt;&gt;&gt; softmax_reg.predict([[5, 2]])array([2])&gt;&gt;&gt; softmax_reg.predict_proba([[5, 2]])array([[ 6.33134078e-07, 5.75276067e-02, 9.42471760e-01]])是 图4-25用不同背景色表示了结果的决策边界。注意，任何两个类之间的决策边界是线性的。 该图的曲线表示Versicolor类的概率（例如，用0.450标记的曲线表示45％的概率边界）。注意模型也可以预测一个概率低于50％的类。 例如，在所有决策边界相遇的地方，所有类的估计概率相等，分别为33％。 练习题 如果你有一个数百万特征的训练集，你应该选择哪种线性回归训练算法？ 假设你训练集中特征的数值尺度（scale）有着非常大的差异，哪种算法会受到影响？有多大的影响？对于这些影响你可以做什么？ 训练 Logistic 回归模型时，梯度下降是否会陷入局部最低点？ 在有足够的训练时间下，是否所有的梯度下降都会得到相同的模型参数？ 假设你使用批量梯度下降法，画出每一代的验证误差。当你发现验证误差一直增大，接下来会发生什么？你怎么解决这个问题？ 当验证误差升高时，立即停止小批量梯度下降是否是一个好主意？ 哪个梯度下降算法（在我们讨论的那些算法中）可以最快到达解的附近？哪个的确实会收敛？怎么使其他算法也收敛？ 假设你使用多项式回归，画出学习曲线，在图上发现学习误差和验证误差之间有着很大的间隙。这表示发生了什么？有哪三种方法可以解决这个问题？ 假设你使用岭回归，并发现训练误差和验证误差都很高，并且几乎相等。你的模型表现是高偏差还是高方差？这时你应该增大正则化参数$\alpha$ 还是降低它？ 你为什么要这样做： 使用岭回归代替线性回归？ Lasso 回归代替岭回归？ 弹性网络代替 Lasso 回归？ 假设你想判断一副图片是室内还是室外，白天还是晚上。你应该选择二个逻辑回归分类器，还是一个 Softmax 分类器？ 1、如果您拥有具有数百万个功能的训练集，则可以使用随机梯度下降或小批量梯度下降，如果计算内存足够的话，则可使用批量梯度下降。 但是你不能使用正态方程，因为计算复杂度随着特征数量的增长而快速增长（超过二次方），求矩阵特征的逆非常花时间。 2、如果训练集中的特征具有非常不同的比例，则损失函数将具有细长碗的形状，因此梯度下降优化将花费很长时间来收敛。 要解决此问题，您应该在训练模型之前缩放数据。 另外，正态方程在没有缩放的情况下可以正常工作。 3、在训练Logistic回归模型时，梯度下降不会陷入在局部最小值，因为它的损失函数是凸函数的。 4、如果优化问题是凸函数的（例如线性回归或逻辑回归），并且假设学习速率不是太高，则所有梯度下降算法将接近全局最优并最终产生相当类似的模型。 但是，除非你逐渐降低学习率，否则随机梯度下降和小批量GD将永远不会真正收敛; 相反，他们将继续围绕全局最佳状态来回跳跃。 这意味着即使你让它们运行很长时间，这些Gradient Descent算法也会产生略微不同的模型。 5、如果验证误差在每个时期之后一直上升，则一种可能性是学习速率太高并且算法发散。如果训练误差也会增加，那么这显然是问题，你应该降低学习率。 但是，如果训练错误没有增加，那么您的模型将过度拟合训练集，您应该停止训练。 6、由于随机性，随机梯度下降和小批量梯度下降都不能保证在每次训练迭代中都取得进展。 因此，如果在验证损失增加时立即停止训练，你可能会在达到最佳值之前过早停止。 更好的选择是定期保存模型，当它长时间没有改进时（意味着它可能永远不会超过记录），你可以恢复到最佳保存模型。 7、随机梯度下降具有最快的训练迭代，因为它一次只考虑一个训练实例，因此它通常是第一个到达全局最优值（或具有非常小的小批量大小的Minibatch GD）附近。 但是，如果有足够的训练时间，只有批量梯度下降实际上会收敛。 如上所述，除非你逐渐降低学习速度，否则随机指标GD和小批量GD将在最佳状态下反弹。 8、如果验证误差远远高于训练误差，则可能是因为你的模型过度拟合了训练集。 尝试解决此问题的一种方法是降低多项式度：具有较少自由度的模型不太可能过度拟合。 你可以尝试的另一件事是加入正则项，例如，通过在成本函数中添加ℓ2惩罚（岭）或ℓ1惩罚（Lasso）。 这也会降低模型的自由度。 最后，你还可以尝试增加训练集的大小。 9、如果训练误差和验证误差几乎相等且相当高，则模型可能欠拟合训练集，这意味着它具有高偏差。 你应该尝试减少正则化超参数α。 10、 具有一些正则化的模型通常比没有任何正则化的模型表现更好，因此通常应该优先选择岭回归而不是简单的线性回归。 Lasso回归使用ℓ1惩罚，这往往会将权重降低到恰好为零。 这导致稀疏模型，除了最重要的权重之外，所有权重都为零。 这是一种自动执行特征选择的方法，如果你怀疑只有少数特征真正重要，这是很好的。 当你不确定时，你应该更偏向岭回归。 弹性网络常比Lasso更受欢迎，因为Lasso在某些情况下可能表现不稳定（当有些特征强烈相关或者特征数量比训练样本数量还要多）。 但是，它确实添加了一个额外的超参数来调整。 如果你想要具有稳定行为的Lasso，你可以使用弹性网络，并设置比率r接近1。 11、如果你想将图片分类为室外/室内和白天/夜晚，因为这些不是专属类别（即，所有四种组合都是可能的），你应该训练两个Logistic回归分类器。]]></content>
      <categories>
        <category>Sklearn 与 TensorFlow 机器学习实用指南</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
        <tag>逻辑回归</tag>
        <tag>softmax</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn 与 TensorFlow 机器学习实用指南（二）：分类]]></title>
    <url>%2F2018%2F07%2F11%2FSklearn%20%E4%B8%8E%20TensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%2FSklearn-%E4%B8%8E-TensorFlow-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[MNIST：手写数字分类数据集1234567891011121314&gt;&gt;&gt; from sklearn.datasets import fetch_mldata&gt;&gt;&gt; mnist = fetch_mldata('MNIST original')&gt;&gt;&gt; mnist&#123;'COL_NAMES': ['label', 'data'],'DESCR': 'mldata.org dataset: mnist-original', # DESCR键描述数据集'data': array([[0, 0, 0, ..., 0, 0, 0], # 数组的一行表示一个样例，一列表示一个特征 [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),'target': array([ 0., 0., 0., ..., 9., 9., 9.])&#125; # target键存放一个标签数组X, y = mnist["data"], mnist["target"] # 获取样本或标签 MNIST 有 70000 张图片，每张图片有 784 个特征。这是因为每个图片都是28×28像素的，并且每个像素的值介于 0~255 之间。让我们看一看数据集的某一个数字。你只需要将某个实例的特征向量，reshape为28*28的数组，然后使用 Matplotlib 的imshow函数展示出来 12345678910%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltsome_digit = X[36000] some_digit_image = some_digit.reshape(28, 28) # 将样本转为28大小的像素矩阵# 按‘0’‘1’数值转为灰度图像 # interpolation当小图像放大时,interpolation ='nearest'效果很好，否则用None。plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation="nearest") plt.axis("off")plt.show() MNIST 数据集已经事先被分成了一个训练集（前 6000 张图片）和一个测试集（最后 10000 张图片） 1X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] 打乱数据 1234import numpy as npshuffle_index = np.random.permutation(60000)X_train, y_train = X_train[shuffle_index], y_train[shuffle_index] 训练一个二分类器现在我们简化一下问题，只尝试去识别一个数字，比如说，数字 5。这个“数字 5 检测器”就是一个二分类器，能够识别两类别，“是 5”和“非 5”。让我们为这个分类任务创建目标向量： 123# 在训练和测试集上区分是否为5转为0,1标签矩阵y_train_5 = (y_train == 5) # True for all 5s, False for all other digits.y_test_5 = (y_test == 5) 采用随机梯度下降分类器 123from sklearn.linear_model import SGDClassifiersgd_clf = SGDClassifier(random_state=42) #如果你想重现结果，你应该固定参数random_state sgd_clf.fit(X_train, y_train_5) 输出预测结果 12&gt;&gt;&gt; sgd_clf.predict([some_digit])array([ True], dtype=bool) 分类器猜测这个数字代表 5（True）。看起来在这个例子当中，它猜对了。现在让我们评估这个模型的性能。 使用交叉验证测量准确性评估一个模型的好方法是使用交叉验证，像之前提过一样。但有时为了有更好的控制权，可以写自己版本的交叉验证，以下代码粗略地做了和cross_val_score()相同的事情，并且输出相同的结果。 12345678910111213from sklearn.model_selection import StratifiedKFoldfrom sklearn.base import cloneskfolds = StratifiedKFold(n_splits=3, random_state=42) # 三组for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = (y_train_5[train_index]) X_test_fold = X_train[test_index] y_test_fold = (y_train_5[test_index]) clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct / len(y_pred)) # prints 0.9502, 0.96565 and 0.96495 StratifiedKFold类实现了分层采样，生成的折（fold）包含了各类相应比例的样例。在每一次迭代，上述代码生成分类器的一个克隆版本，在训练折（training folds）的克隆版本上进行训，在测试折（test folds）上进行预测。然后它计算出被正确预测的数目和输出正确预测的比例。 这里使用sklearn提供的cross_val_score()函数来评估SGDClassifier模型 123&gt;&gt;&gt; from sklearn.model_selection import cross_val_score&gt;&gt;&gt; cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")array([ 0.9502 , 0.96565, 0.96495] 有大于 95% 的精度（accuracy），特别高！但要注意这是一个有数据偏差的数据集，这是因为只有 10% 的图片是数字 5，所以你总是猜测某张图片不是 5，你也会有90%的可能性是对的。处理这类问题，要回归到之前讲的准确率和召回率和ORC曲线了。 混淆矩阵对分类器来说，一个好得多的性能评估指标是混淆矩阵，为了计算混淆矩阵，首先你需要有一系列的预测值，这样才能将预测值与真实值做比较。你或许想在测试集上做预测。但是我们现在先不碰它。（记住，只有当你处于项目的尾声，当你准备上线一个分类器的时候，你才应该使用测试集）。相反，你应该使用cross_val_predict()函数 12from sklearn.model_selection import cross_val_predicty_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3) 就像 cross_val_score()，cross_val_predict()也使用 K 折交叉验证。它不是返回一个评估分数，而是返回基于每一个测试折做出的一个预测值。这意味着，对于每一个训练集的样例，你得到一个干净的预测（“干净”是说一个模型在训练过程当中没有用到测试集的数据）。 现在使用 confusion_matrix()函数，你将会得到一个混淆矩阵。传递目标类(y_train_5)和预测类（y_train_pred）给它。 1234&gt;&gt;&gt; from sklearn.metrics import confusion_matrix&gt;&gt;&gt; confusion_matrix(y_train_5, y_train_pred)array([[53272, 1307], [ 1077, 4344]]) 混淆矩阵中的每一行表示一个实际的类, 而每一列表示一个预测的类。该矩阵的第一行认为“非 5”（反例）中的 53272 张被正确归类为 “非 5”（他们被称为真反例，true negatives）, 而其余 1307 被错误归类为”是 5” （假正例，false positives）。第二行认为“是 5” （正例）中的 1077 被错误地归类为“非 5”（假反例，false negatives），其余 4344 正确分类为 “是 5”类（真正例，true positives）。一个完美的分类器将只有真反例和真正例，所以混淆矩阵的非零值仅在其主对角线（左上至右下）。 Scikit-Learn 提供了一些函数去计算分类器的指标，包括精确率和召回率（之前的文章是tensorflow，这里主要讲Scikit-Learn） 12345&gt;&gt;&gt; from sklearn.metrics import precision_score, recall_score&gt;&gt;&gt; precision_score(y_train_5, y_pred) # == 4344 / (4344 + 1307)0.76871350203503808&gt;&gt;&gt; recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077)0.79136690647482011 通常结合精确率和召回率会更加方便，这个指标叫做“F1 值”，特别是当你需要一个简单的方法去比较两个分类器的优劣的时候。F1 值是精确率和召回率的调和平均。普通的平均值平等地看待所有的值，而调和平均会给小的值更大的权重。所以，要想分类器得到一个高的 F1 值，需要召回率和精确率。 F1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} = 2 * \frac{precison * recall}{precison + recall} = \frac{TP}{TP + \frac{FN + FP}{2}}为了计算 F1 值，简单调用f1_score() 123&gt;&gt;&gt; from sklearn.metrics import f1_score&gt;&gt;&gt; f1_score(y_train_5, y_pred)0.78468208092485547 F1 支持那些有着相近精确率和召回率的分类器。这不会总是你想要的。有的场景你会绝大程度地关心精确率，而另外一些场景你会更关心召回率。不幸的是，你不能同时拥有两者。增加精确率会降低召回率，反之亦然。这叫做精确率与召回率之间的折衷. 一般来说，提高分类阈值会减少假正例，从而提高精确率。降低分类阈值会提高召回率。 Scikit-Learn 不让你直接设置阈值，但是它给你提供了设置决策分数的方法，这个决策分数可以用来产生预测。它不是调用分类器的predict()方法，而是调用decision_function()方法。这个方法返回每一个样例的分数值，然后基于这个分数值，使用你想要的任何阈值做出预测。 123456&gt;&gt;&gt; y_scores = sgd_clf.decision_function([some_digit])&gt;&gt;&gt; y_scoresarray([ 161855.74572176])&gt;&gt;&gt; threshold = 0&gt;&gt;&gt; y_some_digit_pred = (y_scores &gt; threshold)array([ True], dtype=bool) SGDClassifier用了一个等于 0 的阈值，所以前面的代码返回了跟predict()方法一样的结果（都返回了true）。让我们提高这个阈值： 1234&gt;&gt;&gt; threshold = 200000&gt;&gt;&gt; y_some_digit_pred = (y_scores &gt; threshold)&gt;&gt;&gt; y_some_digit_predarray([False], dtype=bool) 这证明了提高阈值会降调召回率。这个图片实际就是数字 5，当阈值等于 0 的时候，分类器可以探测到这是一个 5，当阈值提高到 20000 的时候，分类器将不能探测到这是数字 5。 那么，你应该如何使用哪个阈值呢？首先，你需要再次使用cross_val_predict()得到每一个样例的分数值，但是这一次指定返回一个决策分数，而不是预测值。 12y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method="decision_function") 现在有了这些分数值。对于任何可能的阈值，使用precision_recall_curve(),你都可以计算精确率和召回率: 12from sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) 最后，你可以使用 Matplotlib 画出精确率和召回率，这里把精确率和召回率当作是阈值的一个函数。 12345678def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], "b--", label="Precision") plt.plot(thresholds, recalls[:-1], "g-", label="Recall") plt.xlabel("Threshold") plt.legend(loc="upper left") plt.ylim([0, 1])plot_precision_recall_vs_threshold(precisions, recalls, thresholds)plt.show() 你也许会好奇为什么精确率曲线比召回率曲线更加起伏不平（右上部分）。原因是精确率有时候会降低，尽管当你提高阈值的时候，通常来说精确率会随之提高。另一方面，当阈值提高时候，召回率只会降低。这也就说明了为什么召回率的曲线更加平滑。 现在你可以选择适合你任务的最佳阈值。另一个选出好的精确率/召回率折衷的方法是直接画出精确率对召回率的曲线(PR曲线)，如图所示。 我们假设你决定达到 90% 的准确率，在 70000 附近找到一个阈值。为了作出预测（目前为止只在训练集上预测），你可以运行以下代码，而不是运行分类器的predict()方法。 1y_train_pred_90 = (y_scores &gt; 70000) 检查这些预测的准确率和召回率： 1234&gt;&gt;&gt; precision_score(y_train_5, y_train_pred_90)0.8998702983138781&gt;&gt;&gt; recall_score(y_train_5, y_train_pred_90)0.63991883416343853 ROC 曲线受试者工作特征（ROC）曲线是另一个二分类器常用的工具。它非常类似与准确率/召回率曲线（PR曲线），但不是画出准确率对召回率的曲线，ROC 曲线是真正例率（true positive rate，另一个名字叫做召回率）对假正例率（false positive rate, FPR）的曲线。FPR 是反例被错误分成正例的比率。它等于 1 减去真反例率（true negative rate， TNR）。TNR是反例被正确分类的比率。TNR也叫做特异性。所以 ROC 曲线画出召回率对（1 减特异性）的曲线。 TPR = \frac{TP}{P} = \frac{TP}{TP+FN} FPR = \frac{FP}{N} = \frac{FP}{FP+TN} = 1-TNR TNR = \frac{TN}{N} = \frac{TN}{TN+FP} 为了画出 ROC 曲线，你首先需要计算各种不同阈值下的 TPR、FPR，使用roc_curve()函数： 12from sklearn.metrics import roc_curvefpr, tpr, thresholds = roc_curve(y_train_5, y_scores) 然后你可以使用 matplotlib，画出 FPR 对 TPR 的曲线 12345678def plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], 'k--') plt.axis([0, 1, 0, 1]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate')plot_roc_curve(fpr, tpr)plt.show() 一个比较分类器之间优劣的方法是：测量ROC曲线下的面积（AUC）**。一个完美的分类器的 ROC AUC 等于 1，而一个纯随机分类器的 ROC AUC 等于 0.5。Scikit-Learn 提供了一个函数来计算 ROC AUC： 123&gt;&gt;&gt; from sklearn.metrics import roc_auc_score&gt;&gt;&gt; roc_auc_score(y_train_5, y_scores)0.97061072797174941 因为 ROC 曲线跟准确率/召回率曲线（或者叫 PR）很类似，你或许会好奇如何决定使用哪一个曲线呢？一个笨拙的规则是，优先使用 PR 曲线当正例很少，或者当你关注假正例多于假反例的时候。其他情况使用 ROC 曲线。举例子，回顾前面的 ROC 曲线和 ROC AUC 数值，你或许人为这个分类器很棒。但是这几乎全是因为只有少数正例（“是 5”），而大部分是反例（“非 5”）。相反，PR 曲线清楚显示出这个分类器还有很大的改善空间（PR 曲线应该尽可能地靠近右上角）。 我们训练一个RandomForestClassifier，然后拿它的的ROC曲线和ROC AUC数值去跟SGDClassifier的比较。首先你需要得到训练集每个样例的数值。但是由于随机森林分类器的工作方式，RandomForestClassifier不提供decision_function()方法。相反，它提供了predict_proba()方法。Skikit-Learn分类器通常二者中的一个。predict_proba()方法返回一个数组，数组的每一行代表一个样例，每一列代表一个类。数组当中的值的意思是：给定一个样例属于给定类的概率。比如，70%的概率这幅图是数字 5。 1234from sklearn.ensemble import RandomForestClassifierforest_clf = RandomForestClassifier(random_state=42)y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method="predict_proba") 但是要画 ROC 曲线，你需要的是样例的分数，而不是概率。一个简单的解决方法是使用正例的概率当作样例的分数。 12y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class 预测为正例概率fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest) 现在你即将得到 ROC 曲线。将前面一个分类器的 ROC 曲线一并画出来是很有用的，可以清楚地进行比较。 1234plt.plot(fpr, tpr, "b:", label="SGD")plot_roc_curve(fpr_forest, tpr_forest, "Random Forest")plt.legend(loc="bottom right")plt.show() 如你所见，RandomForestClassifier的 ROC 曲线比SGDClassifier的好得多：它更靠近左上角。所以，它的 ROC AUC 也会更大。 12&gt;&gt;&gt; roc_auc_score(y_train_5, y_scores_forest)0.99312433660038291 现在你知道如何训练一个二分类器，选择合适的标准，使用交叉验证去评估你的分类器，选择满足你需要的准确率/召回率折衷方案，和比较不同模型的 ROC 曲线和 ROC AUC 数值。现在让我们检测更多的数字，而不仅仅是一个数字 5。 多类别分类一些算法（比如随机森林分类器或者朴素贝叶斯分类器）可以直接处理多类分类问题。其他一些算法（比如 SVM 分类器或者线性分类器）则是严格的二分类器。然后，有许多策略可以让你用二分类器去执行多类分类。 一个方法是：训练10个二分类器，每一个对应一个数字（探测器 0，探测器 1，探测器 2，以此类推）。然后当你想对某张图片进行分类的时候，让每一个分类器对这个图片进行分类，选出决策分数最高的那个分类器（One vs all 里面分数最高的One）。这叫做“一对所有”（OvA）策略 另一个策略是对每一对数字都训练一个二分类器：一个分类器用来处理数字 0 和数字 1，一个用来处理数字 0 和数字 2，一个用来处理数字 1 和 2，以此类推。这叫做“一对一”（OvO）策略。如果有 N 个类。你需要训练N*(N-1)/2个分类器。 一些算法（比如 SVM 分类器）在训练集的大小上很难扩展，所以对于这些算法，OvO 是比较好的，因为它可以在小的数据集上面可以更多地训练，较之于巨大的数据集而言。但是，对于大部分的二分类器来说，OvA 是更好的选择。Scikit-Learn 可以探测出你想使用一个二分类器去完成多分类的任务，它会自动地执行 OvA（除了 SVM 分类器，它使用 OvO）让我们试一下SGDClassifier. 123&gt;&gt;&gt; sgd_clf.fit(X_train, y_train) # y_train, not y_train_5&gt;&gt;&gt; sgd_clf.predict([some_digit])array([ 5.]) 上面的代码在训练集上训练了一个SGDClassifier。这个分类器处理原始的目标class，从 0 到 9（y_train），而不是仅仅探测是否为 5 （y_train_5）。然后它做出一个判断（在这个案例下只有一个正确的数字）。在幕后，Scikit-Learn 实际上训练了 10 个二分类器，每个分类器都产到一张图片的决策数值，选择数值最高的那个类。 为了证明这是真实的，你可以调用decision_function()方法。不是返回每个样例的一个数值，而是返回 10 个数值，一个数值对应于一个类。 123456&gt;&gt;&gt; some_digit_scores = sgd_clf.decision_function([some_digit])&gt;&gt;&gt; some_digit_scoresarray([[-311402.62954431, -363517.28355739, -446449.5306454 , -183226.61023518, -414337.15339485, 161855.74572176, -452576.39616343, -471957.14962573, -518542.33997148, -536774.63961222]]) 最高数值是对应于类别 5 : 123456&gt;&gt;&gt; np.argmax(some_digit_scores) # 找最大值的索引5&gt;&gt;&gt; sgd_clf.classes_array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])&gt;&gt;&gt; sgd_clf.classes[5] # 用索引匹配类别5.0 一个分类器被训练好了之后，它会保存目标类别列表到它的属性classes_ 中去，按照值排序。在本例子当中，在classes_ 数组当中的每个类的索引方便地匹配了类本身，比如，索引为 5 的类恰好是类别 5 本身。但通常不会这么幸运。 如果你想强制 Scikit-Learn 使用 OvO 策略或者 OvA 策略，你可以使用OneVsOneClassifier类或者OneVsRestClassifier类。创建一个样例，传递一个二分类器给它的构造函数。举例子，下面的代码会创建一个多类分类器，使用 OvO 策略，基于SGDClassifier。 1234567&gt;&gt;&gt; from sklearn.multiclass import OneVsOneClassifier&gt;&gt;&gt; ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))&gt;&gt;&gt; ovo_clf.fit(X_train, y_train)&gt;&gt;&gt; ovo_clf.predict([some_digit])array([ 5.])&gt;&gt;&gt; len(ovo_clf.estimators_)45 训练一个RandomForestClassifier同样简单： 123&gt;&gt;&gt; forest_clf.fit(X_train, y_train)&gt;&gt;&gt; forest_clf.predict([some_digit])array([ 5.]) 这次 Scikit-Learn 没有必要去运行 OvO 或者 OvA，因为随机森林分类器能够直接将一个样例分到多个类别。你可以调用predict_proba()，得到样例对应的类别的概率值的列表： 12&gt;&gt;&gt; forest_clf.predict_proba([some_digit])array([[ 0.1, 0. , 0. , 0.1, 0. , 0.8, 0. , 0. , 0. , 0. ]]) 你可以看到这个分类器相当确信它的预测：在数组的索引 5 上的 0.8，意味着这个模型以 80% 的概率估算这张图片代表数字 5。它也认为这个图片可能是数字 0 或者数字 3，分别都是 10% 的几率。 现在当然你想评估这些分类器。像平常一样，你想使用交叉验证。让我们用cross_val_score()来评估SGDClassifier的精度。 12&gt;&gt;&gt; cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring="accuracy")array([ 0.84063187, 0.84899245, 0.86652998]) 在所有测试折（test fold）上，它有 84% 的精度。如果你是用一个随机的分类器，你将会得到 10% 的正确率。所以这不是一个坏的分数，但是你可以做的更好。举例子，简单将输入正则化，将会提高精度到 90% 以上。 12345&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler&gt;&gt;&gt; scaler = StandardScaler()&gt;&gt;&gt; X_train_scaled = scaler.fit_transform(X_train.astype(np.float64)) # 特征正则化，没说用哪种&gt;&gt;&gt; cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring="accuracy")array([ 0.91011798, 0.90874544, 0.906636 ]) 误差分析：首先，你可以检查混淆矩阵。你需要使用cross_val_predict()做出预测，然后调用confusion_matrix()函数，像你早前做的那样。 12345678910111213&gt;&gt;&gt; y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)&gt;&gt;&gt; conf_mx = confusion_matrix(y_train, y_train_pred)&gt;&gt;&gt; conf_mxarray([[5725, 3, 24, 9, 10, 49, 50, 10, 39, 4], [ 2, 6493, 43, 25, 7, 40, 5, 10, 109, 8], [ 51, 41, 5321, 104, 89, 26, 87, 60, 166, 13], [ 47, 46, 141, 5342, 1, 231, 40, 50, 141, 92], [ 19, 29, 41, 10, 5366, 9, 56, 37, 86, 189], [ 73, 45, 36, 193, 64, 4582, 111, 30, 193, 94], [ 29, 34, 44, 2, 42, 85, 5627, 10, 45, 0], [ 25, 24, 74, 32, 54, 12, 6, 5787, 15, 236], [ 52, 161, 73, 156, 10, 163, 61, 25, 5027, 123], [ 43, 35, 26, 92, 178, 28, 2, 223, 82, 5240]]) 这里是一对数字。使用 Matplotlib 的matshow()函数，将混淆矩阵以图像的方式呈现，将会更加方便 12plt.matshow(conf_mx, cmap=plt.cm.gray) # #灰度图,对应位置的值越大色块越亮plt.show() 这个混淆矩阵看起来相当好，因为大多数的图片在主对角线上。在主对角线上意味着被分类正确。数字 5 对应的格子看起来比其他数字要暗淡许多。这可能是数据集当中数字 5 的图片比较少，又或者是分类器对于数字 5 的表现不如其他数字那么好。你可以验证两种情况. 让我们关注仅包含误差数据的图像呈现。首先你需要将混淆矩阵的每一个值除以相应类别的图片的总数目。这样子，你可以比较错误率，而不是绝对的错误数（这对大的类别不公平）。 12row_sums = conf_mx.sum(axis=1, keepdims=True)norm_conf_mx = conf_mx / row_sums 现在让我们用 0 来填充对角线。这样子就只保留了被错误分类的数据。让我们画出这个结果。(此时数值为错误率) 123np.fill_diagonal(norm_conf_mx, 0)plt.matshow(norm_conf_mx, cmap=plt.cm.gray)plt.show() 现在你可以清楚看出分类器制造出来的各类误差。记住：行代表实际类别，列代表预测的类别。第 8、9 列相当亮，这告诉你许多图片被误分成数字 8 或者数字 9。相似的，第 8、9 行也相当亮，告诉你数字 8、数字 9 经常被误以为是其他数字。相反，一些行相当黑，比如第一行：这意味着大部分的数字 1 被正确分类（一些被误分类为数字 8 ）。留意到误差图不是严格对称的。举例子，比起将数字 8 误分类为数字 5 的数量，有更多的数字 5 被误分类为数字 8。 分析混淆矩阵通常可以给你提供深刻的见解去改善你的分类器。回顾这幅图，看样子你应该努力改善分类器在数字 8 和数字 9 上的表现，和纠正 3/5 的混淆。举例子，你可以尝试去收集更多的数据，或者你可以构造新的、有助于分类器的特征。举例子，写一个算法去数闭合的环（比如，数字 8 有两个环，数字 6 有一个， 5 没有）。又或者你可以预处理图片（比如，使用 Scikit-Learn，Pillow， OpenCV）去构造一个模式，比如闭合的环。 分析独特的误差，是获得关于你的分类器是如何工作及其为什么失败的洞见的一个好途径。但是这相对难和耗时。举例子，我们可以画出数字 3 和 5 的例子 1234567891011cl_a, cl_b = 3, 5X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)]X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)]X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)]X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)]plt.figure(figsize=(8,8))plt.subplot(221); plot_digits(X_aa[:25], ../images_per_row=5)plt.subplot(222); plot_digits(X_ab[:25], ../images_per_row=5)plt.subplot(223); plot_digits(X_ba[:25], ../images_per_row=5)plt.subplot(224); plot_digits(X_bb[:25], ../images_per_row=5)plt.show() 左边两个5*5的块将数字识别为 3，右边的将数字识别为 5。一些被分类器错误分类的数字（比如左下角和右上角的块）是书写地相当差，甚至让人类分类都会觉得很困难（比如第 8 行第 1 列的数字 5，看起来非常像数字 3 ）。但是，大部分被误分类的数字，在我们看来都是显而易见的错误。很难明白为什么分类器会分错。原因是我们使用的简单的SGDClassifier，这是一个线性模型。它所做的全部工作就是分配一个类权重给每一个像素，然后当它看到一张新的图片，它就将加权的像素强度相加，每个类得到一个新的值。所以，因为 3 和 5 只有一小部分的像素有差异，这个模型很容易混淆它们。 3 和 5 之间的主要差异是连接顶部的线和底部的线的细线的位置。如果你画一个 3，连接处稍微向左偏移，分类器很可能将它分类成 5。反之亦然。换一个说法，这个分类器对于图片的位移和旋转相当敏感。所以，减轻 3/5 混淆的一个方法是对图片进行预处理，确保它们都很好地中心化和不过度旋转。这同样很可能帮助减轻其他类型的错误。 多标签分类先看一个简单点的例子，仅仅是为了阐明的目的 123456from sklearn.neighbors import KNeighborsClassifiery_train_large = (y_train &gt;= 7)y_train_odd = (y_train % 2 == 1)y_multilabel = np.c_[y_train_large, y_train_odd]knn_clf = KNeighborsClassifier()knn_clf.fit(X_train, y_multilabel) 这段代码创造了一个y_multilabel数组，里面包含两个目标标签。第一个标签指出这个数字是否为大数字（7，8 或者 9），第二个标签指出这个数字是否是奇数。接下来几行代码会创建一个KNeighborsClassifier样例（它支持多标签分类，但不是所有分类器都可以），然后我们使用多目标数组来训练它。现在你可以生成一个预测，然后它输出两个标签： 12&gt;&gt;&gt; knn_clf.predict([some_digit])array([[False, True]], dtype=bool) 它工作正确。数字 5 不是大数（False），同时是一个奇数（True） 有许多方法去评估一个多标签分类器，和选择正确的量度标准，这取决于你的项目。举个例子，一个方法是对每个个体标签去量度 F1 值（或者前面讨论过的其他任意的二分类器的量度标准），然后计算平均值。下面的代码计算全部标签的平均 F1 值： 123&gt;&gt;&gt; y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)&gt;&gt;&gt; f1_score(y_train, y_train_knn_pred, average="macro")0.96845540180280221 这里假设所有标签有着同等的重要性，但可能不是这样。特别是，如果你的 Alice 的照片比 Bob 或者 Charlie 更多的时候，也许你想让分类器在 Alice 的照片上具有更大的权重。一个简单的选项是：给每一个标签的权重等于它的支持度（比如，那个标签的样例的数目）。为了做到这点，简单地在上面代码中设置average=”weighted”。 多输出分类我们即将讨论的最后一种分类任务被叫做“多输出-多类分类”（或者简称为多输出分类）。它是多标签分类的简单泛化，在这里每一个标签可以是多类别的（比如说，它可以有多于两个可能值）。 为了说明这点，我们建立一个系统，它可以去除图片当中的噪音。它将一张混有噪音的图片作为输入，期待它输出一张干净的数字图片，用一个像素强度的数组表示，就像 MNIST 图片那样。注意到这个分类器的输出是多标签的（一个像素一个标签）和每个标签可以有多个值（像素强度取值范围从 0 到 255）。所以它是一个多输出分类系统的例子。 分类与回归之间的界限是模糊的，比如这个例子。按理说，预测一个像素的强度更类似于一个回归任务，而不是一个分类任务。而且，多输出系统不限于分类任务。你甚至可以让你一个系统给每一个样例都输出多个标签，包括类标签和值标签。 让我们从 MNIST 的图片创建训练集和测试集开始，然后给图片的像素强度添加噪声，这里是用 NumPy 的randint()函数。目标图像是原始图像。 123456noise = rnd.randint(0, 100, (len(X_train), 784))noise = rnd.randint(0, 100, (len(X_test), 784))X_train_mod = X_train + noiseX_test_mod = X_test + noisey_train_mod = X_trainy_test_mod = X_test 让我们看一下测试集当中的一张图片（是的，我们在窥探测试集，所以你应该马上邹眉）： 左边的加噪声的输入图片。右边是干净的目标图片。现在我们训练分类器，让它清洁这张图片： 123knn_clf.fit(X_train_mod, y_train_mod)clean_digit = knn_clf.predict([X_test_mod[some_index]])plot_digit(clean_digit) 到这里就讲完分类的内容了，有点混乱对不对，我们来总结梳理一下。 要掌握自定义k折交叉验证的方法（≈cross_val_score） cross_val_score为验证模型的一个好方法，但是只能得到准确率的评估分数 如果正反例数据偏差大，我们需要用到混淆矩阵，这个矩阵要用到预测值而不是评估分数，所以改cross_val_predict，这会返回每个测试折做出的预测值，即y_train_pred 利用预测值y_train_pred可以得到混淆矩阵，精确率，召回率，F1 有时我们需要阈值来平衡精确率，召回率，而Scikit-Learn 不让你直接设置阈值，它会调用decision_function()方法。返回样例的分数值，然后基于这个分数值，使用你想要的任何阈值做出预测。 123456&gt;&gt;&gt; y_scores = sgd_clf.decision_function([some_digit])&gt;&gt;&gt; y_scoresarray([ 161855.74572176])&gt;&gt;&gt; threshold = 0&gt;&gt;&gt; y_some_digit_pred = (y_scores &gt; threshold)array([ True], dtype=bool) 每次都设定阈值不是一个完美的方法，如何才能找到合适的阈值呢？你需要再次使用cross_val_predict()得到每一个样例的分数值，但是这一次指定返回一个决策分数，而不是预测值。(阈值相关，就要进行打分) 12y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method="decision_function") 现在有了这些分数值。对于任何可能的阈值，使用precision_recall_curve(),你都可以计算精确率和召回率；precisions, recalls, thresholds是任何阈值的范围值，可以变化曲线和PR曲线 12from sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) 与PR曲线另一个相关的是ROC曲线（TPR/FPR），为了画出 ROC 曲线，你首先需要计算各种不同阈值下的 TPR、FPR，使用roc_curve()函数（还是要打分）；跳过ROC曲线(其实相当于已经做了)，想直接计算出ROC AUC也行。 12345from sklearn.metrics import roc_curvefpr, tpr, thresholds = roc_curve(y_train_5, y_scores)from sklearn.metrics import roc_auc_scoreroc_auc_score(y_train_5, y_scores) 如果想得到RandomForestClassifier的ROC曲线，由于RandomForestClassifier不提供decision_function()方法，相反，它提供了predict_proba()方法（另外一种概率打分），返回概率值，此时用正例概率作为分值。例如70%的概率是垃圾邮件。 另外，因为 ROC 曲线跟准确率/召回率曲线（或者叫 PR）很类似，你或许会好奇如何决定使用哪一个曲线呢？一个笨拙的规则是，优先使用 PR 曲线当正例很少，或者当你关注假正例多于假反例的时候。其他情况使用 ROC 曲线 多类别分类有一对一ovo, 一对多ova两种方法，一般svm由于在训练集的大小上很难扩展，因为它可以在小的数据集上面可以更多地训练，故用ovo，其他大部分用ova。如果Scikit-Learn嗅探出你想做一个多分类任务，它会自动使用ova，svm训练器除外 误差分析，将混淆矩阵归一化后用图片色块输出，查看哪些类别经常被错误分类。]]></content>
      <categories>
        <category>Sklearn 与 TensorFlow 机器学习实用指南</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ROC</tag>
        <tag>MNIST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn 与 TensorFlow 机器学习实用指南（一）：一个完整的程序]]></title>
    <url>%2F2018%2F07%2F09%2FSklearn%20%E4%B8%8E%20TensorFlow%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%2FSklearn-%E4%B8%8E-TensorFlow-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[写在前面：这个系列打算把「Hands-On Machine Learning with Scikit-Learn and TensorFlow 」重新梳理一遍，这本书在看完机器学习基础知识之后有一个很好的算法实践，对于算法落地有很多帮助。这次写的Sklearn 与 TensorFlow 机器学习实用指南系列，目的是让自己更清楚算法的每个流程处理，加强对一些机器学习模型理解。这本书在github有中文的翻译版本（还在更新）. 拆分数据集训练集+测试集12345678910import numpy as npdef split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) # 打乱序列 test_set_size = int(len(data) * test_ratio) # 拆分比例 test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] train_set, test_set = split_train_test(housing, 0.2) # housing数据二八拆分 或者直接将整体数据打乱，然后按需取量。(california_housing_dataframe为谷歌机器学习教程提供的加州住房数据) 1234california_housing_dataframe = california_housing_dataframe.reindex( # 整体打乱 np.random.permutation(california_housing_dataframe.index))train_set = california_housing_dataframe.head(12000)test_set = california_housing_dataframe.tail(5000) 以上为训练集+测试集的拆分方式 训练集+验证集+测试集这样的拆分方式主要有存在一些不足。1、程序多次运行后，测试集的数据有可能会加入到训练集当中，调参时用于改进模型超参数的测试集会造成过拟合。2、不便于新数据的加入 更好的办法是将数据集拆分为训练集+验证集+测试集。 那如何解决新加入数据的问题呢？一个通常的解决办法是使用每个实例的识别码，以判定是否这个实例是否应该放入测试集（假设实例有单一且不变的识别码）。例如，你可以计算出每个实例识别码的哈希值，只保留其最后一个字节，如果值小于等于 51（约为 256 的 20%），就将其放入测试集。这样可以保证在多次运行中，测试集保持不变，即使更新了数据集。新的测试集会包含新实例中的 20%，但不会有之前位于训练集的实例。可能很多数据没有稳定的特征，最简单的办法就是利用索引作为识别码。下面的代码根据识别码按0.7,0.2,0.1比例拆分训练集、验证集和测试集。 12345678910111213141516# 参数identifier为单一且不变的识别码，可以为索引id# hash(np.int64(identifier)).digest()[-1]返回识别码的哈希摘要值的最后一个字节def validate_set_check(identifier, validate_ratio, test_ratio, hash): return 256 * test_ratio &lt;= hash(np.int64(identifier)).digest()[-1] &lt; 256 * (validate_ratio+test_ratio) def test_set_check(identifier, test_ratio, hash): return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio # 记录满足条件的索引def split_train_test_by_id(data, validate_ratio, test_ratio, id_column, hash=hashlib.md5): ids = data[id_column] # 确定识别码 in_validate_set = ids.apply(lambda id_: validate_set_check(id_, validate_ratio, test_ratio，hash)) in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash)) combine_set = np.bitwise_or(in_validate_set, in_test_set) return data.loc[~combine_set], data.loc[in_validate_set], data.loc[in_test_set] housing_with_id = housing.reset_index() # housing数据增加一个索引列，放在数据的第一列train_set, validate_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 0.1, "index") 分成采样另外一种拆分方式：分成采样 将人群分成均匀的子分组，称为分层，从每个分层去除合适数量的实例，以保证测试集对总人数有代表性。例如，美国人口的 51.3% 是女性，48.7% 是男性。所以在美国，严谨的调查需要保证样本也是这个比例：513 名女性，487 名男性作为数据样本。数据集中的每个分层都要有足够的实例位于你的数据中，这点很重要。否则，对分层重要性的评估就会有偏差。这意味着，你不能有过多的分层，且每个分层都要足够大。后面的代码通过将收入中位数除以 1.5（以限制收入分类的数量），创建了一个收入类别属性，用ceil对值舍入（以产生离散的分类），然后将所有大于 5的分类归入到分类5 ： 1234567891011121314151617181920# 预处理，创建"income_cat"属性 # 凡是会对原数组作出修改并返回一个新数组的，往往都有一个 inplace可选参数# inplace=True,原数组名对应的内存值直接改变;inplace=False,原数组名对应的内存值并不改变，新的结果赋给一个新的数组.housing["income_cat"] = np.ceil(housing["median_income"] / 1.5)housing["income_cat"].where(housing["income_cat"] &lt; 5, 5.0, inplace=True)# 现在，就可以根据收入分类，进行分层采样。你可以使用 Scikit-Learn 的StratifiedShuffleSplit类from sklearn.model_selection import StratifiedShuffleSplit# random_state为随机种子生成器，可以得到相同的随机结果# n_splits是将训练数据分成train/test对的组数，这里汇总成一组数据split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing["income_cat"]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index]# 现在，你需要删除income_cat属性，使数据回到初始状态： for set in (strat_train_set, strat_test_set): set.drop(["income_cat"], axis=1, inplace=True) 数据预处理将原始数据映射到特征我们在进行机器学习的时候，采用的数据样本往往是矢量（特征矢量），而我们的原始数据并不是以矢量的形式呈现给我们的，这是便需要将数据映射到特征 整数和浮点数映射直接映射便ok（虽然机器学习是根据浮点值进行的训练，但是不需要将整数6转换为6.0，这个过程是默认的） 字符串映射好多时候，有的特征是字符串，比如此前训练的加利福尼亚房产数据集中的街区名称，机器学习是无法根据字符串来学习规律的，所以需要转换。但是存在一个问题，如果字符特征是’’一环’’ ‘’二环’’ ‘’三环’’…（代表某个城市的地理位置），那么对其进行数值转换的时候，是不可以编码为形如1，2，3，4…这样的数据的，因为其存在数据大小的问题，学习模型会把他们的大小关系作为特征而学习，所以我们需要引入独热编码,（具体解释见链接，解释的很好）.我们需要把这些文本标签转换为数字。Scikit-Learn 为这个任务提供了一个转换器LabelEncoder： 1234567891011# 简单来说 LabelEncoder 是对不连续的数字或者文本进行编号# le.fit([1,5,67,100])# le.transform([1,1,100,67,5])# 输出： array([0,0,3,2,1])&gt;&gt;&gt; from sklearn.preprocessing import LabelEncoder&gt;&gt;&gt; encoder = LabelEncoder()&gt;&gt;&gt; housing_cat = housing["ocean_proximity"]&gt;&gt;&gt; housing_cat_encoded = encoder.fit_transform(housing_cat) # 装换器&gt;&gt;&gt; housing_cat_encodedarray([1, 1, 4, ..., 1, 0, 3]) 译注: 在原书中使用LabelEncoder转换器来转换文本特征列的方式是错误的，该转换器只能用来转换标签（正如其名）。在这里使用LabelEncoder没有出错的原因是该数据只有一列文本特征值，在有多个文本特征列的时候就会出错。应使用factorize()方法来进行操作： 123&gt; housing_cat_encoded, housing_categories = housing_cat.factorize()&gt; housing_cat_encoded[:10]&gt; &gt; 处理离散特征这还不够，Scikit-Learn 提供了一个编码器OneHotEncoder，用于将整书分类值转变为独热向量。注意fit_transform()用于 2D 数组，而housing_cat_encoded是一个 1D 数组，所以需要将其变形： 12345678# reshape(-1,1)里面的-1代表将数据自动计算有多少行，但是列数明确设置为1# reshape(-1)则是变形为1行和自动计算有多少列&gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder&gt;&gt;&gt; encoder = OneHotEncoder()&gt;&gt;&gt; housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))&gt;&gt;&gt; housing_cat_1hot&lt;16513x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;' with 16513 stored elements in Compressed Sparse Row format&gt; 注意输出结果是一个 SciPy 稀疏矩阵，而不是 NumPy 数组。当类别属性有数千个分类时，这样非常有用。经过独热编码，我们得到了一个有数千列的矩阵，这个矩阵每行只有一个 1，其余都是 0。使用大量内存来存储这些 0 非常浪费，所以稀疏矩阵只存储非零元素的位置。你可以像一个 2D 数据那样进行使用，但是如果你真的想将其转变成一个（密集的）NumPy 数组，只需调用toarray()方法： 12345678&gt;&gt;&gt; housing_cat_1hot.toarray()array([[ 0., 1., 0., 0., 0.], [ 0., 1., 0., 0., 0.], [ 0., 0., 0., 0., 1.], ..., [ 0., 1., 0., 0., 0.], [ 1., 0., 0., 0., 0.], [ 0., 0., 0., 1., 0.]]) 使用类LabelBinarizer，我们可以用一步执行这两个转换（从文本分类到整数分类，再从整数分类到独热向量）： 1234567891011&gt;&gt;&gt; from sklearn.preprocessing import LabelBinarizer&gt;&gt;&gt; encoder = LabelBinarizer()&gt;&gt;&gt; housing_cat_1hot = encoder.fit_transform(housing_cat)&gt;&gt;&gt; housing_cat_1hotarray([[0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], ..., [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0]]) 注意默认返回的结果是一个密集 NumPy 数组。向构造器LabelBinarizer传递sparse_output=True，就可以得到一个稀疏矩阵。 译注: 在原书中使用LabelBinarizer的方式也是错误的，该类也应用于标签列的转换。正确做法是使用sklearn即将提供的CategoricalEncoder类。如果在你阅读此文时sklearn中尚未提供此类，用如下方式代替：（来自Pull Request #9151） 1234567&gt; #from sklearn.preprocessing import CategoricalEncoder # in future versions of Scikit-Learn&gt;&gt; cat_encoder = CategoricalEncoder()&gt; housing_cat_reshaped = housing_cat.values.reshape(-1, 1)&gt; housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)&gt; housing_cat_1hot&gt; &gt; 寻找良好特征（的特点）当得到特征之后，还是要进行筛选的，因为有的特征没有参考价值，就像我们的在做合成特征的时候，正常的特征数据是人均几间房间，而有的人是几十间，这明显没有参考价值良好特征的几点原则 避免很少使用的离散特征值：如果只是出现了一两次的特征几乎是没有意义的 最好具有清晰明确的含义：特征的含义不仅仅是让机器学习的模型学习的，人也要知道其具体的含义，不然不利于分析数据（最好将数值很大的秒转换为天数，或者年，让人看起来直观一些） 将“神奇”的值与实际数据混为一谈：有些特征中会出现一些”神奇的数据”，当然这些数据并不是很少的特征，而是超出范围的异常值，比如特征应该是介于0——1之间的，但是因为这个数据是空缺的，而采用的默认数值-1，那么这样的数值就是”神奇”，解决办法是，将该特征转换为两个特征： 一个特征只存储质正常范围的值，不含神奇值。 一个特征存储布尔值，表示的信息为是否为空 考虑上游不稳定性：由经验可知，特征的定义不应随时间发生变化，代表城市名称的话，那么特征值始终都该是城市的名称，但是有的时候，上游模型将特征值处理完毕后，返还给下游模型的却变成了数值，这样是不好的，因为这种表示在未来运行其他模型时可能轻易发生变化，那么特征就乱套了 ​ 可视化数据寻找规律：12345housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population", c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,)plt.legend() 每个圈的半径表示街区的人口（选项s），颜色代表价格（选项c）。我们用预先定义的名为jet的颜色图（选项cmap），它的范围是从蓝色（低价）到红色（高价）： 皮尔逊相关系数1corr_matrix = housing.corr() Pandas 的scatter_matrix函数另一种检测属性间相关系数的方法是使用 Pandas 的scatter_matrix函数,它能画出每个数值属性对每个其它数值属性的图。因为现在共有 11 个数值属性，你可以得到11 ** 2 = 121张图。 12345from pandas.tools.plotting import scatter_matrixattributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]scatter_matrix(housing[attributes], figsize=(12, 8)) 得到两个属性的散点图 清查数据截至目前，我们假定用于训练和测试的所有数据都是值得信赖的。在现实生活中，数据集中的很多样本是不可靠的，原因有以下一种或多种： 遗漏值。 例如，有人忘记为某个房屋的年龄输入值。(值会为-1，所以要分为两个特征，忘了的看上面) 重复样本。 例如，服务器错误地将同一条记录上传了两次。 不良标签。 例如，有人错误地将一颗橡树的图片标记为枫树。 不良特征值。 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。 一旦检测到存在这些问题，通常需要将相应样本从数据集中移除，从而“修正”不良样本。要检测遗漏值或重复样本，可以编写一个简单的程序。检测不良特征值或标签可能会比较棘手，可采用可视化数据的方法。 对于处理特征丢失的问题。前面，你应该注意到了属性total_bedrooms有一些缺失值。有三个解决选项： 去掉对应的街区；（数据大可用） 去掉整个属性； 进行赋值（0、平均值、中位数等等）。 用DataFrame的dropna()，drop()，和fillna()方法，可以方便地实现： 1234housing.dropna(subset=["total_bedrooms"]) # 选项1housing.drop("total_bedrooms", axis=1) # 选项2 axis=0对行操作，axis=1对列操作median = housing["total_bedrooms"].median()housing["total_bedrooms"].fillna(median) # 选项3 如果选择选项 3，你需要计算训练集的中位数，用中位数填充训练集的缺失值，不要忘记保存该中位数。后面用测试集评估系统时，需要替换测试集中的缺失值，也可以用来实时替换新数据中的缺失值。 Scikit-Learn 提供了一个方便的类来处理缺失值：Imputer。下面是其使用方法：首先，需要创建一个Imputer实例，指定用该属性的中位数替换它的每个缺失值： 123from sklearn.preprocessing import Imputerimputer = Imputer(strategy="median") # 进行中位数赋值 因为只有数值属性才能算出中位数，我们需要创建一份不包括文本属性ocean_proximity的数据副本： 1housing_num = housing.drop("ocean_proximity", axis=1) # 去除ocean_proximity不为数值属性的特征 现在，就可以用fit()方法将imputer实例拟合到训练数据： 1imputer.fit(housing_num) imputer计算出了每个属性的中位数，并将结果保存在了实例变量statistics_中。只有属性total_bedrooms有缺失值，但是我们确保一旦系统运行起来，新的数据中没有缺失值，所以安全的做法是将imputer应用到每个数值： 1234&gt;&gt;&gt; imputer.statistics_ # 实例变量statistics_和housing_num数值数据得到的中位数是一样的array([ -118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.5414])&gt;&gt;&gt; housing_num.median().valuesarray([ -118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.5414]) 现在，你就可以使用这个“训练过的”imputer来对训练集进行转换，通过将缺失值替换为中位数： 1X = imputer.transform(housing_num) 结果是一个普通的 Numpy 数组，包含有转换后的特征。如果你想将其放回到 PandasDataFrame中，也很简单： 1housing_tr = pd.DataFrame(X, columns=housing_num.columns) # 得到处理缺失值后的DF数据 整理数据：数据缩放有两种常见的方法可以让所有的属性有相同的量度：线性函数归一化（Min-Max scaling）和标准化（standardization）。Scikit-Learn 提供了一个转换器MinMaxScaler来实现这个功能。它有一个超参数feature_range，可以让你改变范围，如果不希望范围是 0 到 1；Scikit-Learn 提供了一个转换器StandardScaler来进行标准化 min-max方式,对应的方法为 1MinMaxScaler(self, feature_range=(0, 1), copy=True) standardization 标准化数据,对应的方法为 1StandardScaler(self, copy=True, with_mean=True, with_std=True) 警告：与所有的转换一样，缩放器只能向训练集拟合，而不是向完整的数据集（包括测试集）。只有这样，你才能用缩放器转换训练集和测试集（和新数据）。 处理极端离群值还是举加利福尼亚州住房数据集中的人均住房数的例子，有的极端值达到了50对于这些极端值其实很好处理，无非几个办法 对数缩放 1roomsPerPerson = log((totalRooms / population) + 1) 特征值限制到 某个上限或者下限 1roomsPerPerson = min(totalRooms / population, 4) # 大于4.0的取4.0 分箱分箱其实是一个形象化的说法，就是把数据分开来，装在一个个箱子里，这样一个箱子里的数据就是一家人了。那有什么用呢？下面就举个栗子！ 在数据集中，latitude 是一个浮点值。不过，在我们的模型中将 latitude 表示为浮点特征没有意义。这是因为纬度和房屋价值之间不存在线性关系。例如，纬度 35 处的房屋并不比纬度 34 处的房屋贵 35/34（或更便宜）。但是，纬度或许能很好地预测房屋价值。 我们现在拥有 11 个不同的布尔值特征（LatitudeBin1、LatitudeBin2、…、LatitudeBin11），而不是一个浮点特征。拥有 11 个不同的特征有点不方便，因此我们将它们统一成一个 11 元素矢量。这样做之后，我们可以将纬度 37.4 表示为： [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] 分箱之后，我们的模型现在可以为每个纬度学习完全不同的权重。（是不是觉得有点像独热编码，没错，就是的） 为了简单起见，我们在纬度样本中使用整数作为分箱边界。如果我们需要更精细的解决方案，我们可以每隔 1/10 个纬度拆分一次分箱边界。添加更多箱可让模型从纬度 37.4 处学习和维度 37.5 处不一样的行为，但前提是每 1/10 个纬度均有充足的样本可供学习。 另一种方法是按分位数分箱，这种方法可以确保每个桶内的样本数量是相等的。按分位数分箱完全无需担心离群值。 123456789101112131415161718192021222324'''分桶也称为分箱。例如，我们可以将 population 分为以下 3 个分桶：bucket_0 (&lt; 5000)：对应于人口分布较少的街区bucket_1 (5000 - 25000)：对应于人口分布适中的街区bucket_2 (&gt; 25000)：对应于人口分布较多的街区根据前面的分桶定义，以下 population 矢量：[[10001], [42004], [2500], [18000]]将变成以下经过分桶的特征矢量：[[1], [2], [0], [1]]这些特征值现在是分桶索引。请注意，这些索引被视为离散特征。通常情况下，这些特征将被进一步转换为上述独热表示法，但这是以透明方式实现的。要为分桶特征定义特征列，我们可以使用 bucketized_column（而不是使用 numeric_column），该列将数字列作为输入，并使用 boundardies 参数中指定的分桶边界将其转换为分桶特征。以下代码为 households 和 longitude 定义了分桶特征列；get_quantile_based_boundaries 函数会根据分位数计算边界，以便每个分桶包含相同数量的元素'''def get_quantile_based_boundaries(feature_values, num_buckets): boundaries = np.arange(1.0, num_buckets) / num_buckets quantiles = feature_values.quantile(boundaries) return [quantiles[q] for q in quantiles.keys()]# Divide households into 7 buckets.households = tf.feature_column.numeric_column("households") # 定义数值特征# 分桶特征bucketized_column第一个参数用数字列 numeric_column得到的households，第二个参数用上面get_quantile_based_boundaries方法得到的分桶数据，返回的bucketized_households为可使用的分桶特征bucketized_households = tf.feature_column.bucketized_column( households,boundaries=get_quantile_based_boundaries(california_housing_dataframe["households"], 7)) 自定义转换器尽管 Scikit-Learn 提供了许多有用的转换器，你还是需要自己动手写转换器执行任务，比如自定义的清理操作，或属性组合。你需要让自制的转换器与 Scikit-Learn 组件（比如流水线）无缝衔接工作，因为 Scikit-Learn 是依赖鸭子类型的（而不是继承，忽略对象，只要行为像就行），你所需要做的是创建一个类并执行三个方法：fit()（返回self），transform()，和fit_transform()。通过添加TransformerMixin作为基类，可以很容易地得到最后一个。另外，如果你添加BaseEstimator作为基类（且构造器中避免使用args和kargs），你就能得到两个额外的方法（get_params()和set_params()），二者可以方便地进行超参数自动微调。例如，一个小转换器类添加了上面讨论的属性： 12345678910111213141516171819202122# 添加一个特征组合的装换器from sklearn.base import BaseEstimator, TransformerMixinrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6# 这里的示例没有定义fit_transform()，可能是因为fit()没有做任何动作（我猜的class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, household_ix] # X[:,3]表示的是第4列所有数据 population_per_household = X[:, population_ix] / X[:, household_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, # np.c_表示的是拼接数组。 bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household]attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)housing_extra_attribs = attr_adder.transform(housing.values) # 返回一个加入新特征的数据 在这个例子中，转换器有一个超参数add_bedrooms_per_room，默认设为True（提供一个合理的默认值很有帮助）。这个超参数可以让你方便地发现添加了这个属性是否对机器学习算法有帮助。更一般地，你可以为每个不能完全确保的数据准备步骤添加一个超参数。数据准备步骤越自动化，可以自动化的操作组合就越多，越容易发现更好用的组合（并能节省大量时间）。 另外sklearn是不能直接处理DataFrames的，那么我们需要自定义一个处理的方法将之转化为numpy类型 1234567class DataFrameSelector(BaseEstimator,TransformerMixin): def __init__(self,attribute_names): #可以为列表 self.attribute_names = attribute_names def fit(self,X,y=None): return self def transform(self,X): return X[self.attribute_names].values #返回的为numpy array 转换流水线目前在数据预处理阶段，我们需要对缺失值进行处理、特征组合和特征缩放。每一步的执行都有着先后顺序，存在许多数据转换步骤，需要按一定的顺序执行。sklearn提供了Pipeline帮助顺序完成转换幸运的是，Scikit-Learn 提供了类Pipeline，来进行这一系列的转换。下面是一个数值属性的小流水线： 12345678910from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalernum_pipeline = Pipeline([ ('imputer', Imputer(strategy="median")), # 处理缺失值 ('attribs_adder', CombinedAttributesAdder()), # 特征组合 ('std_scaler', StandardScaler()), # 特征缩放 ])housing_num_tr = num_pipeline.fit_transform(housing_num) Pipeline构造器需要一个定义步骤顺序的名字/估计器对的列表。除了最后一个估计器，其余都要是转换器（即，它们都要有fit_transform()方法）。名字可以随意起。 当你调用流水线的fit()方法，就会对所有转换器顺序调用fit_transform()方法，将每次调用的输出作为参数传递给下一个调用，一直到最后一个估计器，它只执行fit()方法。 估计器（Estimator）：很多时候可以直接理解成分类器，主要包含两个函数：fit()和predict()转换器（Transformer）：转换器用于数据预处理和数据转换，主要是三个方法：fit（）,transform()和fit_transform() 最后的估计器是一个StandardScaler，它是一个转换器，因此这个流水线有一个transform()方法，可以顺序对数据做所有转换（它还有一个fit_transform方法可以使用，就不必先调用fit()再进行transform()）。 1234567891011121314num_attribs = list(housing_num) # 返回的为列名[col1,col2,....]cat_attribs = ["ocean_proximity"]num_pipeline = Pipeline([ # 数值类型 ('selector', DataFrameSelector(num_attribs)), # DataFrames转为numpy array ('imputer', Imputer(strategy="median")), # 缺失值处理 ('attribs_adder', CombinedAttributesAdder()), # 特征组合 ('std_scaler', StandardScaler()), # 缩放 ])cat_pipeline = Pipeline([ # 标签类型 ('selector', DataFrameSelector(cat_attribs)), # DataFrames转为numpy array ('cat_encoder', CategoricalEncoder(encoding="onehot-dense")), ]) 上面定义的为分别处理数值类型和标签类型的转换流程，housing_num为DataFrame类型，list(DataFrame)的结果返回的为列名字。上面着两个流程还可以再整合一起。 1234567891011121314from sklearn.pipeline import FeatureUnionfull_pipeline = FeatureUnion(transformer_list=[ ("num_pipeline", num_pipeline), ("cat_pipeline", cat_pipeline), ])housing_prepared = full_pipeline.fit_transform(housing) # 最终的结果&gt;&gt;&gt; housing_preparedarray([[ 0.73225807, -0.67331551, 0.58426443, ..., 0. , 0. , 0. ], [-0.99102923, 1.63234656, -0.92655887, ..., 0. , 0. , 0. ], [...]&gt;&gt;&gt; housing_prepared.shape(16513, 17) 译注: 如果你在上面代码中的cat_pipeline流水线使用LabelBinarizer转换器会导致执行错误，解决方案是用上文提到的CategoricalEncoder转换器来代替： 12345&gt; cat_pipeline = Pipeline([&gt; ('selector', DataFrameSelector(cat_attribs)),&gt; ('cat_encoder', CategoricalEncoder(encoding="onehot-dense")),&gt; ])&gt; &gt; 每个子流水线都以一个选择转换器开始：通过选择对应的属性（数值或分类）、丢弃其它的，来转换数据，并将输出DataFrame转变成一个 NumPy 数组。Scikit-Learn 没有工具来处理 PandasDataFrame，因此我们需要写一个简单的自定义转换器来做这项工作： 123456789from sklearn.base import BaseEstimator, TransformerMixinclass DataFrameSelector(BaseEstimator, TransformerMixin): def __init__(self, attribute_names): self.attribute_names = attribute_names def fit(self, X, y=None): return self def transform(self, X): return X[self.attribute_names].values 训练模型我们先来训练一个线性回归模型： 1234from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(housing_prepared, housing_labels) # 利用预处理好的数据进行训练模型 完毕！你现在就有了一个可用的线性回归模型。用一些训练集中的实例做下验证： 1234567&gt;&gt;&gt; some_data = housing.iloc[:5] # 前五个作为预测数据&gt;&gt;&gt; some_labels = housing_labels.iloc[:5]&gt;&gt;&gt; some_data_prepared = full_pipeline.transform(some_data)&gt;&gt;&gt; print("Predictions:\t", lin_reg.predict(some_data_prepared)) # 预测结果Predictions: [ 303104. 44800. 308928. 294208. 368704.]&gt;&gt;&gt; print("Labels:\t\t", list(some_labels))Labels: [359400.0, 69700.0, 302100.0, 301300.0, 351900.0] # 实际结果 行的通，尽管预测并不怎么准确（比如，第二个预测偏离了 50%！）。让我们使用 Scikit-Learn 的mean_squared_error函数，用全部训练集来计算下这个回归模型的 RMSE： 123456&gt;&gt;&gt; from sklearn.metrics import mean_squared_error&gt;&gt;&gt; housing_predictions = lin_reg.predict(housing_prepared)&gt;&gt;&gt; lin_mse = mean_squared_error(housing_labels, housing_predictions)&gt;&gt;&gt; lin_rmse = np.sqrt(lin_mse)&gt;&gt;&gt; lin_rmse68628.413493824875 OK，有总比没有强，但显然结果并不好，这是一个模型欠拟合训练数据的例子。当这种情况发生时，意味着特征没有提供足够多的信息来做出一个好的预测，或者模型并不强大，修复欠拟合的主要方法是选择一个更强大的模型，给训练算法提供更好的特征，或去掉模型上的限制，你可以尝试添加更多特征（比如，人口的对数值），但是首先让我们尝试一个更为复杂的模型，看看效果。训练一个决策树模型DecisionTreeRegressor。这是一个强大的模型，可以发现数据中复杂的非线性关系。 123456789from sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor()tree_reg.fit(housing_prepared, housing_labels)&gt;&gt;&gt; housing_predictions = tree_reg.predict(housing_prepared)&gt;&gt;&gt; tree_mse = mean_squared_error(housing_labels, housing_predictions)&gt;&gt;&gt; tree_rmse = np.sqrt(tree_mse)&gt;&gt;&gt; tree_rmse0.0 等一下，发生了什么？没有误差？这个模型可能是绝对完美的吗？当然，更大可能性是这个模型严重过拟合数据。如何确定呢？如前所述，直到你准备运行一个具备足够信心的模型，都不要碰测试集，因此你需要使用训练集的部分数据来做训练，用一部分来做模型验证。 用交叉验证做更佳的评估使用 Scikit-Learn 的交叉验证功能。下面的代码采用了 K 折交叉验证（K-fold cross-validation）：它随机地将训练集分成十个不同的子集，成为“折”，然后训练评估决策树模型 10 次，每次选一个不用的折来做评估，用其它 9 个来做训练。结果是一个包含 10 个评分的数组： 1234from sklearn.model_selection import cross_val_scorescores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)rmse_scores = np.sqrt(-scores) 警告：Scikit-Learn 交叉验证功能期望的是效用函数（越大越好）而不是损失函数（越低越好），因此得分函数实际上与 MSE 相反（即负值），这就是为什么前面的代码在计算平方根之前先计算-scores。 来看下结果 1234567891011&gt;&gt;&gt; def display_scores(scores):... print("Scores:", scores)... print("Mean:", scores.mean())... print("Standard deviation:", scores.std())...&gt;&gt;&gt; display_scores(tree_rmse_scores)Scores: [ 74678.4916885 64766.2398337 69632.86942005 69166.67693232 71486.76507766 73321.65695983 71860.04741226 71086.32691692 76934.2726093 69060.93319262]Mean: 71199.4280043Standard deviation: 3202.70522793 现在决策树就不像前面看起来那么好了。实际上，它看起来比线性回归模型还糟！注意到交叉验证不仅可以让你得到模型性能的评估，还能测量评估的准确性（即，它的标准差）。决策树的评分大约是 71200，通常波动有 ±3200。如果只有一个验证集，就得不到这些信息。但是交叉验证的代价是训练了模型多次，不可能总是这样。 让我们计算下线性回归模型的的相同分数，以做确保： 12345678910&gt;&gt;&gt; lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,... scoring="neg_mean_squared_error", cv=10)...&gt;&gt;&gt; lin_rmse_scores = np.sqrt(-lin_scores)&gt;&gt;&gt; display_scores(lin_rmse_scores)Scores: [ 70423.5893262 65804.84913139 66620.84314068 72510.11362141 66414.74423281 71958.89083606 67624.90198297 67825.36117664 72512.36533141 68028.11688067]Mean: 68972.377566Standard deviation: 2493.98819069 判断没错：决策树模型过拟合很严重，它的性能比线性回归模型还差 现在再尝试最后一个模型：RandomForestRegressor（随机森林），随机森林是通过用特征的随机子集训练许多决策树。在其它多个模型之上建立模型成为集成学习（Ensemble Learning），它是推进 ML 算法的一种好方法。我们会跳过大部分的代码，因为代码本质上和其它模型一样： 123456789101112&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor&gt;&gt;&gt; forest_reg = RandomForestRegressor()&gt;&gt;&gt; forest_reg.fit(housing_prepared, housing_labels)&gt;&gt;&gt; [...]&gt;&gt;&gt; forest_rmse22542.396440343684&gt;&gt;&gt; display_scores(forest_rmse_scores)Scores: [ 53789.2879722 50256.19806622 52521.55342602 53237.44937943 52428.82176158 55854.61222549 52158.02291609 50093.66125649 53240.80406125 52761.50852822]Mean: 52634.1919593Standard deviation: 1576.20472269 现在好多了：随机森林看起来很有希望。但是，训练集的评分仍然比验证集的评分低很多。解决过拟合可以通过简化模型，给模型加限制（即，正则化），或用更多的训练数据。在深入随机森林之前，你应该尝试下机器学习算法的其它类型模型（不同核心的支持向量机，神经网络，等等），不要在调节超参数上花费太多时间。目标是列出一个可能模型的列表（两到五个）。 提示：你要保存每个试验过的模型，以便后续可以再用。要确保有超参数和训练参数，以及交叉验证评分，和实际的预测值。这可以让你比较不同类型模型的评分，还可以比较误差种类。你可以用 Python 的模块pickle，非常方便地保存 Scikit-Learn 模型，或使用sklearn.externals.joblib，后者序列化大 NumPy 数组更有效率： 12345from sklearn.externals import joblibjoblib.dump(my_model, "my_model.pkl")# 然后my_model_loaded = joblib.load("my_model.pkl") 模型微调网格搜索：使用 Scikit-Learn 的GridSearchCV来做这项搜索工作。你所需要做的是告诉GridSearchCV要试验有哪些超参数，要试验什么值，GridSearchCV就能用交叉验证试验所有可能超参数值的组合。例如，下面的代码搜索了RandomForestRegressor超参数值的最佳组合（很费时间）： 12345678910111213from sklearn.model_selection import GridSearchCVparam_grid = [ &#123;'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]&#125;, &#123;'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]&#125;, ]forest_reg = RandomForestRegressor()grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')grid_search.fit(housing_prepared, housing_labels) 当你不能确定超参数该有什么值，一个简单的方法是尝试连续的 10 的幂（如果想要一个粒度更小的搜寻，可以用更小的数，就像在这个例子中对超参数n_estimators做的）。 param_grid告诉 Scikit-Learn 首先评估所有的列在第一个dict中的n_estimators和max_features的3 × 4 = 12种组合（不用担心这些超参数的含义，会在第 7 章中解释）。然后尝试第二个dict中超参数的2 × 3 = 6种组合，这次会将超参数bootstrap设为False而不是True（后者是该超参数的默认值）。完成后，你就能获得参数的最佳组合，如下所示： 12&gt;&gt;&gt; grid_search.best_params_&#123;'max_features': 6, 'n_estimators': 30&#125; 你还能直接得到最佳的估计器： 123456&gt;&gt;&gt; grid_search.best_estimator_RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=6, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=None, verbose=0, warm_start=False) 当然，也可以得到评估得分： 12345678910111213141516171819202122&gt;&gt;&gt; cvres = grid_search.cv_results_... for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):... print(np.sqrt(-mean_score), params)...64912.0351358 &#123;'max_features': 2, 'n_estimators': 3&#125;55535.2786524 &#123;'max_features': 2, 'n_estimators': 10&#125;52940.2696165 &#123;'max_features': 2, 'n_estimators': 30&#125;60384.0908354 &#123;'max_features': 4, 'n_estimators': 3&#125;52709.9199934 &#123;'max_features': 4, 'n_estimators': 10&#125;50503.5985321 &#123;'max_features': 4, 'n_estimators': 30&#125;59058.1153485 &#123;'max_features': 6, 'n_estimators': 3&#125;52172.0292957 &#123;'max_features': 6, 'n_estimators': 10&#125;49958.9555932 &#123;'max_features': 6, 'n_estimators': 30&#125;59122.260006 &#123;'max_features': 8, 'n_estimators': 3&#125;52441.5896087 &#123;'max_features': 8, 'n_estimators': 10&#125;50041.4899416 &#123;'max_features': 8, 'n_estimators': 30&#125;62371.1221202 &#123;'bootstrap': False, 'max_features': 2, 'n_estimators': 3&#125;54572.2557534 &#123;'bootstrap': False, 'max_features': 2, 'n_estimators': 10&#125;59634.0533132 &#123;'bootstrap': False, 'max_features': 3, 'n_estimators': 3&#125;52456.0883904 &#123;'bootstrap': False, 'max_features': 3, 'n_estimators': 10&#125;58825.665239 &#123;'bootstrap': False, 'max_features': 4, 'n_estimators': 3&#125;52012.9945396 &#123;'bootstrap': False, 'max_features': 4, 'n_estimators': 10&#125; 在这个例子中，我们通过设定超参数max_features为 6，n_estimators为 30，得到了最佳方案。对这个组合，RMSE 的值是 49959，这比之前使用默认的超参数的值（52634）要稍微好一些。祝贺你，你成功地微调了最佳模型！ 随机搜索：当探索相对较少的组合时，就像前面的例子，网格搜索还可以。但是当超参数的搜索空间很大时，最好使用RandomizedSearchCV。这个类的使用方法和类GridSearchCV很相似，但它不是尝试所有可能的组合，而是通过选择每个超参数的一个随机值的特定数量的随机组合。这个方法有两个优点： 如果你让随机搜索运行，比如 1000 次，它会探索每个超参数的 1000 个不同的值（而不是像网格搜索那样，只搜索每个超参数的几个值） 你可以方便地通过设定搜索次数，控制超参数搜索的计算量。 分析最佳模型和它们的误差通过分析最佳模型，常常可以获得对问题更深的了解。比如，RandomForestRegressor可以指出每个属性对于做出准确预测的相对重要性： 12345678&gt;&gt;&gt; feature_importances = grid_search.best_estimator_.feature_importances_&gt;&gt;&gt; feature_importancesarray([ 7.14156423e-02, 6.76139189e-02, 4.44260894e-02, 1.66308583e-02, 1.66076861e-02, 1.82402545e-02, 1.63458761e-02, 3.26497987e-01, 6.04365775e-02, 1.13055290e-01, 7.79324766e-02, 1.12166442e-02, 1.53344918e-01, 8.41308969e-05, 2.68483884e-03, 3.46681181e-03]) 将重要性分数和属性名放到一起： 1234567891011121314151617181920&gt;&gt;&gt; extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]&gt;&gt;&gt; cat_one_hot_attribs = list(encoder.classes_)&gt;&gt;&gt; attributes = num_attribs + extra_attribs + cat_one_hot_attribs&gt;&gt;&gt; sorted(zip(feature_importances,attributes), reverse=True)[(0.32649798665134971, 'median_income'), (0.15334491760305854, 'INLAND'), (0.11305529021187399, 'pop_per_hhold'), (0.07793247662544775, 'bedrooms_per_room'), (0.071415642259275158, 'longitude'), (0.067613918945568688, 'latitude'), (0.060436577499703222, 'rooms_per_hhold'), (0.04442608939578685, 'housing_median_age'), (0.018240254462909437, 'population'), (0.01663085833886218, 'total_rooms'), (0.016607686091288865, 'total_bedrooms'), (0.016345876147580776, 'households'), (0.011216644219017424, '&lt;1H OCEAN'), (0.0034668118081117387, 'NEAR OCEAN'), (0.0026848388432755429, 'NEAR BAY'), (8.4130896890070617e-05, 'ISLAND')] 有了这个信息，你就可以丢弃一些不那么重要的特征（比如，显然只要一个分类ocean_proximity就够了，所以可以丢弃掉其它的）。你还应该看一下系统犯的误差，搞清为什么会有些误差，以及如何改正问题（添加更多的特征，或相反，去掉没有什么信息的特征，清洗异常值等等）。 模型评估调节完系统之后，你终于有了一个性能足够好的系统。现在就可以用测试集评估最后的模型了。这个过程没有什么特殊的：从测试集得到预测值和标签，运行full_pipeline转换数据（调用transform()，而不是fit_transform()！），再用测试集评估最终模型： 1234567891011final_model = grid_search.best_estimator_X_test = strat_test_set.drop("median_house_value", axis=1)y_test = strat_test_set["median_house_value"].copy()X_test_prepared = full_pipeline.transform(X_test)final_predictions = final_model.predict(X_test_prepared)final_mse = mean_squared_error(y_test, final_predictions)final_rmse = np.sqrt(final_mse) # =&gt; evaluates to 48,209.6 评估结果通常要比交叉验证的效果差一点，如果你之前做过很多超参数微调（因为你的系统在验证集上微调，得到了不错的性能，通常不会在未知的数据集上有同样好的效果）。这个例子不属于这种情况，但是当发生这种情况时，你一定要忍住不要调节超参数，使测试集的效果变好；这样的提升不能推广到新数据上。]]></content>
      <categories>
        <category>Sklearn 与 TensorFlow 机器学习实用指南</category>
      </categories>
      <tags>
        <tag>拆分数据</tag>
        <tag>机器学习</tag>
        <tag>数据预处理</tag>
        <tag>K折交叉验证</tag>
        <tag>格子搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（13）：兼容、缓存、上下文]]></title>
    <url>%2F2018%2F07%2F04%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%8813%EF%BC%89%EF%BC%9A%E5%85%BC%E5%AE%B9%E3%80%81%E7%BC%93%E5%AD%98%E3%80%81%E4%B8%8A%E4%B8%8B%E6%96%87%2F</url>
    <content type="text"><![CDATA[兼容Python2+和Python3+很多时候你可能希望你开发的程序能够同时兼容Python2+和Python3+。 试想你有一个非常出名的Python模块被很多开发者使用着，但并不是所有人都只使用Python2或者Python3。这时候你有两个办法。第一个办法是开发两个模块，针对Python2一个，针对Python3一个。还有一个办法就是调整你现在的代码使其同时兼容Python2和Python3。 本节中，我将介绍一些技巧，让你的脚本同时兼容Python2和Python3。 Future模块导入第一种也是最重要的方法，就是导入__future__模块。它可以帮你在Python2中导入Python3的功能。这有一组例子： 上下文管理器是Python2.6+引入的新特性，如果你想在Python2.5中使用它可以这样做： 1from __future__ import with_statement 在Python3中print已经变为一个函数。如果你想在Python2中使用它可以通过__future__导入： 123456print# Output:from __future__ import print_functionprint(print)# Output: &lt;built-in function print&gt; 模块重命名首先，告诉我你是如何在你的脚本中导入模块的。大多时候我们会这样做： 123import foo # orfrom foo import bar 你知道么，其实你也可以这样做： 1import foo as foo 这样做可以起到和上面代码同样的功能，但最重要的是它能让你的脚本同时兼容Python2和Python3。现在我们来看下面的代码： 1234try: import urllib.request as urllib_request # for Python 3except ImportError: import urllib2 as urllib_request # for Python 2 过期的Python2内置功能另一个需要了解的事情就是Python2中有12个内置功能在Python3中已经被移除了。要确保在Python2代码中不要出现这些功能来保证对Python3的兼容。这有一个强制让你放弃12内置功能的方法： 1from future.builtins.disabled import * 现在，只要你尝试在Python3中使用这些被遗弃的模块时，就会抛出一个NameError异常如下： 1234from future.builtins.disabled import *apply()# Output: NameError: obsolete Python 2 builtin apply is disabled 标准库向下兼容的外部支持 有一些包在非官方的支持下为Python2提供了Python3的功能。例如，我们有： enum pip install enum34 singledispatch pip install singledispatch pathlib pip install pathlib 想更多了解，在Python文档中有一个全面的指南可以帮助你让你的代码同时兼容Python2和Python3。 函数缓存 (Function caching)函数缓存允许我们将一个函数对于给定参数的返回值缓存起来。当一个I/O密集的函数被频繁使用相同的参数调用的时候，函数缓存可以节约时间。在Python 3.2版本以前我们只有写一个自定义的实现。在Python 3.2以后版本，有个lru_cache的装饰器，允许我们将一个函数的返回值快速地缓存或取消缓存。 Python 3.2及以后版本我们来实现一个斐波那契计算器，并使用lru_cache。 12345678910from functools import lru_cache@lru_cache(maxsize=32)def fib(n): if n &lt; 2: return n return fib(n-1) + fib(n-2)&gt;&gt;&gt; print([fib(n) for n in range(10)])# Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] 那个maxsize参数是告诉lru_cache，最多缓存最近多少个返回值。 我们也可以轻松地对返回值清空缓存，通过这样： 1fib.cache_clear() 上下文管理器(Context managers)上下文管理器允许你在有需要的时候，精确地分配和释放资源。上下文管理器的常用于一些资源的操作，需要在资源的正确获取与释放相关的操作 ，先看一个例子,我们经常会用到 try … catch … finally 语句确保一些系统资源得以正确释放。如: 12345678try: f = open('somefile') for line in f: print(line)except Exception as e: print(e)finally: f.close() 我们经常用到上面的代码模式，用复用代码的模式来讲，并不够好。于是 with 语句出现了，通过定义一个上下文管理器来封装这个代码块: 123with open('somefile') as f: for line in f: print(line) 使用上下文管理器最广泛的案例就是with语句了。想象下你有两个需要结对执行的相关操作，然后还要在它们中间放置一段代码。 上下文管理器就是专门让你做这种事情的。上面这段代码打开了一个文件，往里面写入了一些数据，然后关闭该文件。如果在往文件写数据时发生异常，它也会尝试去关闭文件。这就是with语句的主要优势，它确保我们的文件会被关闭，而不用关注嵌套代码如何退出。 上下文管理器的一个常见用例，是资源的加锁和解锁，以及关闭已打开的文件（就像我已经展示给你看的）。 实际上，我们可以同时处理多个上下文管理器： 12with A() as a, B() as b: suite 上下文管理协议与迭代器类似，实现了迭代协议的函数/对象即为迭代器。实现了上下文协议的函数/对象即为上下文管理器。迭代器协议是实现了__iter__方法。上下文管理协议则是一个类实现__enter__ (self)和__exit__(self, exc_type, exc_valye, traceback)方法就可以了。实行如下结构： 123456789101112class Contextor: # __enter__返回一个对象，通常是当前类的实例，也可以是其他对象。 def __enter__(self): pass def __exit__(self, exc_type, exc_val, exc_tb): passcontextor = Contextor()with contextor [as var]: with_body Contextor 实现了__enter__和__exit__这两个上下文管理器协议，当Contextor调用/实例化的时候，则创建了上下文管理器contextor 通过定义__enter__和__exit__方法的类（包括自己定义的类，只要加上特定的两个方法即可），我们可以在with语句里使用它。我们来看看在底层都发生了什么。 执行步骤： 执行 contextor (实例化具有上下文协议的对象，这里也称为上下文表达式)以获取上下文管理器，上下文表达式就是 with 和 as 之间的代码。 加载上下文管理器对象的 exit()方法，备用。 调用上下文管理器的 enter() 方法 如果有 as var 从句，则将 enter() 方法的返回值赋给 var 执行子代码块 with_body with语句调用上下文管理器之前暂存的 exit() 方法，如果 with_body 的退出是由异常引发的，那么该异常的 type、value 和 traceback 会作为参数传给 exit()，否则传三个 None。然后，exit()需要明确地返回 True 或 False。当返回 True 时，异常不会被向上抛出，当返回 False 时曾会向上抛出。 如果 with_body 的退出由异常引发，它让exit()方法来处理异常，并且 exit() 的返回值等于 False，那么这个异常将被with语句重新引发抛出一次；如果 exit() 的返回值等于 True，那么这个异常就被无视掉，继续执行后面的代码。 上下文管理器工具通过实现上下文协议定义创建上下文管理器很方便，Python为了更优雅，还专门提供了一个模块用于实现更函数式的上下文管理器用法。Python的contextlib模块专门用于这个目的。 AbstractContextManager ： 此类在 Python3.6中新增，提供了默认的enter()和exit()实现。enter()返回自身，exit()返回 None。 contextmanager： 我们要实现上下文管理器，总是要写一个类。此函数则容许我们通过一个装饰一个生成器函数得到一个上下文管理器。 123456789101112131415import contextlib@contextlib.contextmanagerdef database(): db = Database() try: if not db.connected: db.connect() yield db # 生成器 except Exception as e: db.close()def handle_query(): with database() as db: print 'handle ---', db.query() 使用contextlib 定义一个上下文管理器函数，通过with语句，database调用生成一个上下文管理器，然后调用函数隐式的__enter__方法，并将结果通yield返回。最后退出上下文环境的时候，在exception代码块中执行了__exit__方法。当然我们可以手动模拟上述代码的执行的细节。注意：yield 只能返回一次，返回的对象 被绑定到 as 后的变量，不需要返回时可以直接 yield，不带返回值。退出时则从 yield 之后执行。由于contextmanager继承自ContextDecorator，所以被contextmanager装饰过的生成器也可以用作装饰器。 1234567891011121314151617181920212223In [1]: context = database() # 创建上下文管理器In [2]: context&lt;contextlib.GeneratorContextManager object at 0x107188f10&gt;In [3]: db = context.__enter__() # 进入with语句In [4]: db # as语句，返回 Database实例Out[4]: &lt;__main__.Database at 0x107188a10&gt;In [5]: db.query() Out[5]: 'query data'In [6]: db.connectedOut[6]: TrueIn [7]: db.__exit__(None, None, None) # 退出with语句In [8]: dbOut[8]: &lt;__main__.Database at 0x107188a10&gt;In [9]: db.connectedOut[9]: False ContextDecorator： 我们可以实现一个上下文管理器，同时可以用作装饰器。 12345678910111213141516171819202122class AContext(ContextDecorator): def __enter__(self): print('Starting') return self def __exit__(self, exc_type, exc_value, traceback): print('Finishing') return False# 在 with 中使用with AContext(): print('祖国伟大')# 用作装饰器@AContext()def print_sth(sth): print(sth)print_sth('祖国伟大')#在这两种写法中，有没有发现，第二种写法更好，因为我们减少了一次代码缩进，可读性更强 还有一种好处：当我们已经实现了某个上下文管理器时，只要增加一个继承类，该上下文管理器立刻编程装饰器。 1234567from contextlib import ContextDecoratorclass mycontext(ContextBaseClass, ContextDecorator): def __enter__(self): return self def __exit__(self, *exc): return False]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（12）：协程与异步IO]]></title>
    <url>%2F2018%2F07%2F03%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%8812%EF%BC%89%EF%BC%9A%E5%8D%8F%E7%A8%8B%E4%B8%8E%E5%BC%82%E6%AD%A5IO%2F</url>
    <content type="text"><![CDATA[协程子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。 子程序调用总是一个入口，一次返回，调用顺序是明确的。而协程的调用和子程序不同。协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。 注意，在一个子程序中中断，去执行其他子程序，不是函数调用，有点类似CPU的中断。比如子程序A、B： 123456789def A(): print '1' print '2' print '3'def B(): print 'x' print 'y' print 'z' 假设由协程执行，在执行A的过程中，可以随时中断，去执行B，B也可能在执行过程中中断再去执行A，结果可能是： 12345612xy3z 多线程比，协程有何优势？最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。 第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。 因为协程是一个线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。Python对协程的支持是通过generator实现的。 一个例子：生产者－消费者的协程现在我们要让生产者发送1,2,3,4,5给消费者，消费者接受数字，返回状态给生产者，而我们的消费者只需要3,4,5就行了，当数字等于3时，会返回一个错误的状态。最终我们需要由主程序来监控生产者－消费者的过程状态，调度结束程序。 1234567891011121314151617181920212223242526#-*- coding:utf-8def consumer(): status = True while True: n = yield status print("我拿到了&#123;&#125;!".format(n)) if n == 3: status = Falsedef producer(consumer): n = 5 while n &gt; 0: # yield给主程序返回消费者的状态 # consumer.send(n)把n传值给c生成器，同时返回c生成器yield的结果（相当于fetch取一个放一个东西） yield consumer.send(n) n -= 1 if __name__ == '__main__': c = consumer() # c产生一个生成器(带yield语句) c.send(None) # consumer()程序推进到yield，但yield还未被执行.send()是传值给生成器的语句 p = producer(c) # p也产生一个生成器，但传入c生成器，与p进行通信 for status in p:# 循环获取p生成器yield回来的状态 if status == False: print("我只要3,4,5就行啦") break print("程序结束") 上面这个例子是典型的生产者－消费者问题，我们用协程的方式来实现它。 第一句c = consumer()，因为consumer函数中存在yield语句，python会把它当成一个generator，因此在运行这条语句后，python并不会像执行函数一样，而是返回了一个generator object。 第二条语句c.send(None)，这条语句的作用是将consumer（即变量c，它是一个generator）中的语句推进到第一个yield语句出现的位置，那么在例子中，consumer中的status = True和while True:都已经被执行了，程序停留在n = yield status的位置（注意：此时这条语句还没有被执行），上面说的send(None)语句十分重要，如果漏写这一句，那么程序直接报错 第三句p = producer(c)，这里则像上面一样定义了producer的生成器，注意的是这里我们传入了消费者的生成器，来让producer跟consumer通信。 第四句for status in p:，这条语句会循环地运行producer和获取它yield回来的状态。 现在程序流进入了producer里面，我们直接看yield consumer.send(n)，生产者调用了消费者的send()方法，把n发送给consumer（即c），在consumer中的n = yield status，n拿到的是消费者发送的数字，同时，consumer用yield的方式把状态（status）返回给消费者，注意：这时producer（即消费者）的consumer.send()调用返回的就是consumer中yield的status！消费者马上将status返回给调度它的主程序，主程序获取状态，判断是否错误，若错误，则终止循环，结束程序。上面看起来有点绕，其实这里面generator.send(n)的作用是：把n发送generator(生成器)中yield的赋值语句中，同时返回generator中yield的变量（结果）。 于是程序便一直运作，直至consumer中获取的n的值变为3！此时consumer把status变为False，最后返回到主程序，主程序中断循环，程序结束。 Coroutine与Generator有些人会把生成器（generator）和协程（coroutine）的概念混淆，我以前也会这样，不过其实发现，两者的区别还是很大的。 直接上最重要的区别： generator总是生成值，一般是迭代的序列 coroutine关注的是消耗值，是数据(data)的消费者 coroutine不会与迭代操作关联，而generator会 coroutine强调协同控制程序流，generator强调保存状态和产生数据 相似的是，它们都是不用return来实现重复调用的函数/对象，都用到了yield(中断/恢复)的方式来实现 asyncioasyncio是Python 3.4版本引入的标准库，直接内置了对异步IO的支持。 asyncio的编程模型就是一个消息循环。我们从asyncio模块中直接获取一个EventLoop的引用，然后把需要执行的协程扔到EventLoop中执行，就实现了异步IO。用asyncio实现Hello world代码如下： 123456789101112131415import asyncio#@asyncio.coroutine把一个generator标记为coroutine类型，然后，我们就把这个coroutine扔到EventLoop中执行。@asyncio.coroutinedef hello(): print("Hello world!") # 异步调用asyncio.sleep(1): r = yield from asyncio.sleep(1) print("Hello again!")# 获取EventLoop:loop = asyncio.get_event_loop()# 执行coroutineloop.run_until_complete(hello()) # 循环执行EventLoop里要完成的事件loop.close() hello()会首先打印出Hello world!，然后，yield from语法可以让我们方便地调用另一个generator。由于asyncio.sleep()也是一个coroutine，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()返回时，线程就可以从yield from拿到返回值（此处是None），然后接着执行下一行语句。把asyncio.sleep(1)看成是一个耗时1秒的IO操作，在此期间，主线程并未等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。 我们用Task封装两个coroutine试试： 12345678910111213import threadingimport asyncio@asyncio.coroutinedef hello(): print('Hello world! (%s)' % threading.currentThread()) yield from asyncio.sleep(1) print('Hello again! (%s)' % threading.currentThread())loop = asyncio.get_event_loop()tasks = [hello(), hello()]loop.run_until_complete(asyncio.wait(tasks))loop.close() 观察执行过程： 12345Hello world! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)Hello world! (&lt;_MainThread(MainThread, started 140735195337472)&gt;) # asyncio.sleep(1)会挂起并去执行下一个hello()协程(暂停约1秒)Hello again! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)Hello again! (&lt;_MainThread(MainThread, started 140735195337472)&gt;) 总结： asyncio提供了完善的异步IO支持； 异步操作需要在coroutine中通过yield from`完成； 多个coroutine可以封装成一组Task然后并发执行 async/await用asyncio提供的@asyncio.coroutine可以把一个generator标记为coroutine类型，然后在coroutine内部用yield from调用另一个coroutine实现异步操作。 为了简化并更好地标识异步IO，从Python 3.5开始引入了新的语法async和await，可以让coroutine的代码更简洁易读。 请注意，async和await是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换： 把@asyncio.coroutine替换为async； 把yield from替换为await。 1234567891011121314151617import asyncioasync def compute(x, y): print("Compute %s + %s ..." % (x, y)) await asyncio.sleep(1.0) return x + yasync def print_sum(x, y): result = await compute(x, y) print("%s + %s = %s" % (x, y, result))loop = asyncio.get_event_loop()loop = asyncio.get_event_loop()loop.run_until_complete(print_sum(1, 2))# tasks = [print_sum(1, 2), print_sum(3, 4)]# loop.run_until_complete(asyncio.wait(tasks))loop.close() 当事件循环开始运行时，它会在Task中寻找coroutine来执行调度，因为事件循环注册了print_sum()，因此print_sum()被调用，执行result = await compute(x, y)这条语句（等同于result = yield from compute(x, y)），因为compute()自身就是一个coroutine，因此print_sum()这个协程就会暂时被挂起，compute()被加入到事件循环中，程序流执行compute()中的print语句，打印”Compute %s + %s …”，然后执行了await asyncio.sleep(1.0)，因为asyncio.sleep()也是一个coroutine，接着compute()就会被挂起，等待计时器读秒，在这1秒的过程中，事件循环会在队列中查询可以被调度的coroutine，而因为此前print_sum()与compute()都被挂起了，因此事件循环会停下来等待协程的调度(如果有其他协程task就会在等待时间内去执行并返回)，当计时器读秒结束后，程序流便会返回到compute()中执行return语句，结果会返回到print_sum()中的result中，最后打印result，事件队列中没有可以调度的任务了，此时loop.close()把事件队列关闭，程序结束。]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（11）：使用C扩展]]></title>
    <url>%2F2018%2F07%2F01%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%8811%EF%BC%89%EF%BC%9A%E4%BD%BF%E7%94%A8C%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[使用C扩展CPython还为开发者实现了一个有趣的特性，使用Python可以轻松调用C代码 开发者有三种方法可以在自己的Python代码中来调用C编写的函数-ctypes，SWIG，Python/C API。每种方式也都有各自的利弊。 首先，我们要明确为什么要在Python中调用C？ 常见原因如下： 你要提升代码的运行速度，而且你知道C要比Python快50倍以上 C语言中有很多传统类库，而且有些正是你想要的，但你又不想用Python去重写它们 想对从内存到文件接口这样的底层资源进行访问 不需要理由，就是想这样做 CTypesPython中的ctypes模块可能是Python调用C方法中最简单的一种。ctypes模块提供了和C语言兼容的数据类型和函数来加载dll文件，因此在调用时不需对源文件做任何的修改。也正是如此奠定了这种方法的简单性。 示例如下 实现两数求和的C代码，保存为add.c 12345678910111213141516//sample C file to add 2 numbers - int and floats#include &lt;stdio.h&gt;int add_int(int, int);float add_float(float, float);int add_int(int num1, int num2)&#123; return num1 + num2;&#125;float add_float(float num1, float num2)&#123; return num1 + num2;&#125; 接下来将C文件编译为.so文件(windows下为DLL)。下面操作会生成adder.so文件 12345#For Linux$ gcc -shared -Wl,-soname,adder -o adder.so -fPIC add.c#For Mac$ gcc -shared -Wl,-install_name,adder.so -o adder.so -fPIC add.c 现在在你的Python代码中来调用它 12345678910111213141516from ctypes import *#load the shared object fileadder = CDLL('./adder.so')#Find sum of integersres_int = adder.add_int(4,5)print "Sum of 4 and 5 = " + str(res_int)#Find sum of floatsa = c_float(5.5)b = c_float(4.1)add_float = adder.add_floatadd_float.restype = c_floatprint "Sum of 5.5 and 4.1 = ", str(add_float(a, b)) 输出如下 12Sum of 4 and 5 = 9Sum of 5.5 and 4.1 = 9.60000038147 在这个例子中，C文件是自解释的，它包含两个函数，分别实现了整形求和和浮点型求和。 在Python文件中，一开始先导入ctypes模块，然后使用CDLL函数来加载我们创建的库文件。这样我们就可以通过变量adder来使用C类库中的函数了。当adder.add_int()被调用时，内部将发起一个对C函数add_int的调用。ctypes接口允许我们在调用C函数时使用原生Python中默认的字符串型和整型。 而对于其他类似布尔型和浮点型这样的类型，必须要使用正确的ctype类型才可以。如向adder.add_float()函数传参时, 我们要先将Python中的十进制值转化为c_float类型，然后才能传送给C函数。这种方法虽然简单，清晰，但是却很受限。例如，并不能在C中对对象进行操作。 SWIGSWIG是Simplified Wrapper and Interface Generator的缩写。是Python中调用C代码的另一种方法。在这个方法中，开发人员必须编写一个额外的接口文件来作为SWIG(终端工具)的入口。 Python开发者一般不会采用这种方法，因为大多数情况它会带来不必要的复杂。而当你有一个C/C++代码库需要被多种语言调用时，这将是个非常不错的选择。 示例如下(来自SWIG官网) 12345678910111213141516171819202122​```C#include &lt;time.h&gt;double My_variable = 3.0;int fact(int n) &#123; if (n &lt;= 1) return 1; else return n*fact(n-1);&#125;int my_mod(int x, int y) &#123; return (x%y);&#125;char *get_time()&#123; time_t ltime; time(&amp;ltime); return ctime(&amp;ltime);&#125; 编译它 1234unix % swig -python example.iunix % gcc -c example.c example_wrap.c \ -I/usr/local/include/python2.1unix % ld -shared example.o example_wrap.o -o _example.so 最后，Python的输出 12345678&gt;&gt;&gt; import example&gt;&gt;&gt; example.fact(5)120&gt;&gt;&gt; example.my_mod(7,3)1&gt;&gt;&gt; example.get_time()'Sun Feb 11 23:01:07 1996'&gt;&gt;&gt; 我们可以看到，使用SWIG确实达到了同样的效果，虽然下了更多的工夫，但如果你的目标是多语言还是很值得的。 Python/C APIPython/C API可能是被最广泛使用的方法。它不仅简单，而且可以在C代码中操作你的Python对象。 这种方法需要以特定的方式来编写C代码以供Python去调用它。所有的Python对象都被表示为一种叫做PyObject的结构体，并且Python.h头文件中提供了各种操作它的函数。例如，如果PyObject表示为PyListType(列表类型)时，那么我们便可以使用PyList_Size()函数来获取该结构的长度，类似Python中的len(list)函数。大部分对Python原生对象的基础函数和操作在Python.h头文件中都能找到。 示例 编写一个C扩展，添加所有元素到一个Python列表(所有元素都是数字) 来看一下我们要实现的效果，这里演示了用Python调用C扩展的代码 12345#Though it looks like an ordinary python import, the addList module is implemented in Cimport addListl = [1,2,3,4,5]print "Sum of List - " + str(l) + " = " + str(addList.add(l)) 上面的代码和普通的Python文件并没有什么分别，导入并使用了另一个叫做addList的Python模块。唯一差别就是这个模块(addList)并不是用Python编写的，而是C。 接下来我们看看如何用C编写addList模块，这可能看起来有点让人难以接受，但是一旦你了解了这之中的各种组成，你就可以一往无前了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455//Python.h has all the required function definitions to manipulate the Python objects#include &lt;Python.h&gt;//This is the function that is called from your python codestatic PyObject* addList_add(PyObject* self, PyObject* args)&#123; PyObject * listObj; //The input arguments come as a tuple, we parse the args to get the various variables //In this case it's only one list variable, which will now be referenced by listObj if (! PyArg_ParseTuple( args, "O", &amp;listObj )) return NULL; //length of the list long length = PyList_Size(listObj); //iterate over all the elements int i, sum =0; for (i = 0; i &lt; length; i++) &#123; //get an element out of the list - the element is also a python objects PyObject* temp = PyList_GetItem(listObj, i); //we know that object represents an integer - so convert it into C long long elem = PyInt_AsLong(temp); sum += elem; &#125; //value returned back to python code - another python object //build value here converts the C long to a python integer return Py_BuildValue("i", sum);&#125;//This is the docstring that corresponds to our 'add' function.static char addList_docs[] ="add( ): add all elements of the list\n";/* This table contains the relavent info mapping - &lt;function-name in python module&gt;, &lt;actual-function&gt;, &lt;type-of-args the function expects&gt;, &lt;docstring associated with the function&gt; */static PyMethodDef addList_funcs[] = &#123; &#123;"add", (PyCFunction)addList_add, METH_VARARGS, addList_docs&#125;, &#123;NULL, NULL, 0, NULL&#125;&#125;;/* addList is the module name, and this is the initialization block of the module. &lt;desired module name&gt;, &lt;the-info-table&gt;, &lt;module's-docstring&gt; */PyMODINIT_FUNC initaddList(void)&#123; Py_InitModule3("addList", addList_funcs, "Add all ze lists");&#125; 逐步解释 Python.h头文件中包含了所有需要的类型(Python对象类型的表示)和函数定义(对Python对象的操作) 接下来我们编写将要在Python调用的函数, 函数传统的命名方式由{模块名}_{函数名}组成，所以我们将其命名为addList_add 然后填写想在模块内实现函数的相关信息表，每行一个函数，以空行作为结束 最后的模块初始化块签名为PyMODINIT_FUNC init{模块名}。 函数addList_add接受的参数类型为PyObject类型结构(同时也表示为元组类型，因为Python中万物皆为对象，所以我们先用PyObject来定义)。传入的参数则通过PyArg_ParseTuple()来解析。第一个参数是被解析的参数变量。第二个参数是一个字符串，告诉我们如何去解析元组中每一个元素。字符串的第n个字母正是代表着元组中第n个参数的类型。例如，”i”代表整形，”s”代表字符串类型, “O”则代表一个Python对象。接下来的参数都是你想要通过PyArg_ParseTuple()函数解析并保存的元素。这样参数的数量和模块中函数期待得到的参数数量就可以保持一致，并保证了位置的完整性。例如，我们想传入一个字符串，一个整数和一个Python列表，可以这样去写 1234int n;char *s;PyObject* list;PyArg_ParseTuple(args, "siO", &amp;n, &amp;s, &amp;list); 在这种情况下，我们只需要提取一个列表对象，并将它存储在listObj变量中。然后用列表对象中的PyList_Size()函数来获取它的长度。就像Python中调用len(list)。 现在我们通过循环列表，使用PyList_GetItem(list, index)函数来获取每个元素。这将返回一个PyObject*对象。既然Python对象也能表示PyIntType，我们只要使用PyInt_AsLong(PyObj *)函数便可获得我们所需要的值。我们对每个元素都这样处理，最后再得到它们的总和。 总和将被转化为一个Python对象并通过Py_BuildValue()返回给Python代码，这里的i表示我们要返回一个Python整形对象。 现在我们已经编写完C模块了。将下列代码保存为setup.py 123456#build the modulesfrom distutils.core import setup, Extensionsetup(name='addList', version='1.0', \ ext_modules=[Extension('addList', ['adder.c'])]) 并且运行 1python setup.py install 现在应该已经将我们的C文件编译安装到我们的Python模块中了。 在一番辛苦后，让我们来验证下我们的模块是否有效 12345#module that talks to the C codeimport addListl = [1,2,3,4,5]print "Sum of List - " + str(l) + " = " + str(addList.add(l)) 输出结果如下 1Sum of List - [1, 2, 3, 4, 5] = 15 如你所见，我们已经使用Python.h API成功开发出了我们第一个Python C扩展。这种方法看似复杂，但你一旦习惯，它将变的非常有效。 Python调用C代码的另一种方式便是使用Cython让Python编译的更快。但是Cython和传统的Python比起来可以将它理解为另一种语言，所以我们就不在这里过多描述了。 补充两个知识点列表辗平可以通过使用itertools包中的itertools.chain.from_iterable轻松快速的辗平一个列表。下面是一个简单的例子： 1234567a_list = [[1, 2], [3, 4], [5, 6]]print(list(itertools.chain.from_iterable(a_list)))# Output: [1, 2, 3, 4, 5, 6]# orprint(list(itertools.chain(*a_list)))# Output: [1, 2, 3, 4, 5, 6] for-else从句for循环还有一个else从句，我们大多数人并不熟悉。这个else从句会在循环正常结束时执行。这意味着，循环没有遇到任何break。若循环被某些因素打破，则不会执行else语句. 一旦你掌握了何时何地使用它，它真的会非常有用。我自己对它真是相见恨晚。 有个常见的构造是跑一个循环，并查找一个元素。如果这个元素被找到了，我们使用break来中断这个循环。有两个场景会让循环停下来。 第一个是当一个元素被找到，break被触发。 第二个场景是循环结束。 现在我们也许想知道其中哪一个，才是导致循环完成的原因。一个方法是先设置一个标记，然后在循环结束时打上标记。另一个是使用else从句。 这就是for/else循环的基本结构： 12345678for item in container: if search_something(item): # Found it! process(item) breakelse: # Didn't find anything.. not_found_in_container() 考虑下这个简单的案例 12345for n in range(2, 10): for x in range(2, n): if n % x == 0: print(n, 'equals', x, '*', n / x) break 它会找出2到10之间的数字的因子。现在是趣味环节了。我们可以加上一个附加的else语句块，来抓住质数，并且告诉我们： 12345678for n in range(2, 10): for x in range(2, n): if n % x == 0: print(n, 'equals', x, '*', n / x) break else: # 输出没有循环结束仍未找到因子的质数 # loop fell through without finding a factor print(n, 'is a prime number')]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（10）：sort、lambda]]></title>
    <url>%2F2018%2F06%2F30%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%8810%EF%BC%89%EF%BC%9Asort%E3%80%81lambda%2F</url>
    <content type="text"><![CDATA[sort与sorted区别我们需要对List进行排序，Python提供了两个方法对给定的List L进行排序， 方法1.用List的成员函数sort进行排序 方法2.用built-in函数sorted进行排序 list.sort()与sorted()的不同在于，list.sort是在原位重新排列列表，而sorted()是产生一个新的列表。python中列表的内置函数list.sort（）只可以对列表中的元素进行排序，而全局性的sorted（）函数则对所有可迭代的对象都是适用的；并且list.sort（）函数是内置函数，会改变当前对象，而sorted（）函数只会返回一个排序后的当前对象的副本，而不会改变当前对象。 原型：sort（fun，key，reverse=False） sorted(itrearble, cmp=None, key=None,reverse=False) 内置函数sort（）参数fun是表明此sort函数是基于何种算法进行排序的，一般默认情况下python中用的是归并排序，并且一般情况下我们是不会重写此参数的，所以基本可以忽略； 参数key用来指定一个函数，此函数在每次元素比较时被调用，此函数代表排序的规则，也就是你按照什么规则对你的序列进行排序； 参数reverse是用来表明是否逆序，默认的False情况下是按照升序的规则进行排序的，当reverse=True时，便会按照降序进行排序。 1234567891011121314151617181920212223242526#coding:utf-8from operator import attrgetter,itemgetter list1 = [(2,'huan',23),(12,'the',14),(23,'liu',90)] #使用默认参数进行排序，即按照元组中第一个元素进行排序list1.sort()print list1#输出结果为[(2, 'huan', 23), (12, 'the', 14), (23, 'liu', 90)] #使用匿名表达式重写key所代表的函数,按照元组的第二个元素（下标为1）进行排序list1.sort(key=lambda x:(x[1]))print list1#[(2, 'huan', 23), (23, 'liu', 90), (12, 'the', 14)]#使用匿名函数重写key所代表的函数，先按照元组中下标为2的进行排序，# 对于下标2处元素相同的，则按下标为0处的元素进行排序list1.sort(key=lambda x:(x[2],x[0]))print list1#[(12, 'the', 14), (2, 'huan', 23), (23, 'liu', 90)]#使用operator模块中的itemgetter函数进行重写key所代表的函数，按照下标为1处的元素(第二个)进行排序list1.sort(key=itemgetter(1))print list1#[(2, 'huan', 23), (23, 'liu', 90), (12, 'the', 14)] 全局函数sorted（）对于sorted（）函数中key的重写，和sort（）函数中是一样的，所以刚刚对于sort（）中讲解的方法，都是适用于sorted（）函数中。sorted（）最后会将排序的结果放到一个新的列表中，而不是对iterable本身进行修改。 1234567891011121314151617sorted('123456') # 字符串['1', '2', '3', '4', '5', '6']sorted([1,4,5,2,3,6]) # 列表[1, 2, 3, 4, 5, 6]sorted(&#123;1:'q',3:'c',2:'g'&#125;) # 字典， 默认对字典的键进行排序[1, 2, 3]sorted(&#123;1:'q',3:'c',2:'g'&#125;.keys()) # 对字典的键[1, 2, 3]sorted(&#123;1:'q',3:'c',2:'g'&#125;.values()) # 对字典的值['c', 'g', 'q']sorted(&#123;1:'q',3:'c',2:'g'&#125;.items()) # 对键值对组成的元组的列表[(1, 'q'), (2, 'g'), (3, 'c')] 对元素指定的某一部分进行排序,关键字排序 1234567891011# 想要按照-后的数字的大小升序排序。要用到keys =['Chr1-10.txt','Chr1-1.txt','Chr1-2.txt','Chr1-14.txt','Chr1-3.txt','Chr1-20.txt','Chr1-5.txt']sorted(s, key=lambda d :int(d.split('-')[-1].split('.')[0]))# 输出 ['Chr1-1.txt', 'Chr1-2.txt', 'Chr1-3.txt','Chr1-5.txt', 'Chr1-10.txt', 'Chr1-14.txt', 'Chr1-20.txt']# 这就是key的功能，制定排序的关键字，通常都是一个lambda函数，当然你也可以事先定义好这个函数。如果不讲这个关键字转化为整型，结果是这样的：sorted(s, key=lambda d : d.split('-')[-1].split('.')[0])# 输出 ['Chr1-1.txt', 'Chr1-10.txt','Chr1-14.txt', 'Chr1-2.txt', 'Chr1-20.txt', 'Chr1-3.txt', 'Chr1-5.txt'] 这相当于把这个关键字当做字符串了，很显然，在python中，’2’ &gt; ‘10’。cmp不怎么用，因为key和reverse比单独一个cmp效率要高。 lambda的各种用法1， 用在过滤函数中，指定过滤列表元素的条件： 12filter(lambda x: x % 3 == 0, [1, 2, 3, 4, 5, 6, 7, 8, 9]) &gt; [3, 6, 9] 2， 用在排序函数中，指定对列表中所有元素进行排序的准则： 12sorted([1, 2, 3, 4, 5, 6, 7, 8, 9], key=lambda x: abs(5-x))&gt; [5, 4, 6, 3, 7, 2, 8, 1, 9] 3， 用在reduce函数中，指定列表中两两相邻元素的结合条件 12reduce(lambda a, b: '&#123;&#125;, &#123;&#125;'.format(a, b), [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt; '1, 2, 3, 4, 5, 6, 7, 8, 9' 4， 用在map函数中，指定对列表中每一个元素的共同操作 12map(lambda x: x+1, [1, 2,3])&gt; [2, 3, 4] 5， 从另一函数中返回一个函数，常用来实现函数装饰器(Wrapper)，例如python的function decorators 12345def transform(n): return lambda x: x + nf = transform(3)print f(3)&gt; 7 6，列表排序 12345a = [(1, 2), (4, 1), (9, 10), (13, -3)]a.sort(key=lambda x: x[1])print(a)# Output: [(13, -3), (4, 1), (1, 2), (9, 10)] 7，列表并行排序 1234data = zip(list1, list2)data = sorted(data)list1, list2 = map(lambda t: list(t), zip(*data))# zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（9）：枚举、自省、推导式]]></title>
    <url>%2F2018%2F06%2F30%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%889%EF%BC%89%EF%BC%9A%E6%9E%9A%E4%B8%BE%E3%80%81%E8%87%AA%E7%9C%81%E3%80%81%E6%8E%A8%E5%AF%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[枚举枚举(enumerate)是Python内置函数。它的用处很难在简单的一行中说明，但是大多数的新人，甚至一些高级程序员都没有意识到它。 它允许我们遍历数据并自动计数， 下面是一个例子： 12for counter, value in enumerate(some_list): print(counter, value) 不只如此，enumerate也接受一些可选参数，这使它更有用。 123456789my_list = ['apple', 'banana', 'grapes', 'pear']for c, value in enumerate(my_list, 1): print(c, value)# 输出:(1, 'apple')(2, 'banana')(3, 'grapes')(4, 'pear') 上面这个可选参数允许我们定制从哪个数字开始枚举。你还可以用来创建包含索引的元组列表， 例如： 1234my_list = ['apple', 'banana', 'grapes', 'pear']counter_list = list(enumerate(my_list, 1))print(counter_list)# 输出: [(1, 'apple'), (2, 'banana'), (3, 'grapes'), (4, 'pear')] 对象自省自省(introspection)，在计算机编程领域里，是指在运行时来判断一个对象的类型的能力。它是Python的强项之一。Python中所有一切都是一个对象，而且我们可以仔细勘察那些对象。Python还包含了许多内置函数和模块来帮助我们。 dir在这个小节里我们会学习到dir以及它在自省方面如何给我们提供便利。 它是用于自省的最重要的函数之一。它返回一个列表，列出了一个对象所拥有的属性和方法。这里是一个例子： 12345678910my_list = [1, 2, 3]dir(my_list)# Output: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',# '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',# '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',# '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',# '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',# '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',# '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',# 'remove', 'reverse', 'sort'] 上面的自省给了我们一个列表对象的所有方法的名字。当你没法回忆起一个方法的名字，这会非常有帮助。如果我们运行dir()而不传入参数，那么它会返回当前作用域的所有名字。 type和idtype函数返回一个对象的类型。举个例子： 1234567891011121314print(type(''))# Output: &lt;type 'str'&gt;print(type([]))# Output: &lt;type 'list'&gt;print(type(&#123;&#125;))# Output: &lt;type 'dict'&gt;print(type(dict))# Output: &lt;type 'type'&gt;print(type(3))# Output: &lt;type 'int'&gt; id()函数返回任意不同种类对象的唯一ID内存地址，举个例子： 123name = "Yasoob"print(id(name))# Output: 139972439030304 inspect模块inspect模块也提供了许多有用的函数，来获取活跃对象的信息。比方说，你可以查看一个对象的成员，只需运行： 123import inspectprint(inspect.getmembers(str))# Output: [('__add__', &lt;slot wrapper '__add__' of ... ... 还有好多个其他方法也能有助于自省。如果你愿意，你可以去探索它们。 inspect.ismodule(object)： 是否为模块inspect.isclass(object)：是否为类inspect.ismethod(object)：是否为方法（bound method written in python）inspect.isfunction(object)：是否为函数(python function, including lambda expression)inspect.isgeneratorfunction(object)：是否为python生成器函数inspect.isgenerator(object):是否为生成器inspect.istraceback(object)： 是否为tracebackinspect.isframe(object)：是否为frameinspect.iscode(object)：是否为codeinspect.isbuiltin(object)：是否为built-in函数或built-in方法inspect.isroutine(object)：是否为用户自定义或者built-in函数或方法inspect.isabstract(object)：是否为抽象基类inspect.ismethoddescriptor(object)：是否为方法标识符inspect.isdatadescriptor(object)：是否为数字标识符，数字标识符有__get__ 和__set__属性； 通常也有__name__和__doc__属性inspect.isgetsetdescriptor(object)：是否为getset descriptorinspect.ismemberdescriptor(object)：是否为member descriptor 各种推导式(comprehensions)推导式（又称解析式）是Python的一种独有特性，如果我被迫离开了它，我会非常想念。推导式是可以从一个数据序列构建另一个新的数据序列的结构体。 共有三种推导，在Python2和3中都有支持： 列表(list)推导式 字典(dict)推导式 集合(set)推导式 我们将一一进行讨论。一旦你知道了使用列表推导式的诀窍，你就能轻易使用任意一种推导式了。 列表推导式（list comprehensions）列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表。它的结构是在一个中括号里包含一个表达式，然后是一个for语句，然后是0个或多个for或者if语句。那个表达式可以是任意的，意思是你可以在列表中放入任意类型的对象。返回结果将是一个新的列表，在这个以if和for语句为上下文的表达式运行完成之后产生。 规范1variable = [out_exp for out_exp in input_list if out_exp == 2] 字典推导式（dict comprehensions）字典推导和列表推导的使用方法是类似的,只不中括号该改成大括号，毕竟字典本身用的就是大括号。这里有个我最近发现的例子： 12345678mcase = &#123;'a': 10, 'b': 34, 'A': 7, 'Z': 3&#125;mcase_frequency = &#123; k.lower(): mcase.get(k.lower(), 0) + mcase.get(k.upper(), 0) # 执行函数，k为每个字典的关键字 for k in mcase.keys()&#125;# mcase_frequency == &#123;'a': 17, 'z': 3, 'b': 34&#125; 在上面的例子中我们把同一个字母但不同大小写的值合并起来了。 就我个人来说没有大量使用字典推导式。 你还可以快速对换一个字典的键和值： 1&#123;v: k for k, v in some_dict.items()&#125; 集合推导式（set comprehensions）集合推导式跟列表推导式差不多，都是对一个列表的元素全部执行相同的操作，但集合是一种无重复无序的序列区别：跟列表推到式的区别在于：1.不使用中括号，使用大括号；2.结果中无重复；3.结果是一个set()集合，集合里面是一个序列： 123squared = &#123;x**2 for x in [1, 1, 2]&#125;print(squared)# Output: &#123;1, 4&#125;]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（8）：容器Collections]]></title>
    <url>%2F2018%2F06%2F30%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%888%EF%BC%89%EF%BC%9A%E5%AE%B9%E5%99%A8Collections%2F</url>
    <content type="text"><![CDATA[容器(Collections)Python附带一个模块，它包含许多容器数据类型，名字叫作collections。我们将讨论它的作用和用法。 我们将讨论的是： defaultdict counter deque namedtuple enum.Enum (包含在Python 3.4以上) defaultdict众所周知，在Python中如果访问字典中不存在的键，会引发KeyError异常（JavaScript中如果对象中不存在某个属性，则返回undefined）。但是有时候，字典中的每个键都存在默认值是非常方便的。例如下面的例子： 123456strings = ('puppy', 'kitten', 'puppy', 'puppy', 'weasel', 'puppy', 'kitten', 'puppy')counts = &#123;&#125;for kw in strings: counts[kw] += 1 # 第一次统计时没有键对应的默认值 该例子统计strings中某个单词出现的次数，并在counts字典中作记录。单词每出现一次，在counts相对应的键所存的值数字加1。但是事实上，运行这段代码会抛出KeyError异常，出现的时机是每个单词第一次统计的时候，因为Python的dict中不存在默认值的说法，可以在Python命令行中验证： 1234567&gt;&gt;&gt; counts = dict()&gt;&gt;&gt; counts&#123;&#125;&gt;&gt;&gt; counts['puppy'] += 1Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;KeyError: 'puppy' 使用判断语句检查既然如此，首先可能想到的方法是在单词第一次统计的时候，在counts中相应的键存下默认值1。这需要在处理的时候添加一个判断语句： 123456789101112strings = ('puppy', 'kitten', 'puppy', 'puppy', 'weasel', 'puppy', 'kitten', 'puppy')counts = &#123;&#125;for kw in strings: if kw not in counts: counts[kw] = 1 else: counts[kw] += 1# counts:# &#123;'puppy': 5, 'weasel': 1, 'kitten': 2&#125; 使用dict.setdefault()方法也可以通过dict.setdefault()方法来设置默认值： 1234567strings = ('puppy', 'kitten', 'puppy', 'puppy', 'weasel', 'puppy', 'kitten', 'puppy')counts = &#123;&#125;for kw in strings: counts.setdefault(kw, 0) counts[kw] += 1 dict.setdefault()方法接收两个参数，第一个参数是健的名称，第二个参数是默认值。假如字典中不存在给定的键，则返回参数中提供的默认值；反之，则返回字典中保存的值。利用dict.setdefault()方法的返回值可以重写for循环中的代码，使其更加简洁： 123456strings = ('puppy', 'kitten', 'puppy', 'puppy', 'weasel', 'puppy', 'kitten', 'puppy')counts = &#123;&#125;for kw in strings: counts[kw] = counts.setdefault(kw, 0) + 1 使用collections.defaultdict类以上的方法虽然在一定程度上解决了dict中不存在默认值的问题，但是这时候我们会想，有没有一种字典它本身提供了默认值的功能呢？答案是肯定的，那就是collections.defaultdict。 defaultdict类就好像是一个dict，但是它是使用一个类型来初始化的： 1234&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(list) # 接受一个list类型作为初始化参数&gt;&gt;&gt; dddefaultdict(&lt;type 'list'&gt;, &#123;&#125;) defaultdict类的初始化函数接受一个类型作为参数，当所访问的键不存在的时候，可以实例化一个值作为默认值： 1234567&gt;&gt;&gt; dd['foo'][]&gt;&gt;&gt; dddefaultdict(&lt;type 'list'&gt;, &#123;'foo': []&#125;)&gt;&gt;&gt; dd['bar'].append('quux')&gt;&gt;&gt; dddefaultdict(&lt;type 'list'&gt;, &#123;'foo': [], 'bar': ['quux']&#125;) 需要注意的是，这种形式的默认值只有在通过dict[key]或者dict.__getitem__(key)访问的时候才有效，这其中的原因在下文会介绍。 1234567891011&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(list)&gt;&gt;&gt; 'something' in ddFalse&gt;&gt;&gt; dd.pop('something')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;KeyError: 'pop(): dictionary is empty'&gt;&gt;&gt; dd.get('something')&gt;&gt;&gt; dd['something'][] 该类除了接受类型名称作为初始化函数的参数之外，还可以使用任何不带参数的可调用函数，到时该函数的返回结果作为默认值，这样使得默认值的取值更加灵活。下面用一个例子来说明，如何用自定义的不带参数的函数zero()作为初始化函数的参数： 1234567891011&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; def zero():... return 0...&gt;&gt;&gt; dd = defaultdict(zero)&gt;&gt;&gt; dddefaultdict(&lt;function zero at 0xb7ed2684&gt;, &#123;&#125;)&gt;&gt;&gt; dd['foo']0&gt;&gt;&gt; dddefaultdict(&lt;function zero at 0xb7ed2684&gt;, &#123;'foo': 0&#125;) 利用collections.defaultdict来解决最初的单词统计问题，代码如下： 12345678from collections import defaultdictstrings = ('puppy', 'kitten', 'puppy', 'puppy', 'weasel', 'puppy', 'kitten', 'puppy')counts = defaultdict(lambda: 0) # 使用lambda来定义简单的函数for s in strings: counts[s] += 1 defaultdict 类是如何实现的通过上面的内容，想必大家已经了解了defaultdict类的用法，那么在defaultdict类中又是如何来实现默认值的功能呢？这其中的关键是使用了看__missing__()这个方法： 123456&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; print defaultdict.__missing__.__doc____missing__(key) # Called by __getitem__ for missing key; pseudo-code: if self.default_factory is None: raise KeyError(key) self[key] = value = self.default_factory() return value 通过查看__missing__()方法的docstring，可以看出当使用__getitem__()方法访问一个不存在的键时(dict[key]这种形式实际上是__getitem__()方法的简化形式)，会调用__missing__()方法获取默认值，并将该键添加到字典中去。 counterCounter是一个计数器，它可以帮助我们针对某项数据进行计数。比如它可以用来计算每个人喜欢多少种颜色： 123456789101112131415161718192021from collections import Countercolours = ( ('Yasoob', 'Yellow'), ('Ali', 'Blue'), ('Arham', 'Green'), ('Ali', 'Black'), ('Yasoob', 'Red'), ('Ahmed', 'Silver'),)favs = Counter(name for name, colour in colours)print(favs)## 输出:## Counter(&#123;## 'Yasoob': 2,## 'Ali': 2,## 'Arham': 1,## 'Ahmed': 1## &#125;) 我们也可以在利用它统计一个文件，例如： 123with open('filename', 'rb') as f: line_count = Counter(f)print(line_count) 还有 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1&#125;) dequedeque提供了一个双端队列，你可以从头/尾两端添加或删除元素。要想使用它，首先我们要从collections中导入deque模块： 1from collections import deque 现在，你可以创建一个deque对象。 1d = deque() 它的用法就像python的list，并且提供了类似的方法，例如： 12345678910111213141516d = deque()d.append('1')d.append('2')d.append('3')print(len(d))## 输出: 3print(d[0])## 输出: '1'print(d[-1])## 输出: '3' 你可以从两端取出(pop)数据： 12345678910111213141516d = deque(range(5))print(len(d))## 输出: 5d.popleft()## 输出: 0d.pop()## 输出: 4print(d)## 输出: deque([1, 2, 3]) 我们也可以限制这个列表的大小，当超出你设定的限制时，数据会从对队列另一端被挤出去(pop)。最好的解释是给出一个例子： 1d = deque(maxlen=30) 现在当你插入30条数据时，最左边一端的数据将从队列中删除。 你还可以从任一端扩展这个队列中的数据： 123456d = deque([1,2,3,4,5])d.extendleft([0])d.extend([6,7,8])print(d)## 输出: deque([0, 1, 2, 3, 4, 5, 6, 7, 8]) namedtuple您可能已经熟悉元组。一个元组是一个不可变的列表，你可以存储一个数据的序列，它和命名元组(namedtuples)非常像，但有几个关键的不同。主要相似点是都不像列表，你不能修改元组中的数据。为了获取元组中的数据，你需要使用整数作为索引： 1234man = ('Ali', 30)print(man[0])## 输出: Ali 嗯，那namedtuples是什么呢？它把元组变成一个针对简单任务的容器。你不必使用整数索引来访问一个namedtuples的数据。你可以像字典(dict)一样访问namedtuples，但namedtuples是不可变的。 123456789101112from collections import namedtupleAnimal = namedtuple('Animal', 'name age type')perry = Animal(name="perry", age=31, type="cat")print(perry)## 输出: Animal(name='perry', age=31, type='cat')print(perry.name)## 输出: 'perry' 现在你可以看到，我们可以用名字来访问namedtuple中的数据。我们再继续分析它。一个命名元组(namedtuple)有两个必需的参数。它们是元组名称和字段名称。 在上面的例子中，我们的元组名称是Animal，字段名称是’name’，’age’和’type’。namedtuple让你的元组变得自文档了。你只要看一眼就很容易理解代码是做什么的。你也不必使用整数索引来访问一个命名元组，这让你的代码更易于维护。而且，namedtuple的每个实例没有对象字典，所以它们很轻量，与普通的元组比，并不需要更多的内存。这使得它们比字典更快。 然而，要记住它是一个元组，属性值在namedtuple中是不可变的，所以下面的代码不能工作： 12345678910from collections import namedtupleAnimal = namedtuple('Animal', 'name age type')perry = Animal(name="perry", age=31, type="cat")perry.age = 42## 输出:## Traceback (most recent call last):## File "", line 1, in## AttributeError: can't set attribute 你应该使用命名元组来让代码自文档，它们向后兼容于普通的元组，这意味着你可以既使用整数索引，也可以使用名称来访问namedtuple： 1234567from collections import namedtupleAnimal = namedtuple('Animal', 'name age type')perry = Animal(name="perry", age=31, type="cat")print(perry[0])## 输出: perry 最后，你可以将一个命名元组转换为字典，方法如下： 1234567from collections import namedtupleAnimal = namedtuple('Animal', 'name age type')perry = Animal(name="Perry", age=31, type="cat")print(perry._asdict())## 输出: OrderedDict([('name', 'Perry'), ('age', 31), ... enum.Enum (Python 3.4+)另一个有用的容器是枚举对象，它属于enum模块，存在于Python 3.4以上版本中（同时作为一个独立的PyPI包enum34供老版本使用）。Enums(枚举类型)基本上是一种组织各种东西的方式。 让我们回顾一下上一个’Animal’命名元组的例子。它有一个type字段，问题是，type是一个字符串。那么问题来了，万一程序员输入了Cat，因为他按到了Shift键，或者输入了’CAT’，甚至’kitten’？解决的方法是为这样的枚举类型定义一个class类型，然后，每个常量都是class的一个唯一实例。Python提供了Enum类来实现这个功能： 123from enum import EnumMonth = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员： 12for name, member in Month.__members__.items(): print(name, '=&gt;', member, ',', member.value) value属性则是自动赋给成员的int常量，默认从1开始计数。 如果需要更精确地控制枚举类型，可以从Enum派生出自定义类： 12345678910111213141516171819202122232425from collections import namedtuplefrom enum import Enum@unique # @unique装饰器可以帮助我们检查保证没有重复值。class Species(Enum): cat = 1 dog = 2 horse = 3 aardvark = 4 butterfly = 5 owl = 6 platypus = 7 dragon = 8 unicorn = 9 # 依次类推 # 但我们并不想关心同一物种的年龄，所以我们可以使用一个别名 kitten = 1 # (译者注：幼小的猫咪) puppy = 2 # (译者注：幼小的狗狗)Animal = namedtuple('Animal', 'name age type')perry = Animal(name="Perry", age=31, type=Species.cat)drogon = Animal(name="Drogon", age=4, type=Species.dragon)tom = Animal(name="Tom", age=75, type=Species.cat)charlie = Animal(name="Charlie", age=2, type=Species.kitten) 现在，我们进行一些测试： 1234&gt;&gt;&gt; charlie.type == tom.typeTrue&gt;&gt;&gt; charlie.type&lt;Species.cat: 1&gt; 这样就没那么容易错误，我们必须更明确，而且我们应该只使用定义后的枚举类型。 有三种方法访问枚举数据，例如以下方法都可以获取到’cat’的值： 123Species(1)Species['cat']Species.cat 参考资料 【1】：http://kodango.com/understand-defaultdict-in-python]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（7）：可变对象和slots]]></title>
    <url>%2F2018%2F06%2F30%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%887%EF%BC%89%EF%BC%9A%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1%E5%92%8Cslots%2F</url>
    <content type="text"><![CDATA[对象变动(Mutation)当你将一个变量赋值为另一个可变类型的变量时，对这个数据的任意改动会同时反映到这两个变量上去。新变量只不过是老变量的一个别名而已。对象可变与不可变性，是对内存地址而言的。现在讲述的这个情况只是针对可变数据类型。 不可变对象（需要复制到新内存） 常见不可变对象类型：int，string，float，tuple，bool ，frozenset，bytes 1234567891011121314def int_test(): i = 77 j = 77 print(id(77)) #140396579590760 print('i id:' + str(id(i))) #i id:140396579590760 print('j id:' + str(id(j))) #j id:140396579590760 print i is j #True j = j + 1 print('new i id:' + str(id(i))) #new i id:140396579590760 print('new j id:' + str(id(j))) #new j id:140396579590736 print i is j #Falseif __name__ == '__main__': int_test() 首先i和j都指向77这个内存块。然后我们修改j的值，按道理j修改之后应该i的值也发生改变的，因为它们都是指向的同一块内存，但结果是并没有。因为int类型是不可变类型，所有其实是j复制了一份到新的内存地址然后+1，然后j又指向了新的地址。所以j的内存id发生了变化。 内存变化如下： 可变对象（在原内存上修改） 常见可变对象类型：list，dict，set，user-defined classes(unless specifically made immutable) 123456789101112def dict_test(): a = &#123;&#125; b = a print(id(a)) # 140367329543360 a['a'] = 'hhhh' print('id a:' + str(id(a))) # id a:140367329543360 print('a:' + str(a)) # a:&#123;'a': 'hhhh'&#125; print('id b:' + str(id(b))) # id b:140367329543360 print('b:' + str(b)) # b:&#123;'a': 'hhhh'&#125;if __name__ == '__main__': dict_test() 可以看到a最早的内存地址id是140367329543360 然后把a赋值给b其实就是让变量b的也指向a所指向的内存空间。然后我们发现当a发生变化后，b也跟着发生变化了。因为list是可变类型，所以并不会复制一份再改变，而是直接在a所指向的内存空间修改数据，而b也是指向该内存空间的，自然b也就跟着改变了。 对于列表，首地址是不可变的，而对于列表内的所有元素进行修改，会改变单个元素的地址（指向不同的引用）。所以说对于列表中的单个元素而言是不可变的，对于整体列表而言是可变的，如下图所示 python函数的参数传递由于python规定参数传递都是传递引用，也就是传递给函数的是原变量实际所指向的内存空间，修改的时候就会根据该引用的指向去修改该内存中的内容，所以按道理说我们在函数内改变了传递过来的参数的值的话，原来外部的变量也应该受到影响。但是上面我们说到了python中有可变类型和不可变类型，这样的话，当传过来的是可变类型(list,dict)时，我们在函数内部修改就会影响函数外部的变量。而传入的是不可变类型时在函数内部修改改变量并不会影响函数外部的变量，因为修改的时候会先复制一份再修改。下面通过代码证明一下： 1234567891011121314def test(a_int, b_list): a_int = a_int + 1 b_list.append('13') print('inner a_int:' + str(a_int)) print('inner b_list:' + str(b_list))if __name__ == '__main__': a_int = 5 b_list = [10, 11] test(a_int, b_list) print('outer a_int:' + str(a_int)) print('outer b_list:' + str(b_list)) 运行结果如下: 1234567inner a_int:6inner b_list:[10, 11, '13']outer a_int:5outer b_list:[10, 11, '13'] 好啦！答案显而易见啦，经过test()方法修改后，传递过来的int类型外部变量没有发生改变，而list这种可变类型则因为test()方法的影响导致内容发生了改变。 在很多的其他语言中在传递参数的时候允许程序员选择值传递还是引用传递(比如c语言加上号传递指针就是引用传递，而直接传递变量名就是值传递)，*而python只允许使用引用传递，但是它加上了可变类型和不可变类型，听说python只允许引用传递是为方便内存管理，因为python使用的内存回收机制是计数器回收，就是每块内存上有一个计数器，表示当前有多少个对象指向该内存。每当一个变量不再使用时，就让该计数器-1，有新对象指向该内存时就让计数器+1，当计时器为0时，就可以收回这块内存了。 __slots__魔法在Python中，每个类都有实例属性。默认情况下Python用一个字典来保存一个对象的实例属性。这非常有用，因为它允许我们在运行时去设置任意的新属性。 然而，对于有着已知属性的小类来说，它可能是个瓶颈。这个字典浪费了很多内存。Python不能在对象创建时直接分配一个固定量的内存来保存所有的属性。因此如果你创建许多对象（我指的是成千上万个），它会消耗掉很多内存。不过还是有一个方法来规避这个问题。这个方法需要使用__slots__来告诉Python不要使用字典，而且只给一个固定集合的属性分配空间。 这里是一个使用与不使用__slots__的例子： 不使用 __slots__: 123456class MyClass(object): def __init__(self, name, identifier): self.name = name self.identifier = identifier self.set_up() # ... 使用 __slots__: 1234567class MyClass(object): __slots__ = ['name', 'identifier'] def __init__(self, name, identifier): self.name = name self.identifier = identifier self.set_up() # ... 第二段代码会为你的内存减轻负担。通过这个技巧，有些人已经看到内存占用率几乎40%~50%的减少。 稍微备注一下，你也许需要试一下PyPy。它已经默认地做了所有这些优化。 参考资料： 【1】python可变和不可变对象：https://www.jianshu.com/p/c5582e23b26c]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（6）：装饰器]]></title>
    <url>%2F2018%2F06%2F29%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%886%EF%BC%89%EF%BC%9A%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一切皆对象首先我们来理解下Python中的函数 123456789101112131415161718192021def hi(name="yasoob"): return "hi " + nameprint(hi())# output: 'hi yasoob'# 我们甚至可以将一个函数赋值给一个变量，比如greet = hi# 我们这里没有在使用小括号，因为我们并不是在调用hi函数# 而是在将它放在greet变量里头。我们尝试运行下这个print(greet())# output: 'hi yasoob'# 如果我们删掉旧的hi函数，看看会发生什么！del hiprint(hi())#outputs: NameErrorprint(greet()) # 这里可不像数组那样具有对象可变性噢#outputs: 'hi yasoob' 在函数中定义函数刚才那些就是函数的基本知识了。我们来让你的知识更进一步。在Python中我们可以在一个函数中定义另一个函数： 123456789101112131415161718192021222324def hi(name="yasoob"): print("now you are inside the hi() function") def greet(): return "now you are in the greet() function" def welcome(): return "now you are in the welcome() function" print(greet()) print(welcome()) print("now you are back in the hi() function")hi()#output:now you are inside the hi() function# now you are in the greet() function# now you are in the welcome() function# now you are back in the hi() function# 上面展示了无论何时你调用hi(), greet()和welcome()将会同时被调用。# 然后greet()和welcome()函数在hi()函数之外是不能访问的，比如：greet()#outputs: NameError: name 'greet' is not defined 那现在我们知道了可以在函数中定义另外的函数。也就是说：我们可以创建嵌套的函数。现在你需要再多学一点，就是函数也能返回函数。 从函数中返回函数其实并不需要在一个函数里去执行另一个函数，我们也可以将其作为输出返回出来： 123456789101112131415161718192021def hi(name="yasoob"): def greet(): return "now you are in the greet() function" def welcome(): return "now you are in the welcome() function" if name == "yasoob": return greet else: return welcomea = hi() # 执行hi()函数，得到return greet(注意没有括号)，将函数赋给aprint(a)#outputs: &lt;function greet at 0x7f2143c01500&gt;#上面清晰地展示了`a`现在指向到hi()函数中的greet()函数#现在试试这个print(a())#outputs: now you are in the greet() function 再次看看这个代码。在if/else语句中我们返回greet和welcome，而不是greet()和welcome()。为什么那样？这是因为当你把一对小括号放在后面，这个函数就会执行；然而如果你不放括号在它后面，那它可以被到处传递，并且可以赋值给别的变量而不去执行它。 你明白了吗？让我再稍微多解释点细节。 当我们写下a = hi()，hi()会被执行，而由于name参数默认是yasoob，所以函数greet被返回了。如果我们把语句改为a = hi(name = &quot;ali&quot;)，那么welcome函数将被返回。我们还可以打印出hi()()，这会输出now you are in the greet() function。连续两个括号的函数执行也可以，只要第一个hi()返回一个函数即可. 将函数作为参数传给另一个函数12345678910def hi(): return "hi yasoob!"def doSomethingBeforeHi(func): print("I am doing some boring work before executing hi()") print(func())doSomethingBeforeHi(hi)#outputs:I am doing some boring work before executing hi()# hi yasoob! 现在你已经具备所有必需知识，来进一步学习装饰器真正是什么了。装饰器让你在一个函数的前后去执行代码。 你的第一个装饰器在上一个例子里，其实我们已经创建了一个装饰器！现在我们修改下上一个装饰器，并编写一个稍微更有用点的程序： 1234567891011121314151617181920212223242526def a_new_decorator(a_func): def wrapTheFunction(): print("I am doing some boring work before executing a_func()") a_func() print("I am doing some boring work after executing a_func()") return wrapTheFunctiondef a_function_requiring_decoration(): print("I am the function which needs some decoration to remove my foul smell")a_function_requiring_decoration()#outputs: "I am the function which needs some decoration to remove my foul smell"a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)# 将带参数a_new_decorator函数传递给a_function_requiring_decoration#now a_function_requiring_decoration is wrapped by wrapTheFunction()a_function_requiring_decoration()# 执行a_function_requiring_decoration，也就是执行带参数的a_new_decorator函数#outputs:I am doing some boring work before executing a_func()# I am the function which needs some decoration to remove my foul smell# I am doing some boring work after executing a_func() 你看明白了吗？我们刚刚应用了之前学习到的原理。这正是python中装饰器做的事情！它们封装一个函数，并且用这样或者那样的方式来修改它的行为。现在你也许疑惑，我们在代码里并没有使用@符号？那只是一个简短的方式来生成一个被装饰的函数。这里是我们如何使用@来运行之前的代码： 12345678910111213@a_new_decorator #将@下面的函数作为参数传入a_new_decorator函数，并合成@下面的同名函数。def a_function_requiring_decoration(): """Hey you! Decorate me!""" print("I am the function which needs some decoration to " "remove my foul smell")a_function_requiring_decoration() # 执行同名函数，也即执行带参的修饰器函数#outputs: I am doing some boring work before executing a_func()# I am the function which needs some decoration to remove my foul smell# I am doing some boring work after executing a_func()#the @a_new_decorator is just a short way of saying:a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration) 希望你现在对Python装饰器的工作原理有一个基本的理解。如果我们运行如下代码会存在一个问题： 12print(a_function_requiring_decoration.__name__)# Output: wrapTheFunction 这并不是我们想要的！Ouput输出应该是“a_function_requiring_decoration”。这里的函数被warpTheFunction替代了。它重写了我们函数的名字和注释文档(docstring)。幸运的是Python提供给我们一个简单的函数来解决这个问题，那就是functools.wraps。我们修改上一个例子来使用functools.wraps：在装饰器函数内加入@wraps(a_func) 123456789101112131415161718from functools import wrapsdef a_new_decorator(a_func): @wraps(a_func) def wrapTheFunction(): print("I am doing some boring work before executing a_func()") a_func() print("I am doing some boring work after executing a_func()") return wrapTheFunction@a_new_decoratordef a_function_requiring_decoration(): """Hey yo! Decorate me!""" print("I am the function which needs some decoration to " "remove my foul smell")print(a_function_requiring_decoration.__name__)# Output: a_function_requiring_decoration 现在好多了。我们接下来学习装饰器的一些常用场景。 蓝本规范: 1234567891011121314151617181920from functools import wrapsdef decorator_name(f): @wraps(f) def decorated(*args, **kwargs): if not can_run: return "Function will not run" return f(*args, **kwargs) return decorated@decorator_namedef func(): return("Function is running")can_run = True # 全局变量print(func()) # 由于func函数有装饰器，执行func即执行func为参的修饰器函数# Output: Function is runningcan_run = Falseprint(func())# Output: Function will not run 注意：@wraps接受一个函数来进行装饰，并加入了复制函数名称、注释文档、参数列表等等的功能。这可以让我们在装饰器里面访问在装饰之前的函数的属性。 使用场景：授权(Authorization)装饰器能有助于检查某个人是否被授权去使用一个web应用的端点(endpoint)。它们被大量使用于Flask和Django web框架中。这里是一个例子来使用基于装饰器的授权： 12345678910from functools import wrapsdef requires_auth(f): @wraps(f) def decorated(*args, **kwargs): auth = request.authorization if not auth or not check_auth(auth.username, auth.password): authenticate() return f(*args, **kwargs) return decorated 使用场景：日志(Logging)日志是装饰器运用的另一个亮点。这是个例子： 12345678910111213141516from functools import wrapsdef logit(func): @wraps(func) def with_logging(*args, **kwargs): print(func.__name__ + " was called") # 打印调用函数信息并执行相关函数 return func(*args, **kwargs) return with_logging@logitdef addition_func(x): """Do some math.""" return x + xresult = addition_func(4)# Output: addition_func was called 我敢肯定你已经在思考装饰器的一个其他聪明用法了。 带参数的装饰器来想想这个问题，难道@wraps不也是个装饰器吗？但是，它接收一个参数，就像任何普通的函数能做的那样。那么，为什么我们不也那样做呢？ 这是因为，当你使用@my_decorator语法时，你是在应用一个以单个函数作为参数的一个包裹函数。记住，Python里每个东西都是一个对象，而且这包括函数！记住了这些，我们可以编写一下能返回一个包裹函数的函数。 在函数中嵌入装饰器我们回到日志的例子，并创建一个包裹函数，能让我们指定一个用于输出的日志文件。 12345678910111213141516171819202122232425262728293031from functools import wrapsdef logit(logfile='out.log'): def logging_decorator(func): @wraps(func) def wrapped_function(*args, **kwargs): log_string = func.__name__ + " was called" print(log_string) # 打开logfile，并写入内容 with open(logfile, 'a') as opened_file: # 现在将日志打到指定的logfile opened_file.write(log_string + '\n') return func(*args, **kwargs) return wrapped_function return logging_decorator # return函数就是执行函数@logit()def myfunc1(): passmyfunc1()# Output: myfunc1 was called# 现在一个叫做 out.log 的文件出现了，里面的内容就是上面的字符串@logit(logfile='func2.log')def myfunc2(): passmyfunc2()# Output: myfunc2 was called# 现在一个叫做 func2.log 的文件出现了，里面的内容就是上面的字符串 装饰器类现在我们有了能用于正式环境的logit装饰器，但当我们的应用的某些部分还比较脆弱时，异常也许是需要更紧急关注的事情。比方说有时你只想打日志到一个文件。而有时你想把引起你注意的问题发送到一个email，同时也保留日志，留个记录。这是一个使用继承的场景，但目前为止我们只看到过用来构建装饰器的函数。 幸运的是，类也可以用来构建装饰器。那我们现在以一个类而不是一个函数的方式，来重新构建logit。 1234567891011121314151617181920212223from functools import wrapsclass logit(object): def __init__(self, logfile='out.log'): self.logfile = logfile def __call__(self, func): @wraps(func) def wrapped_function(*args, **kwargs): log_string = func.__name__ + " was called" print(log_string) # 打开logfile并写入 with open(self.logfile, 'a') as opened_file: # 现在将日志打到指定的文件 opened_file.write(log_string + '\n') # 现在，发送一个通知 self.notify() return func(*args, **kwargs) # logit装饰器顺序：写入日志，发送通知，调用相关函数 return wrapped_function def notify(self): # logit只打日志，不做别的 pass 这个实现有一个附加优势，在于比嵌套函数的方式更加整洁，而且包裹一个函数还是使用跟以前一样的语法： 123@logit() # 类作为装饰器def myfunc1(): pass 现在，我们给logit创建子类，来添加email的功能(虽然email这个话题不会在这里展开)。 123456789101112class email_logit(logit): ''' 一个logit的实现版本，可以在函数调用时发送email给管理员 ''' def __init__(self, email='admin@myproject.com', *args, **kwargs): self.email = email super(email_logit, self).__init__(*args, **kwargs) def notify(self): # 发送一封email到self.email # 这里就不做实现了 pass 从现在起，@email_logit将会和@logit产生同样的效果，但是在打日志的基础上，还会多发送一封邮件给管理员。]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（5）：set&三元运算符]]></title>
    <url>%2F2018%2F06%2F28%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%885%EF%BC%89%EF%BC%9Aset%26%E4%B8%89%E5%85%83%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[set(集合)数据结构set(集合)是一个非常有用的数据结构。它与列表(list)的行为类似，区别在于set不能包含重复的值。这在很多情况下非常有用。例如你可能想检查列表中是否包含重复的元素，你有两个选择，第一个需要使用for循环，就像这样： 12345678910some_list = ['a', 'b', 'c', 'b', 'd', 'm', 'n', 'n']duplicates = []for value in some_list: if some_list.count(value) &gt; 1: if value not in duplicates: duplicates.append(value)print(duplicates)### 输出: ['b', 'n'] 但还有一种更简单更优雅的解决方案，那就是使用集合(sets)，你直接这样做： 1234some_list = ['a', 'b', 'c', 'b', 'd', 'm', 'n', 'n']duplicates = set([x for x in some_list if some_list.count(x) &gt; 1])print(duplicates)### 输出: set(['b', 'n']) 集合还有一些其它方法，下面我们介绍其中一部分。 交集你可以对比两个集合的交集（两个集合中都有的数据），如下： 1234valid = set(['yellow', 'red', 'blue', 'green', 'black'])input_set = set(['red', 'brown'])print(input_set.intersection(valid))### 输出: set(['red']) 差集你可以用差集(difference)找出无效的数据，相当于用一个集合减去另一个集合的数据，例如： 1234valid = set(['yellow', 'red', 'blue', 'green', 'black'])input_set = set(['red', 'brown'])print(input_set.difference(valid))### 输出: set(['brown']) 你也可以用{}符号来创建集合，如： 123a_set = &#123;'red', 'blue', 'green'&#125;print(type(a_set))### 输出: &lt;type 'set'&gt; 三元运算符三元运算符通常在Python里被称为条件表达式，这些表达式基于真(true)/假(false)的条件判断，在Python 2.4以上才有了三元操作。 下面是一个伪代码和例子： 伪代码: 12#如果条件为真，返回真 否则返回假condition_is_true if condition else condition_is_false 例子: 12is_fat = Truestate = "fat" if is_fat else "not fat" 它允许用简单的一行快速判断，而不是使用复杂的多行if语句。 这在大多数时候非常有用，而且可以使代码简单可维护。]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（4）：函数式编程]]></title>
    <url>%2F2018%2F06%2F28%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%884%EF%BC%89%EF%BC%9A%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Map，Filter 和 Reduce 三个函数能为函数式编程提供便利。我们会通过实例一个一个讨论并理解它们。 MapMap会将一个函数映射到一个输入列表的所有元素上。这是它的规范： 规范 1map(function_to_apply, list_of_inputs) 大多数时候，我们要把列表中所有元素一个个地传递给一个函数，并收集输出。比方说： 1234items = [1, 2, 3, 4, 5]squared = []for i in items: squared.append(i**2) Map可以让我们用一种简单而漂亮得多的方式来实现。就是这样(x为逗号右边的输入元素items)： 12items = [1, 2, 3, 4, 5]squared = list(map(lambda x: x**2, items)) 大多数时候，我们使用匿名函数(lambdas)来配合map, 所以我在上面也是这么做的。 不仅用于一列表的输入， 我们甚至可以用于一列表的函数！ 12345678910111213141516171819def multiply(x): return (x*x)def add(x): return (x+x)funcs = [multiply, add]for i in range(5): value = map(lambda x: x(i), funcs) # 每次执行i时，map将funcs内元素都执行lambda函数，即执行multiply(i)和add(i) print(list(value)) # 译者注：上面print时，加了list转换，是为了python2/3的兼容性 # 在python2中map直接返回列表，但在python3中返回迭代器 # 因此为了兼容python3, 需要list转换一下# Output:# [0, 0]# [1, 2]# [4, 4]# [9, 6]# [16, 8] Filter顾名思义，filter过滤列表中的元素，并且返回一个由所有符合要求的元素所构成的列表，符合要求即函数映射到该元素时返回值为True. 这里是一个简短的例子： 12345678number_list = range(-5, 5)less_than_zero = filter(lambda x: x &lt; 0, number_list)print(list(less_than_zero)) # 译者注：上面print时，加了list转换，是为了python2/3的兼容性# 在python2中filter直接返回列表，但在python3中返回迭代器# 因此为了兼容python3, 需要list转换一下# Output: [-5, -4, -3, -2, -1] 这个filter类似于一个for循环，但它是一个内置函数，并且更快。 注意：如果map和filter对你来说看起来并不优雅的话，那么你可以看看另外一章：列表/字典/元组推导式。 译者注：大部分情况下推导式的可读性更好 Reduce当需要对一个列表进行一些计算并返回结果时，Reduce 是个非常有用的函数。举个例子，当你需要计算一个整数列表的乘积时。Reduce() 函数会对参数序列中元素进行累积，函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。 语法：reduce(function, iterable[, initializer]) 参数： function — 函数，有两个参数 iterable — 可迭代对象 initializer — 可选，初始参数 现在我们来试试 reduce： 1234from functools import reduceproduct = reduce( (lambda x, y: x * y), [1, 2, 3, 4] )# Output: 24 在 Python3 中，reduce() 函数已经被从全局名字空间里移除了，它现在被放置在 fucntools 模块里，如果想要使用它，则需要通过引入 functools 模块来调用 reduce() 函数： 12&gt; from functools import reduce&gt;]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（3）：生成器]]></title>
    <url>%2F2018%2F06%2F28%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%883%EF%BC%89%EF%BC%9A%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[首先我们要理解迭代器(iterators)。根据维基百科，迭代器是一个让程序员可以遍历一个容器（特别是列表）的对象。然而，一个迭代器在遍历并读取一个容器的数据元素时，并不会执行一个迭代。这里有三个部分： 可迭代对象(Iterable) 迭代器(Iterator) 迭代(Iteration) 上面这些部分互相联系。我们会先各个击破来讨论他们，然后再讨论生成器(generators). 可迭代对象（Iterable）Python中任意的对象，只要它定义了可以返回一个迭代器的__iter__方法，或者定义了可以支持下标索引的__getitem__方法(这些双下划线方法会在其他章节中全面解释)，那么它就是一个可迭代对象。简单说，可迭代对象就是能提供迭代器的任意对象。那迭代器又是什么呢？ 迭代器(Iterator)任意对象，只要定义了next(Python2) 或者__next__方法，它就是一个迭代器。就这么简单。现在我们来理解迭代(iteration) 迭代(Iteration)用简单的话讲，它就是从某个地方（比如一个列表）取出一个元素的过程。当我们使用一个循环来遍历某个东西时，这个过程本身就叫迭代。现在既然我们有了这些术语的基本理解，那我们开始理解生成器吧。 关系结论： 可迭代对象包含迭代器 如果一个对象拥有__iter__方法，其是可迭代对象；如果一个对象拥有next方法，其是迭代器。 定义可迭代对象，必须实现_iter 方法；定义迭代器，必须实现 _iter和next方法。 生成器(Generators)生成器也是一种迭代器，但是你只能对其迭代一次。这是因为它们并没有把所有的值存在内存中，而是在运行时生成值。你通过遍历来使用它们，要么用一个“for”循环，要么将它们传递给任意可以进行迭代的函数和结构。大多数时候生成器是以函数来实现的。然而，它们并不返回一个值，而是yield(暂且译作“生出”)一个值。这里有个生成器函数的简单例子： 1234567891011121314151617def generator_function(): for i in range(10): yield ifor item in generator_function(): print(item)# Output: 0# 1# 2# 3# 4# 5# 6# 7# 8# 9 这个案例并不是非常实用。生成器最佳应用场景是：你不想同一时间将所有计算出来的大量结果集分配到内存当中，特别是结果集里还包含循环。 译者注：这样做会消耗大量资源 许多Python 2里的标准库函数都会返回列表，而Python 3都修改成了返回生成器，因为生成器占用更少的资源。 下面是一个计算斐波那契数列的生成器： 123456# generator versiondef fibon(n): a = b = 1 for i in range(n): yield a a, b = b, a + b 函数使用方法如下： 12for x in fibon(1000000): print(x) 用这种方式，我们可以不用担心它会使用大量资源。然而，之前如果我们这样来实现的话： 1234567def fibon(n): a = b = 1 result = [] for i in range(n): result.append(a) a, b = b, a + b return result 这也许会在计算很大的输入参数时，用尽所有的资源。我们已经讨论过生成器使用一次迭代，但我们并没有测试过。在测试前你需要再知道一个Python内置函数：next()。它允许我们获取一个序列的下一个元素。那我们来验证下我们的理解： 123456789101112131415def generator_function(): for i in range(3): yield igen = generator_function()print(next(gen))# Output: 0print(next(gen))# Output: 1print(next(gen))# Output: 2print(next(gen))# Output: Traceback (most recent call last):# File "&lt;stdin&gt;", line 1, in &lt;module&gt;# StopIteration 我们可以看到，在yield掉所有的值后，next()触发了一个StopIteration的异常。基本上这个异常告诉我们，所有的值都已经被yield完了。你也许会奇怪，为什么我们在使用for循环时没有这个异常呢？啊哈，答案很简单。for循环会自动捕捉到这个异常并停止调用next()。你知不知道Python中一些内置数据类型也支持迭代哦？我们这就去看看： 12345my_string = "Yasoob"next(my_string)# Output: Traceback (most recent call last):# File "&lt;stdin&gt;", line 1, in &lt;module&gt;# TypeError: str object is not an iterator 好吧，这不是我们预期的。这个异常说那个str对象不是一个迭代器。对，就是这样！它是一个可迭代对象，而不是一个迭代器。这意味着它支持迭代，但我们不能直接对其进行迭代操作。那我们怎样才能对它实施迭代呢？是时候学习下另一个内置函数，iter。它将根据一个可迭代对象返回一个迭代器对象。这里是我们如何使用它： 1234my_string = "Yasoob"my_iter = iter(my_string)next(my_iter)# Output: 'Y' 现在好多啦。我肯定你已经爱上了学习生成器。一定要记住，想要完全掌握这个概念，你只有使用它。确保你按照这个模式，并在生成器对你有意义的任何时候都使用它。你绝对不会失望的！]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（2）：调试Debugging]]></title>
    <url>%2F2018%2F06%2F27%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%882%EF%BC%89%EF%BC%9A%E8%B0%83%E8%AF%95Debugging%2F</url>
    <content type="text"><![CDATA[调试（Debugging）利用好调试，能大大提高你捕捉代码Bug的。大部分新人忽略了Python debugger(pdb)的重要性。 在这个章节我只会告诉你一些重要的命令，你可以从官方文档中学习到更多。 参考：https://docs.python.org/2/library/pdb.html Or https://docs.python.org/3/library/pdb.html 从命令行运行你可以在命令行使用Python debugger运行一个脚本， 举个例子： 1$ python -m pdb my_script.py 这会触发debugger在脚本第一行指令处停止执行。这在脚本很短时会很有帮助。你可以通过(Pdb)模式接着查看变量信息，并且逐行调试。 从脚本内部运行同时，你也可以在脚本内部设置断点，这样就可以在某些特定点查看变量信息和各种执行时信息了。这里将使用pdb.set_trace()方法来实现。举个例子： 1234567import pdbdef make_bread(): pdb.set_trace() return "I don't have time"print(make_bread()) 试下保存上面的脚本后运行之。你会在运行时马上进入debugger模式。现在是时候了解下debugger模式下的一些命令了。 命令列表： c: 继续执行 w: 显示当前正在执行的代码行的上下文信息 a: 打印当前函数的参数列表 s: 执行当前代码行，并停在第一个能停的地方（相当于单步进入） n: 继续执行到当前函数的下一行，或者当前行直接返回（单步跳过） 单步跳过（next）和单步进入（step）的区别在于， 单步进入会进入当前行调用的函数内部并停在里面， 而单步跳过会（几乎）全速执行完当前行调用的函数，并停在当前函数的下一行。 pdb真的是一个很方便的功能，上面仅列举少量用法，更多的命令强烈推荐你去看官方文档。]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程进阶（1）：*args和**kwargs]]></title>
    <url>%2F2018%2F06%2F27%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%2Fpython%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%881%EF%BC%89%EF%BC%9A-args%E5%92%8C-kwargs%2F</url>
    <content type="text"><![CDATA[args和*kwargs首先, 其实并不是必须写成*args 和**kwargs。 只有变量前面的 *(星号)才是必须的. 你也可以写成*var 和**vars. 而写成*args 和**kwargs只是一个通俗的命名约定。 *args的用法*args 和 **kwargs 主要用于函数定义。 你可以将不定数量的参数传递给一个函数。这里的不定的意思是：预先并不知道, 函数使用者会传递多少个参数给你, 所以在这个场景下使用这两个关键字。 *args 是用来发送一个非键值对的可变数量的参数列表给一个函数 看下面这个例子 123456def test_var_args(f_arg, *argv): print("first normal arg:", f_arg) for arg in argv: print("another arg through *argv:", arg)test_var_args('yasoob', 'python', 'eggs', 'test') 产生输出: 1234first normal arg: yasoobanother arg through *argv: pythonanother arg through *argv: eggsanother arg through *argv: test **kwargs的用法**kwargs 允许你将不定长度的键值对, 作为参数传递给一个函数。 如果你想要在一个函数里处理带名字的参数, 你应该使用**kwargs 看下面的例子 1234567def greet_me(**kwargs): for key, value in kwargs.items(): print("&#123;0&#125; == &#123;1&#125;".format(key, value))&gt;&gt;&gt; greet_me(name="yasoob")name == yasoo 使用args和*kwargs来调用函数假设，你有这样一个小函数： 1234def test_args_kwargs(arg1, arg2, arg3): print("arg1:", arg1) print("arg2:", arg2) print("arg3:", arg3) 你可以使用*args或**kwargs来给这个小函数传递参数。 下面是怎样做： 12345678910111213# 首先使用 *args&gt;&gt;&gt; args = ("two", 3, 5)&gt;&gt;&gt; test_args_kwargs(*args)arg1: twoarg2: 3arg3: 5# 现在使用 **kwargs:&gt;&gt;&gt; kwargs = &#123;"arg3": 3, "arg2": "two", "arg1": 5&#125;&gt;&gt;&gt; test_args_kwargs(**kwargs)arg1: 5arg2: twoarg3: 3 标准参数与args、*kwargs在使用时的顺序:1some_func(fargs, *args, **kwargs) 什么时候使用它们？这还真的要看你的需求而定。 最常见的用例是在写函数装饰器的时候（会在另一章里讨论）。 此外它也可以用来做猴子补丁(monkey patching)。猴子补丁的意思是在程序运行时(runtime)修改某些代码。 打个比方，你有一个类，里面有个叫get_info的函数会调用一个API并返回相应的数据。如果我们想测试它，可以把API调用替换成一些测试数据。例如： 123456import someclassdef get_info(self, *args): return "Test data"someclass.get_info = get_info]]></content>
      <categories>
        <category>python编程进阶</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Linux Command Line学习笔记（四）]]></title>
    <url>%2F2018%2F06%2F16%2FLinux%2FThe%20Linux%20Command%20Line%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[第二十一章：文本处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355这一章会重新拜访一些老朋友，并且会给我们介绍一些新朋友：cat – 连接文件并且打印到标准输出sort – 给文本行排序uniq – 报告或者省略重复行cut – 从每行中删除文本区域paste – 合并文件文本行join – 基于某个共享字段来联合两个文件的文本行comm – 逐行比较两个有序的文件diff – 逐行比较文件tr – 翻译或删除字符sed – 用于筛选和转换文本的流编辑器cat这个 cat 程序具有许多有趣的选项。其中许多选项用来帮助更好的可视化文本内容。一个例子是-A 选项， 其用来在文本中显示所有非打印字符。[me@linuxbox ~]$ cat -A foo.txt^IThe quick brown fox jumped over the lazy dog. $[me@linuxbox ~]$在输出结果中我们看到，这个 tab 字符在我们的文本中由^I 字符来表示，我们也看到一个$字符出现在文本行真正的结尾处， 表明我们的文本包含末尾的空格。cat 程序也包含用来修改文本的选项。最著名的两个选项是-n，其给文本行添加行号和-s， 禁止输出多个空白行。[me@linuxbox ~]$ cat &gt; foo.txtThe quick brown foxjumped over the lazy dog.[me@linuxbox ~]$ cat -ns foo.txt #增加行号，删除多余的空白行1 The quick brown fox23 jumped over the lazy dog.[me@linuxbox ~]$sort我们能够演示如何用 sort 程序来处理标准输入：[me@linuxbox ~]$ sort &gt; foo.txtcba[me@linuxbox ~]$ cat foo.txtabc表21-1: 常见的 sort 程序选项| 选项 | 长选项 | 描述 ||-------|-------|-------|| -b | --ignore-leading-blanks | 默认情况下，从每行的第一个字符开始。这个选项从第一个非空白字符开始排序。 || -f | --ignore-case | 让排序不区分大小写。 || -n | --numeric-sort | 基于字符串的长度来排序| -r | --reverse | 按相反顺序排序。结果按照降序排列，而不是升序。 || -k | --key=field1[,field2] | 对从 field1到 field2之间的字符排序，而不是整个文本行| -o | --output=file | 把排好序的输出结果发送到文件，而不是标准输出。 || -t | --field-separator=char | 定义域分隔字符。我们通过对 du 命令的输出结果排序来说明这个选项，du 命令可以 确定最大的磁盘空间用户，通常，这个du 命令列出的输出结果按照路径名来序：[me@linuxbox ~]$ du -s /usr/share/\* | head252 /usr/share/aclocal96 /usr/share/acpi-support8 /usr/share/adduser...[me@linuxbox ~]$ du -s /usr/share/* | sort -nr | head #根据长度反序排序509940 /usr/share/locale-langpack242660 /usr/share/doc197560 /usr/share/fonts这种排序起作用是 因为数值出现在每行的开头。但是如果我们想要基于文件行中的某个数值排序忽略 ls 程序能按照文件大小对输出结果进行排序，我们也能够使用 sort 程序来完成此任务：[me@linuxbox ~]$ ls -l /usr/bin | sort -nr -k 5 | head #空格和制表符作为分界符，所以是以第五个字段，文件大小来排序-rwxr-xr-x 1 root root 8234216 2008-04-0717:42 inkscape-rwxr-xr-x 1 root root 8222692 2008-04-07 17:42 inkview...在上面的例子中，我们指定 n 和 r 选项来执行相反的数值排序，并且指定 -k 5，让 sort 程序使用第五字段作为排序的关键值。依赖多个关键值排序，例如 文件中的每一行都有三个字段：发行版的名称，版本号，和 MM/DD/YYYY 格式的发行日期：SUSE 10.2 12/07/2006Fedora 10 11/25/2008SUSE 11.04 06/19/2008Ubuntu 8.04 04/24/2008...为了解决这个问题，我们必须依赖多个键值来排序。sort 程序允许多个 -k 选项的实例，所以可以指定多个排序关键值。[me@linuxbox ~]$ sort --key=1,1 --key=2n distros.txtFedora 5 03/20/2006Fedora 6 10/24/2006Fedora 7 05/31/2007...在第一个 key 选项的实例中， 我们指定了一个字段区域。因为我们只想对第一个字段排序，我们指定了 1,1， 意味着“始于并且结束于第一个字段。”在第二个实例中，我们指定了 2n，意味着第二个字段是排序的键值，选项字母和sort 程序的全局选项一样：b（忽略开头的空格），n（数值序），r（逆向排序），等等。我们列表中第三个字段包含的日期格式不利于排序。在计算机中，日期通常设置为 YYYY-MM-DD 格式，这样使按时间顺序排序变得容易，但是我们的日期为美国格式 MM/DD/YYYY。那么我们怎样能按照 时间顺序来排列这个列表呢？幸运地是，sort 程序提供了一种方式。这个 key 选项允许在字段中指定偏移量，所以我们能在字段中 定义键值。[me@linuxbox ~]$ sort -k 3.7nbr -k 3.1nbr -k 3.4nbr distros.txtFedora 10 11/25/2008Ubuntu 8.10 10/30/2008SUSE 11.0 06/19/2008...通过指定 -k 3.7，我们指示 sort 程序使用一个排序键值，其始于第三个字段中的第七个字符，同样地，我们指定 -k 3.1和 -k 3.4来分离日期中的月和日。 我们也添加了 n 和 r 选项来实现一个逆向的数值排序。这个 b 选项用来删除日期字段中开头的空格（ 行与行之间的空格数迥异，因此会影响 sort 程序的输出结果）。一些文件不会使用 tabs 和空格做为字段界定符；例如，这个 /etc/passwd 文件，这个文件的字段之间通过冒号分隔开，所以我们怎样使用一个 key 字段来排序这个文件？sort 程序提供了一个 -t 选项来自定义分隔符。[me@linuxbox ~]$ sort -t &apos;:&apos; -k 7 /etc/passwd | head # 以 ： 为分隔符uniq与 sort 程序相比，这个 uniq 程序是个轻量级程序，uniq 会删除任意重复行，并且把结果发送到标准输出。 它常常和 sort 程序一块使用，来清理重复的输出。让我们创建一个文本文件，来实验一下：[me@linuxbox ~]$ cat &gt; foo.txtabcabc住输入 Ctrl-d 来终止标准输入。现在，如果我们对文本文件执行 uniq 命令：[me@linuxbox ~]$ uniq foo.txtabcabc输出结果与原始文件没有差异；重复行没有被删除。实际上，uniq 程序能完成任务，其输入必须是排好序的数据，[me@linuxbox ~]$ sort foo.txt | uniqabc这是因为 uniq 只会删除相邻的重复行。uniq 程序有几个选项。这里是一些常用选项：表21-2: 常用的 uniq 选项| 选项 | 说明 ||-------|-----|| -c | 输出所有的重复行，并且每行开头显示重复的次数。 || -d | 只输出重复行，而不是特有的文本行。 || -i | 在比较文本行的时候忽略大小写。 || -u | 只是输出独有的文本行。这是默认的。 |[me@linuxbox ~]$ sort foo.txt | uniq -c 2 a 2 b 2 c切片和切块cut这个 cut 程序被用来从文本行中抽取文本，并把其输出到标准输出。从文本行中指定要抽取的文本有些麻烦，使用以下选项：表21-3: cut 程序选择项| 选项 | 说明 ||-----|------|| -c char_list | 从文本行中抽取由 char_list 定义的文本，也就是字符。| -f field_list | 从文本行中抽取一个或多个由 field_list 定义的字段| -d delim_char | 当指定-f 选项之后，使用 delim_char 做为字段分隔符| --complement | 抽取整个文本行，除了那些由-c 和／或-f 选项指定的文本。 |正如我们所看到的，cut 程序抽取文本的方式相当不灵活。cut 命令最好用来从其它程序产生的文件中 抽取文本，而不是从人们直接输入的文本中抽取。如果我们使用带有 -A 选项的 cat 命令，我们能查看是否这个文件符号由 tab 字符分离字段的要求。[me@linuxbox ~]$ cat -A distros.txtSUSE^I10.2^I12/07/2006$ #看起来不错。字段之间仅仅是单个 tab 字符，没有嵌入空格Fedora^I10^I11/25/2008$SUSE^I11.0^I06/19/2008$...[me@linuxbox ~]$ cut -f 3 distros.txt #第三个字段12/07/200611/25/200806/19/200804/24/2008...因为我们的 distros 文件是由 tab 分隔开的，最好用 cut 来抽取字段而不是字符。这是因为一个由 tab 分离的文件， 每行不太可能包含相同的字符数，这就使计算每行中字符的位置变得困难或者是不可能。然而， 我们已经抽取了一个字段，幸运地是其包含地日期长度相同，所以通过从每行中抽取年份，我们能展示怎样 来抽取字符：[me@linuxbox ~]$ cut -f 3 distros.txt | cut -c 7-10200620082007...通过对我们的列表再次运行 cut 命令，我们能够抽取从位置7到10的字符，其对应于日期字段的年份.当操作字段的时候，有可能指定不同的字段分隔符，而不是 tab 字符。这里我们将会从/etc/passwd 文件中 抽取第一个字段：[me@linuxbox ~]$ cut -d &apos;:&apos; -f 1 /etc/passwd | head #使用-d 选项，我们能够指定冒号做为字段分隔符。paste这个 paste 命令的功能正好与 cut 相反。它会添加一个或多个文本列到文件中，而不是从文件中抽取文本列[me@linuxbox ~]$ paste distros-dates.txt distros-versions.txt #第二个文件粘贴在第一个文件后join一个 join操作通常与关系型数据库有关联，在关系型数据库中来自多个享有共同关键域的表格的 数据结合起来，得到一个期望的结果。这个 join 程序执行相同的操作[me@linuxbox ~]$ join distros-key-names.txt distros-key-vernums.txt | head这两个文件的日期是共同列，可以自动连接。同样，第二个文件链接到第一个文件后面比较文本comm这个 comm 程序会比较两个文本文件，并且会显示每个文件特有的文本行和共有的文本行。通过使用 cat 命令，我们将会创建两个内容几乎相同的文本文件：[me@linuxbox ~]$ cat &gt; file1.txtabcd[me@linuxbox ~]$ cat &gt; file2.txtbcd下一步，我们将使用 comm 命令来比较这两个文件：[me@linuxbox ~]$ comm file1.txt file2.txta b c d e正如我们所见到的，comm 命令产生了三列输出。第一列包含第一个文件独有的文本行；第二列， 文本行是第二列独有的；第三列包含两个文件共有的文本行。comm 支持 -n 形式的选项，这里 n 代表 1，2或 3。这些选项使用的时候，指定了要隐藏的列。例如，如果我们只想输出两个文件共享的文本行， 我们将隐藏第一列和第二列的输出结果：[me@linuxbox ~]$ comm -12 file1.txt file2.txtbcddiff类似于 comm 程序，diff 程序被用来监测文件之间的差异。软件开发员经常使用 diff 程序来检查不同程序源码 版本之间的更改，diff 能够递归地检查源码目录，经常称之为源码树。diff 程序的一个常见用例是 创建 diff 文件或者补丁，它会被其它程序使用，例如 patch 程序（我们一会儿讨论），来把文件 从一个版本转换为另一个版本。如果我们使用 diff 程序，来查看我们之前的文件实例：[me@linuxbox ~]$ diff file1.txt file2.txt1d0&lt; a4a4&gt; e在默认格式中（不常用）， 每组的更改之前都是一个更改命令，其形式为 range operation range ， 用来描述要求更改的位置和类型，从而把第一个文件转变为第二个文件：表21-4: diff 更改命令| 改变 | 说明 ||-----|--------|| r1ar2 | 把第二个文件中位置 r2 处的文件行添加到第一个文件中的 r1 处。 || r1cr2 | 用第二个文件中位置 r2 处的文本行更改（替代）位置 r1 处的文本行。 || r1dr2 | 删除第一个文件中位置 r1 处的文本行，这些文本行将会出现在第二个文件中位置 r2 处。 |常用的是上下文模式和统一模式（记住这个就行了）当使用上下文模式（带上 -c 选项），我们将看到这些：[me@linuxbox ~]$ diff -c file1.txt file2.txt*** file1.txt 2008-12-23 06:40:13.000000000 -0500--- file2.txt 2008-12-23 06:40:34.000000000 -0500****************** 1,4 ****- a b c d--- 1,4 ---- b c d + e表21-5: diff 上下文模式更改指示符| 指示符 | 意思 ||-------|---------|| blank | 上下文显示行。它并不表示两个文件之间的差异。 || - | 删除行。这一行将会出现在第一个文件中，而不是第二个文件内。 || + | 添加行。这一行将会出现在第二个文件内，而不是第一个文件中。 || ! | 更改行。将会显示某个文本行的两个版本，每个版本会出现在更改组的各自部分。 |这个统一模式相似于上下文模式，但是更加简洁。通过 -u 选项来指定它：[me@linuxbox ~]$ diff -u file1.txt file2.txt--- file1.txt 2008-12-23 06:40:13.000000000 -0500+++ file2.txt 2008-12-23 06:40:34.000000000 -0500@@ -1,4 +1,4 @@-a b c d+e上下文模式和统一模式之间最显著的差异就是重复上下文的消除表21-6: diff 统一模式更改指示符| 字符 | 意思 ||-----|-------|| 空格 | 两个文件都包含这一行。 || - | 在第一个文件中删除这一行。 || + | 添加这一行到第一个文件中。 |patchLinux 内核是由一个 大型的，组织松散的贡献者团队开发而成，这些贡献者会提交固定的少量更改到源码包中。 这个 Linux 内核由几百万行代码组成，虽然每个贡献者每次所做的修改相当少。对于一个贡献者 来说，每做一个修改就给每个开发者发送整个的内核源码树，这是没有任何意义的。相反， 提交一个 diff 文件。一个 diff 文件包含先前的内核版本与带有贡献者修改的新版本之间的差异。 然后一个接受者使用 patch 程序，把这些更改应用到他自己的源码树中。运行时编辑tr这个 tr 程序被用来更改字符。我们可以把它看作是一种基于字符的查找和替换操作。我们可以通过 tr 命令来执行这样的转换，如下所示：[me@linuxbox ~]$ echo &quot;lowercase letters&quot; | tr a-z A-ZLOWERCASE LETTERStr 命令接受两个参数：要被转换的字符集以及 相对应的转换后的字符集。tr 也可以完成另一个技巧。使用-s 选项，tr 命令能“挤压”（删除）重复的字符实例：[me@linuxbox ~]$ echo &quot;aaabbbccc&quot; | tr -s ababccc这里我们有一个包含重复字符的字符串。通过给 tr 命令指定字符集“ab”，我们能够消除字符集中 字母的重复实例，然而会留下不属于字符集的字符（“c”）无更改。注意重复的字符必须是相邻的， 如果它们不相邻，那么挤压会没有效果，例如：[me@linuxbox ~]$ echo &quot;abcabcabc&quot; | tr -s ababcabcabcsed名字 sed 是 stream editor（流编辑器）的简称。它对文本流进行编辑，要不是一系列指定的文件， 要不就是标准输入。这里有一个非常简单的 sed 实例：[me@linuxbox ~]$ echo &quot;front&quot; | sed &apos;s/front/back/&apos;back在这个例子中，我们使用 echo 命令产生了一个单词的文本流，然后把它管道给 sed 命令。sed，依次，对流文本执行指令 s/front/back/，随后输出“back”。这个替换命令由字母 s 来代表，其后跟着查找 和替代字符串，斜杠字符做为分隔符。分隔符的选择是随意的。按照惯例，经常使用斜杠字符。sed 中的大多数命令之前都会带有一个地址，其指定了输入流中要被编辑的文本行。如果省略了地址， 然后会对输入流的每一行执行编辑命令。最简单的地址形式是一个行号。我们能够添加一个地址 到我们例子中：[me@linuxbox ~]$ echo &quot;front&quot; | sed &apos;1s/front/back/&apos;back给我们的命令添加地址 1，就导致只对仅有一行文本的输入流的第一行执行替换操作。如果我们指定另一个数字：[me@linuxbox ~]$ echo &quot;front&quot; | sed &apos;2s/front/back/&apos;front我们看到没有执行这个编辑命令，因为我们的输入流没有第二行。地址可以用许多方式来表达。这里是 最常用的：表21-7: sed 地址表示法| 地址 | 说明 ||-----|-------|| n | 行号，n 是一个正整数。 || $ | 最后一行。 || /regexp/ | 所有匹配一个 POSIX 基本正则表达式的文本行| addr1,addr2 | 从 addr1 到 addr2 范围内的文本行，包含地址 addr2 在内。地址可能是上述任意 单独的地址形式。 || first~step | 匹配由数字first代表的文本行，然后随后的每个在 step 间隔处的文本行。例如 5~5 则指第五行和之后每五行位置的文本。行。 || addr1,+n | 匹配地址 addr1 和随后的 n 个文本行。 || addr! | 匹配所有的文本行，除了 addr 之外通过使用这一章中早前的 distros.txt 文件，我们将演示不同种类的地址表示法。首先，一系列行号：[me@linuxbox ~]$ sed -n &apos;1,5p&apos; distros.txtSUSE 10.2 12/07/2006Fedora 10 11/25/2008SUSE 11.0 06/19/2008Ubuntu 8.04 04/24/2008Fedora 8 11/08/2007在这个例子中，我们打印出一系列的文本行，开始于第一行，直到第五行。为此，我们使用 p 命令， 其就是简单地把匹配的文本行打印出来。然而为了高效，我们必须包含选项 -n（不自动打印选项）， 让sed 不要默认地打印每一行。下一步，我们将试用一下正则表达式：[me@linuxbox ~]$ sed -n &apos;/SUSE/p&apos; distros.txt # 斜杠界定的正则表达式SUSE 10.2 12/07/2006SUSE 11.0 06/19/2008SUSE 10.3 10/04/2007SUSE 10.1 05/11/2006最后，我们将试着否定上面的操作，通过给这个地址添加一个感叹号：[me@linuxbox ~]$ sed -n &apos;/SUSE/!p&apos; distros.txtFedora 10 11/25/2008Ubuntu 8.04 04/24/2008Fedora 8 11/08/2007Ubuntu 6.10 10/26/2006这里我们看到期望的结果：输出了文件中所有的文本行，除了那些匹配这个正则表达式的文本行。目前为止，我们已经知道了两个 sed 的编辑命令，s 和 p。这里是一个更加全面的基本编辑命令列表：表21-8: sed 基本编辑命令| 命令 | 说明 ||-----|-------|| = | 输出当前的行号。 || a | 在当前行之后追加文本。 || d | 删除当前行。 || i | 在当前行之前插入文本。 || p | 打印当前行。默认情况下，sed 程序打印每一行，并且只是编辑文件中匹配指定地址的文本行。通过指定-n 选项，这个默认的行为能够被略。 || q | 退出 sed，不再处理更多的文本行。如果不指定-n 选项，输出当前行。 || Q | 退出 sed，不再处理更多的文本行。 || s/regexp/replacement/ | 只要找到一个 regexp 匹配项，就替换为 replacement 的内到目前为止，这个 s 命令是最常使用的编辑命令。[me@linuxbox ~]$ echo &quot;aaabbbccc&quot; | sed &apos;s/b/B/&apos;aaa Bbbccc我们看到虽然执行了替换操作，但是只针对第一个字母 “b” 实例，然而剩余的实例没有更改。通过添加g 标志， 我们能够更改所有的实例：[me@linuxbox ~]$ echo &quot;aaabbbccc&quot; | sed &apos;s/b/B/g&apos; 第二十二章：格式化输出1234567891011121314151617181920212223242526在这章中，我们继续着手于文本相关的工具，关注那些用来格式化输出的程序，而不是改变文本自身。 这些工具通常让文本准备就绪打印，这是我们在下一章会提到的。nl - 添加行号nl 程序是一个相当神秘的工具，用作一个简单的任务。它添加文件的行数。在它最简单的用途中，它相当于 cat -n:[me@linuxbox ~]$ nl distros.txt | head1 Fedora 5 2006-03-202 Fedora 6 2006-10-243 Fedora 7 2007-05-31fold - 限制文件行宽[me@linuxbox ~]$ echo &quot;The quick brown fox jumped over the lazy dog.&quot; | fold -w 12The quick brown fox jumped over thelazy dog.我们设定了行宽为12个字符。 如果没有字符设置，默认是80。注意到文本行不会因为单词边界而不会被分解。增加的 -s 选项将让 fold 分解到最后可用的空白 字符，即会考虑单词边界。[me@linuxbox ~]$ echo &quot;The quick brown fox jumped over the lazy dog.&quot;| fold -w 12 -sThe quickbrown foxjumped overthe lazydog. 第二十三章：打印非目前学习重点 第二十四章：编译程序123456789101112131415161718192021222324252627282930用高级语言编写的程序，经过另一个称为编译器的程序的处理，会转换成机器语言。一些编译器把 高级指令翻译成汇编语言，然后使用一个汇编器完成翻译成机器语言的最后阶段。Python脚本在Linux上怎么运行？[root@qiansw tmp]# cat test.py #这是一个python示例程序123#!/usr/bin/pythonfor i in range(0,5): print i[root@qiansw tmp]# lltotal 48-rw-r--r-- 1 root root 48 Oct 30 11:04 test.py[root@qiansw tmp]# chmod +x test.py #为脚本增加执行权限，另外一种表达[root@qiansw tmp]# chmod 755 test.py #为脚本增加执行权限[root@qiansw tmp]# ./test.py #这是第一种方法01234[root@qiansw tmp]# python test.py #这是第二种方法，不用增加执行权限01234有两种方式：1、直接使用python xxxx.py执行。其中python可以写成python的绝对路径。使用which python进行查询。2、在文件的头部（第一行）写上#!/usr/bin/python2.7，这个地方使用python的绝对路径，就是上面用which python查询来的结果。然后在外面就可以使用./xxx.py执行了。因为在linux中，python啊shell这些程序都是普通的文本格式，都需要一种程序去解释执行它。要么调用的时候指定，要么在文件头指定。 第二十五章：编写第一个shell脚本12345678910111213141516171819202122232425262728293031323334353637最简单的解释，一个 shell 脚本就是一个包含一系列命令的文件。shell 读取这个文件，然后执行 文件中的所有命令，就好像这些命令已经直接被输入到了命令行中一样。我们把此脚本文件保存为 hello_world。下一步我们要做的事情是让我们的脚本可执行。使用 chmod 命令，这很容易做到：[me@linuxbox ~]$ ls -l hello_world-rw-r--r-- 1 me me 63 2009-03-07 10:10 hello_world[me@linuxbox ~]$ chmod 755 hello_world[me@linuxbox ~]$ ls -l hello_world-rwxr-xr-x 1 me me 63 2009-03-07 10:10 hello_world对于脚本文件，有两个常见的权限设置；权限为755的脚本，则每个人都能执行，和权限为700的 脚本，只有文件所有者能够执行。脚本文件位置当设置了脚本权限之后，我们就能执行我们的脚本了：[me@linuxbox ~]$ ./hello_worldHello World!为了能够运行此脚本，我们必须指定脚本文件明确的路径。如果我们没有那样做，我们会得到这样的提示：[me@linuxbox ~]$ hello_worldbash: hello_world: command not found回到第12章，我们讨论了 PATH 环境变量及其它在系统 查找可执行程序方面的作用。回顾一下，如果没有给出可执行程序的明确路径名，那么系统每次都会 搜索一系列的目录，来查找此可执行程序。这个/bin 目录就是其中一个系统会自动搜索的目录。 这个目录列表被存储在一个名为PATH 的环境变量中。大多数的 Linux 发行版会配置 PATH 变量，让其包含一个位于用户家目录下的 bin 目录，从而允许用户能够执行他们自己的程序。所以如果我们创建了 一个bin 目录，并把我们的脚本放在这个目录下，那么这个脚本就应该像其它程序一样开始工作了：[me@linuxbox ~]$ mkdir bin[me@linuxbox ~]$ mv hello_world bin #移动到用户家目录下的bin目录下[me@linuxbox ~]$ hello_worldHello World!如果这个 PATH 变量不包含这个目录（记住以下两个命令就可以了），我们能够轻松地添加它，通过在我们的.bashrc 文件中包含下面 这一行文本：export PATH=~/bin:&quot;$PATH&quot;当做了这个修改之后，它会在每个新的终端会话中生效。为了把这个修改应用到当前的终端会话中， 我们必须让 shell 重新读取这个 .bashrc 文件。这可以通过 “sourcing”.bashrc 文件来完成：[me@linuxbox ~]$ . .bashrc脚本文件的好去处这个 ~/bin 目录是存放为个人所用脚本的好地方。如果我们编写了一个脚本，系统中的每个用户都可以使用它， 那么这个脚本的传统位置是 /usr/local/bin。系统管理员使用的脚本经常放到 /usr/local/sbin 目录下。 大多数情况下，本地支持的软件，不管是脚本还是编译过的程序，都应该放到 /usr/local 目录下，而不是在 /bin 或 /usr/bin 目录下。这些目录都是由 Linux 文件系统层次结构标准指定，只包含由 Linux发行商 所提供和维护的文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Linux Command Line学习笔记（三）]]></title>
    <url>%2F2018%2F06%2F12%2FLinux%2FThe%20Linux%20Command%20Line%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[第十六章：存储媒介非目前学习重点，暂略 第十七章：网络系统1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162我们仅仅会关注一些最经常 使用到的命令。我们要研究的命令包括那些被用来监测网络和传输文件的命令。ping - 发送 ICMP ECHO_REQUEST 软件包到网络主机traceroute - 打印到一台网络主机的路由数据包netstat - 打印网络连接，路由表，接口统计数据，伪装连接，和多路广播成员ftp - 因特网文件传输程序wget - 非交互式网络下载器ssh - Open SSH SSH 客户端（远程登录程序）ping，网络连接验证， 按下组合键 Ctrl-c，中断这个命令之后[me@linuxbox ~]$ ping www.baidu.com网络中传输文件ftpftp 命令属于真正的“经典”程序之一，它的名字来源于其所使用的协议，就是文件传输协议。FTP（它的原始形式）并不是安全的，因为它会以明码形式发送帐号的姓名和密码，由于此种原因，几乎因特网中所有 FTP 服务器 都是匿名的在下面的例子中，我们将展示一个典型的会话，从匿名 FTP 服务器，其名字是 fileserver，的/pub/_images/Ubuntu-8.04的目录下，使用 ftp 程序下载一个 Ubuntu 系统映像文件。[me@linuxbox ~]$ ftp fileserverName (fileserver:me): anonymousftp&gt; cd pub/cd\_images/Ubuntu-8.04ftp&gt; lsftp&gt; lcd Desktopftp&gt; get ubuntu-8.04-desktop-i386.isoftp&gt; bye|---------|------------|1、 唤醒 ftp 程序，让它连接到 FTP 服务器，fileserver。 |2、 登录名。输入登录名后，将出现一个密码提示。一些服务器将会接受空密码， 其它一些则会要求一个邮件地址形式的密码。如果是这种情况，试着输入 “user@example.com”。 |3、 跳转到远端系统中，要下载文件所在的目录下，注意在大多数匿名的 FTP 服务器中，支持公共下载的文件都能在目录 pub 下找到 |4、列出远端系统中的目录。 |5、 跳转到本地系统中的 ~/Desktop目录下。在实例中，ftp 程序在工作目录 ~ 下被唤醒。这个命令把工作目录改为 ~/Desktop |6、 告诉远端系统传送文件到本地。因为本地系统的工作目录 已经更改到了 ~/Desktop，所以文件会被下载到此目录。 |另外还有两个常用传输文件程序：lftp - 更好的 ftp，包括 多协议支持（包括 HTTP），若下载失败会自动地重新下载wget - 不只能下载单个文件，多个文件，甚至整个网站都能下载与远程主机安全通信明码形式来传输所有的交流信息（包括登录命令和密码）。这使它们完全不 适合使用在因特网时代。ssh为了解决这个问题，开发了一款新的协议，叫做 SSH（Secure Shell）。 SSH 首先，它要认证远端主机是否为它 所知道的那台主机（这样就阻止了所谓的“中间人”的攻击），其次，它加密了本地与远程主机之间 所有的通讯信息。SSH 由两部分组成。SSH 服务器运行在远端主机上运行，在端口号22上监听将要到来的连接，而 SSH 客户端用在本地系统中，用来和远端服务器通信。用来与远端 SSH 服务器相连接的 SSH 客户端程序，顺理成章，叫做 ssh。连接到远端名为 remote-sys主机，我们可以这样使用 ssh 客户端程序：[me@linuxbox ~]$ ssh remote-sysThe authenticity of host &apos;remote-sys (192.168.1.4)&apos; can&apos;t beestablished.RSA key fingerprint is41:ed:7a:df:23:19:bf:3c:a5:17:bc:61:b3:7f:d9:bb.Are you sure you want to continue connecting (yes/no)?一旦建立了连接，会提示 用户输入他或她的密码：远端 shell 会话一直存在，直到用户输入 exit 命令后，则关闭了远程连接。这时候，本地的 shell 会话 恢复，本地 shell 提示符重新出现。也有可能使用不同的用户名连接到远程系统。例如，如果本地用户“me”，在远端系统中有一个帐号名“bob”，则用户 me 能够用 bob 帐号登录到远端系统，如下所示：[me@linuxbox ~]$ ssh bob@remote-sysbob@remote-sys&apos;s password:Last login: Sat Aug 30 13:03:21 2008[bob@remote-sys ~]$ 第十八章：查找文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102在这一章中，我们将察看 两个用来在系统中查找文件的工具。这些工具是：locate – 通过名字来查找文件find – 在目录层次结构中搜索文件我们也将看一个经常与文件搜索命令一起使用的命令，它用来处理搜索到的文件列表：xargs – 从标准输入生成和执行命令行另外，我们将介绍两个命令来协助我们探索：touch – 更改文件时间stat – 显示文件或文件系统状态locate - 查找文件的简单方法这个 locate 程序快速搜索路径名数据库，并且输出每个与给定字符串相匹配的文件名，例如，我们想要找到所有名字以“zip”开头的程序，假定包含 匹配程序的目录以”bin/”结尾。[me@linuxbox ~]$ locate bin/ziplocate 命令将会搜索它的路径名数据库，输出任一个包含字符串“bin/zip”的路径名：/usr/bin/zip/usr/bin/zipcloak/usr/bin/zipgrep/usr/bin/zipinfo/usr/bin/zipnote/usr/bin/zipsplitlocate 可以结合其它工具，比如说 grep 命令，来设计更加 有趣的搜索：[me@linuxbox ~]$ locate zip | grep bin/bin/bunzip2/bin/bzip2/bin/bzip2recover/usr/bin/gpg-zip/usr/bin/funzip...find - 查找文件的复杂方式locate 程序只能依据文件名来查找文件，而 find 程序能基于各种各样的属性， 搜索一个给定目录（以及它的子目录），来查找文件.find 命令的最简单使用是，搜索一个或多个目录。例如，输出我们的家目录列表。[me@linuxbox ~]$ find ~对于最活跃的用户帐号，这将产生一张很大的列表,让我们使用 wc 程序来计算出文件的数量：[me@linuxbox ~]$ find ~ | wc -l47068比如说我们想要目录列表。我们可以添加以下测试条件：[me@linuxbox ~]$ find ~ -type d | wc -l这里添加测试条件-type d 限制了只搜索目录表18-1: find 文件类型| 文件类型 | 描述 ||---------|------------|| b | 块设备文件 || c | 字符设备文件 || d | 目录 || f | 普通文件 || l | 符号链接 |我们也可以通过加入一些额外的测试条件，根据文件大小和文件名来搜索：让我们查找所有文件名匹配 通配符模式“*.JPG”和文件大小大于1M 的文件：[me@linuxbox ~]$ find ~ -type f -name &quot;\*.JPG&quot; -size +1M | wc -l840在这个例子里面，我们加入了 -name 测试条件，后面跟通配符模式。注意，我们把它用双引号引起来，从而阻止 shell 展开路径名。紧接着，我们加入 -size 测试条件，后跟字符串“+1M”。开头的加号表明我们正在寻找文件大小大于指定数的文件。若字符串以减号开头，则意味着查找小于指定数的文件。 若没有符号意味着“精确匹配这个数”。结尾字母“M”表明测量单位是兆字节。find 命令支持大量不同的测试条件。下表是列出了一些常见的测试条件。请注意，在需要数值参数的 情况下，可以应用以上讨论的“+”和”-“符号表示法：表18-3: find 测试条件| 测试条件 | 描述 ||---------|------------|| -name pattern | 用指定的通配符模式匹配的文件和目录。 || -size n | 匹配的文件大小为 n。 || -type c | 匹配的文件类型是 c。 || -user name | 匹配的文件或目录属于某个用户。这个用户可以通过用户名或用户 ID 来表示。 |操作符幸运地是，find 命令提供了 一种方法来结合测试条件，通过使用逻辑操作符来创建更复杂的逻辑关系。 为了表达上述的测试条件，我们可以这样做（括号转义，不要被自动展开）：[me@linuxbox ~]$ find ~ \( -type f -not -perm 0600 \) -or \( -type d -not -perm 0700 \)| 操作符 | 描述 ||---------|------------|| -and | 如果操作符两边的测试条件都是真，则匹配。可以简写为 -a。 注意若没有使用操作符，则默认使用 -and。 || -or | 若操作符两边的任一个测试条件为真，则匹配。可以简写为 -o。 || -not | 若操作符后面的测试条件是真，则匹配。可以简写为一个感叹号（!）。 || () | 把测试条件和操作符组合起来形成更大的表达式。这用来控制逻辑计算的优先级预定义的操作幸运地是，find 命令允许基于搜索结果来执行操作表18-6: 几个预定义的 find 命令操作| 操作 | 描述 ||---------|------------|| -delete | 删除当前匹配的文件。 || -ls | 对匹配的文件执行等同的 ls -dils 命令。并将结果发送到标准输出。 || -print | 把匹配文件的全路径名输送到标准输出。如果没有指定其它操作，这是 默认操作。 || -quit | 一旦找到一个匹配，退出。 |find ~ -type f -name &apos;*.BAK&apos; -delete在这个例子里面，用户家目录（和它的子目录）下搜索每个以.BAK 结尾的文件名。当找到后，就删除它们。警告：当使用 -delete 操作时，不用说，你应该格外小心。首先测试一下命令， 用 -print 操作代替 -delete，来确认搜索结果。记住，在每个测试和操作之间会默认应用 -and 逻辑运算符。 我们也可以这样表达这个命令，使逻辑关系更容易看出：find ~ -type f -and -name &apos;*.BAK&apos; -and -print 因为测试和行为之间的逻辑关系决定了哪一个会被执行，我们知道测试和行为的顺序很重要 123456789101112用户定义的行为除了预定义的行为之外，我们也可以唤醒随意的命令。传统方式是通过 -exec 行为。这个 行为像这样工作：-exec command &#123;&#125; ;&#123;&#125;是当前路径名的符号表示，分号是要求的界定符 表明命令结束。这里是一个使用 -exec 行为的例子，其作用如之前讨论的 -delete 行为：-exec rm &apos;&#123;&#125;&apos; &apos;;&apos;重述一遍，因为花括号和分号对于 shell 有特殊含义，所以它们必须被引起来或被转义。通过使用 -ok 行为来代替 -exec，在执行每个指定的命令之前， 起提示用户作用：touch命令通常被用来设置或更新文件的访问，更改，和修改时间。然而，如果一个文件名参数是一个不存在的文件，则会创建一个空文件。注意不同于 ls 命令，find 命令的输出结果是无序的。其顺序由存储设备的布局决定stat，是一款加大马力的 ls 命令版本 第十九章：归档和备份1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798它们就是文件压缩程序：gzip – 压缩或者展开文件bzip2 – 块排序文件压缩器，另外一种压缩算法，基本和gzip一样归档程序：tar – 磁带打包工具zip – 打包和压缩文件还有文件同步程序：rsync – 同步远端文件和目录gzip这个 gzip 程序被用来压缩一个或多个文件。当执行 gzip 命令时，则原始文件的压缩版会替代原始文件。相对应的 gunzip 程序被用来把压缩文件复原为没有被压缩的版本[me@linuxbox ~]$ gzip foo.txt[me@linuxbox ~]$ gunzip foo.txt.gz接下来，我们运行 gzip 命令，它会把原始文件替换为一个叫做 foo.txt.gz 的压缩文件。接下来，我们运行 gunzip 程序来解压缩文件。随后，我们能见到压缩文件已经被原始文件替代了， 同样地保留了相同的权限和时间戳。如果我们的目标只是为了浏览一下压缩文本文件的内容，我们可以这样做：[me@linuxbox ~]$ gunzip -c foo.txt | less归档文件归档就是收集许多文件，并把它们 捆绑成一个大文件的过程。归档经常作为系统备份的一部分来使用。我们经常看到扩展名为 .tar 或者 .tgz 的文件，它们各自表示“普通” 的 tar 包和被 gzip程序压缩过的 tar 包。一个 tar 包可以由一组独立的文件，一个或者多个目录，或者 两者混合体组成。命令语法如下：tar mode[options] pathname... # 语法表19-2: tar 模式| 模式 | 说明 ||---------|------------|| c | 为文件和／或目录列表创建归档文件。 || x | 抽取归档文件，相当于归档后复制到另外一个位置。 | | r | 追加具体的路径到归档文件的末尾。 || t | 列出归档文件的内容。 |让我们创建整个操练场的 tar 包：[me@linuxbox ~]$ tar cf playground.tar playground #f选项可能是force列出归档文件的内容[me@linuxbox ~]$ tar tf playground.tar抽取 tar 包 playground 到一个新位置[me@linuxbox ~]$ cd foo #进到目标目录[me@linuxbox ~]$ tar xf ../playground.tar #xf为抽取后的文件有一个警告，然而：除非你是超级用户，要不然从归档文件中抽取的文件和目录的所有权由执行此复原操作的用户所拥有，而不属于原始所有者。tar 命令另一个有趣的行为是它处理归档文件路径名的方式。默认情况下，路径名是相对的，而不是绝对路径。假定我们想要复制家目录及其内容到另一个系统中， 并且有一个大容量的 USB 硬盘，可以把它作为传输工具[me@linuxbox ~]$ sudo tar cf /media/Big Disk/home.tar /home[me@linuxbox2 /]$ sudo tar xf /media/Big Disk/home.tarc参数为创建归档，x参数为提取归档内容，f应该是强制。第一个路径为归档后放置的路径，第二个路径是对什么内容进行归档。GNU 版本的 tar 命令（在 Linux 发行版中最常出现）通过 --wildcards 选项来 支持通配符。这个例子使用了之前 playground.tar 文件：这个命令将只会抽取匹配特定路径名的文件，路径名中包含了通配符 dir-*[me@linuxbox ~]$ cd foo[me@linuxbox foo]$ tar xf ../playground2.tar --wildcards &apos;home/me/playground/dir-\*/file-A&apos;tar 命令经常结合 find 命令一起来制作归档文件。在这个例子里，我们将会使用 find 命令来 产生一个文件集合，然后这些文件被包含到归档文件中。[me@linuxbox ~]$ find playground -name &apos;file-A&apos; -exec tar rf playground.tar &apos;&#123;&#125;&apos; &apos;+&apos;这里我们使用 find 命令来匹配 playground 目录中所有名为 file-A 的文件，然后使用-exec 行为，来 唤醒带有追加模式（r）的 tar 命令，把匹配的文件添加到归档文件 playground.tar 里面。zip这个 zip 程序既是压缩工具，也是一个打包工具，Windows 用户比较熟悉。然而，在 Linux 中 gzip 是主要的压缩程序，而 bzip2则位居第二。例如，制作一个 playground 的 zip 版本的文件包，这样做：[me@linuxbox ~]$ zip -r playground.zip playground除非我们包含-r 选项，要不然只有 playground 目录（没有任何它的内容）被存储使用 unzip 程序，来直接抽取一个 zip 文件的内容。[me@linuxbox ~]$ cd foo[me@linuxbox foo]$ unzip ../playground.zip使用-l 选项，导致 unzip 命令只是列出文件包中的内容而没有抽取文件 对于 zip 命令（与 tar 命令相反）要注意一点，就是如果指定了一个已经存在的文件包，其被更新 而不是被替代。这意味着会保留此文件包。同步文件和目录通过使用 rsync 远端更新协议，此协议 允许 rsync 快速地检测两个目录的差异，执行最小量的复制来达到目录间的同步。rsync 被这样唤醒：rsync options source destination这里 source 和 destination 是下列选项之一：一个本地文件或目录一个远端文件或目录，以[user@]host:path 的形式存在一个远端 rsync 服务器，由 rsync://[user@]host[:port]/path 指定注意 source 和 destination 两者之一必须是本地文件。rsync 不支持远端到远端的复制。下一步，我们将同步 playground 目录在 foo 目录中[me@linuxbox ~]$ rsync -av playground foo我们包括了-a 选项（递归和保护文件属性）和-v 选项（冗余输出） 第二十章：正则表达式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990grep 程序会在文本文件中查找一个指定的正则表达式，并把匹配行输出到标准输出。就像这样:[me@linuxbox ~]$ ls /usr/bin | grep zip这个命令会列出，位于目录 /usr/bin 中，文件名中包含子字符串“zip”的所有文件。这个 grep 程序以这样的方式来接受选项和参数：grep [options] regex [file...] #这里的 regx 是指一个正则表达式。这是一个常用的 grep 选项列表：表20-1: grep 选项选项 描述-i 忽略大小写。不会区分大小写字符。也可用--ignore-case 来指定。-v 不匹配。通常，grep 程序会打印包含匹配项的文本行。这个选项导致 grep 程序 只会不包含匹配项的文本行-c 打印匹配的数量（或者是不匹配的数目，若指定了-v 选项），而不是文本行本身。 -l 打印包含匹配项的文件名，而不是文本行本身-L 相似于-l 选项，但是只是打印不包含匹配项的文件名-h 应用于多文件搜索，只输出匹配项，不输出文件名（因为默认两个都会输出）[me@linuxbox ~]$ grep bzip dirlist*.txt #搜索字符串bzipdirlist-bin.txt:bzip2dirlist-bin.txt:bzip2recover[me@linuxbox ~]$ grep -l bzip dirlist*.txt # 只输出包含匹配项的文件名dirlist-bin.txt元字符和文本表达式元字符由以下字符组成：^ $ . [ ] &#123; &#125; - ? * + ( ) | \ 然后其它所有字符都被认为是原义字符圆点字符，其被用来匹配任意字符[me@linuxbox ~]$ grep -h &apos;.zip&apos; dirlist*.txtbunzip2bzip2插入符号和美元符号被看作是锚点，表示以**开头和以**结尾[me@linuxbox ~]$ grep -h &apos;^zip&apos; dirlist*.txtzipzipcloak[me@linuxbox ~]$ grep -h &apos;zip$&apos; dirlist*.txtgunzipgzip通过中括号表达式，我们能够指定 一个字符集合[me@linuxbox ~]$ grep -h &apos;[bg]zip&apos; dirlist*.txtbzip2recovergzip否定，如果在正则表示式中的第一个字符是一个插入字符，则剩余的字符被看作是不会在给定的字符位置出现的字符集合[me@linuxbox ~]$ grep -h &apos;[^bg]zip&apos; dirlist*.txt #zip之前的字符一定不是b或者gbunzip2gunzip字符区域[me@linuxbox ~]$ grep -h &apos;^[A-Z]&apos; dirlist*.txtMAKEDEV扩展的正则表达式正则表达式的实现分成了两类： 基本正则表达式（BRE）和扩展的正则表达式（ERE）,因为我们将要讨论的下一个特性是 ERE 的一部分,由 egrep 程序来执行这项操作，但是 GUN 版本的 grep 程序也支持扩展的正则表达式，当使用了-E 选项之后。Alternation我们将要讨论的扩展表达式的第一个特性叫做 alternation（交替），其是一款允许从一系列表达式 之间选择匹配项的实用程序。[me@linuxbox ~]$ echo &quot;AAA&quot; | grep AAA #左边传过来的和右边匹配AAA[me@linuxbox ~]$ echo &quot;BBB&quot; | grep AAA[me@linuxbox ~]$一个相当直截了当的例子，我们把 echo 的输出管道给 grep，然后看到输出结果。当出现 一个匹配项时，我们看到它会打印出来；当没有匹配项时，我们看到没有输出结果。现在我们将添加 alternation，以竖杠线元字符为标记：[me@linuxbox ~]$ echo &quot;AAA&quot; | grep -E &apos;AAA|BBB&apos;AAA[me@linuxbox ~]$ echo &quot;BBB&quot; | grep -E &apos;AAA|BBB&apos;BBB[me@linuxbox ~]$ echo &quot;CCC&quot; | grep -E &apos;AAA|BBB&apos;[me@linuxbox ~]$这里我们看到正则表达式’AAA|BBB’，这意味着“匹配字符串 AAA 或者是字符串 BBB”。注意因为这是 一个扩展的特性，我们给 grep 命令（虽然我们能以 egrep 程序来代替）添加了-E 选项，并且我们 把这个正则表达式用单引号引起来，为的是阻止 shell 把竖杠线元字符解释为一个 pipe 操作符。限定符扩展的正则表达式支持几种方法，来指定一个元素被匹配的次数。? - 匹配零个或一个元素* - 匹配零个或多个元素+ - 匹配一个或多个元素&#123; &#125; - 匹配特定个数的元素表20-3: 指定匹配的数目| 限定符 | 意思 ||-------|-------|| &#123;n&#125; | 匹配前面的元素，如果它确切地出现了 n 次。 || &#123;n,m&#125; | 匹配前面的元素，如果它至少出现了 n 次，但是不多于 m 次。 || &#123;n,&#125; | 匹配前面的元素，如果它出现了 n 次或多于 n 次。 || &#123;,m&#125; | 匹配前面的元素，如果它出现的次数不多于 m 次。 |]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Linux Command Line学习笔记（二）]]></title>
    <url>%2F2018%2F06%2F10%2FLinux%2FThe%20Linux%20Command%20Line%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[第十一章：进程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990这一章将介绍以下命令：这一章将介绍以下命令：ps – 报告当前进程快照top – 显示任务jobs – 列出活跃的任务bg – 把一个任务放到后台执行fg – 把一个任务放到前台执行kill – 给一个进程发送信号killall – 杀死指定名字的进程shutdown – 关机或重启系统系统分配给每个进程一个数字，这个数字叫做进程ID 或 PID。PID 号按升序分配，init 进程的 PID 总是1。查看进程 ps。ps 程序有许多选项，它最简单地使用形式是这样的：[me@linuxbox ~]$ psPID TTY TIME CMD5198 pts/1 00:00:00 bash10129 pts/1 00:00:00 psTTY 是“Teletype” 的简写，是指进程的控制终端，TIME 字段表示 进程所消耗的CPU 时间数量，CMD代表运行的命令[me@linuxbox ~]$ ps x # 可以得到更多关于系统运行状态的信息：输出结果中，新添加了一栏，标题为 STAT 。STAT 是 “state” 的简写，它揭示了进程当前状态：表11-1: 进程状态| 状态 | 意义 || --- | --- || R | 运行。这意味着，进程正在运行或准备运行。 || S | 正在睡眠。 进程没有运行，而是，正在等待一个事件， 比如说，一个按键或者网络数据包。 || D | 不可中断睡眠。进程正在等待 I/O，比方说，一个磁盘驱动器的 I/O。 || T | 已停止. 已经指示进程停止运行。稍后介绍更多。 || Z | 一个死进程或“僵尸”进程| &lt; | 一个高优先级进程| N | 低优先级进程。另一个流行的选项组合是 “aux”（不带开头的”-“字符）。这会给我们更多信息：[me@linuxbox ~]$ ps aux用 top 命令动态查看进程[me@linuxbox ~]$ toptop 显示结果由两部分组成： 最上面是系统概要，下面是进程列表，以 CPU 的使用率排序。输入q退出top程序。把一个进程放置到后台(执行)启动一个程序，让它立即在后台 运行，我们在程序命令之后，加上”&amp;”字符：[me@linuxbox ~]$ xlogo &amp; #xlogo打开一个试验窗口[1] 28236 # 工作号为1[me@linuxbox ~]$如果我们运行 ps 命令，可以看到我们的进程：[me@linuxbox ~]$ psPID TTY TIME CMD10603 pts/1 00:00:00 bash28236 pts/1 00:00:00 xlogo28239 pts/1 00:00:00 ps执行 jobs 命令，我们可以看到这个输出列表：[me@linuxbox ~]$ jobs[1]+ Running xlogo &amp;进程返回到前台一个在后台运行的进程对一切来自键盘的输入都免疫，也不能用 Ctrl-c 来中断它。使用 fg 命令，让一个进程返回前台执行：[me@linuxbox ~]$ jobs[1]+ Running xlogo &amp;[me@linuxbox ~]$ fg %1xlogofg 命令之后，跟随着一个百分号和工作序号，输入 Ctrl-c 来终止 xlogo 程序停止一个进程，输入 Ctrl-z，可以停止一个前台进程。使用 fg 命令，可以恢复程序到前台运行，或者用 bg 命令把程序移到后台。Signalskill 命令被用来“杀死”程序。这样我们就可以终止需要杀死的程序。指定我们想要终止的进程 PID。也可以用 jobspec（例如，“％1”）来代替 PID。[me@linuxbox ~]$ xlogo &amp;[1] 28401[me@linuxbox ~]$ kill 28401[1]+ Terminated xlogo这个 kill 命令不是确切地“杀死”程序，而是给程序 发送信号。当程序 接到信号之后，则做出响应。通过 kill 命令给进程发送信号，kill 命令被用来给程序发送信号。它最常见的语法形式看起来像这样（默认TERM终止信号）：kill [-signal] PID...表 11-4: 常用信号| 编号 | 名字 | 含义 || 1 | HUP | 挂起，这意味着，当发送这个信号到一个守护进程后， 这个进程会重新启动，并且重新读取它的配置文件| 2 | INT | 中断。实现和 Ctrl-c 一样的功能，由终端发送。通常，它会终止一个程序。 || 15 | TERM | 终止。这是 kill 命令发送的默认信号| 18 | CONT | 继续。在停止一段时间后，进程恢复运行。 || 19 | STOP | 停止。这个信号导致进程停止运行，而没有终止例如：[me@linuxbox ~]$ kill -1 13546 #挂起 工作号为13546的进程 第十二章：shell环境1234567891011121314151617181920212223242526272829303132333435363738394041424344在这一章，我们将用到以下命令：printenv - 打印部分或所有的环境变量set - 设置 shell 选项export — 导出环境变量，让随后执行的程序知道。alias - 创建命令别名shell 在环境中存储了两种基本类型的数据，虽然对于 bash 来说，很大程度上这些类型是不可 辨别的。它们是环境变量和 shell 变量。Shell 变量是由 bash 存放的少量数据，而剩下的基本上 都是环境变量[me@linuxbox ~]$ printenv | less #显示环境变量非登录 shell 会话也会继承它们父进程的环境设置，通常是一个登录 shell曾经是否感到迷惑 shell 是怎样知道到哪里找到我们在命令行中输入的命令的？例如，当我们输入 ls 后，shell 不会查找整个计算机系统，来找到 /bin/ls（ls 命令的绝对路径名），而是，它查找一个目录列表，这些目录包含在 PATH 变量中。通过添加字符串 $HOME/bin 到 PATH 变量值的末尾，则目录 $HOME/bin 就添加到了命令搜索目录列表中。这意味着当我们想要在自己的家目录下，创建一个目录来存储我们自己的私人程序时，shell 已经给我们准备好了。我们所要做的事就是 把创建的目录叫做 bin按照通常的规则，添加目录到你的 PATH 变量或者是定义额外的环境变量，要把这些更改放置到.bash_profile 文件中. 对于其它的更改，要放到 .bashrc 文件中。(修改通过vi编辑器)移动 鼠标到文件的最后一行，然后添加以下几行到文件 .bashrc 中：umask 0002export HISTCONTROL=ignoredupsexport HISTSIZE=1000alias l.=&apos;ls -d .* --color=auto&apos;alias ll=&apos;ls -l --color=auto&apos;| 文本行 | 含义 ||-------|---------|| umask 0002 | 设置掩码来解决共享目录的问题。 || export HISTCONTROL=ignoredups | 使得 shell 的历史记录功能忽略一个命令，如果相同的命令已被记录。 || export HISTSIZE=1000 | 增加命令历史的大小，从默认的 500 行扩大到 1000 行。 || alias l.=&apos;ls -d .* --color=auto&apos; | 创建一个新命令，叫做&apos;l.&apos;，这个命令会显示所有以点开头的目录项。 || alias ll=&apos;ls -l --color=auto&apos; | 创建一个叫做&apos;ll&apos;的命令，这个命令会显示长格式目录列表。 |激活我们的修改我们对于文件 .bashrc 的修改不会生效，直到我们关闭终端会话，再重新启动一个新的会话， 因为.bashrc 文件只是在刚开始启动终端会话时读取。然而，我们可以强迫 bash 重新读取修改过的 .bashrc文件，使用下面的命令：[me@linuxbox ~]$ source .bashrc 第十三章：VI简介12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[me@linuxbox ~]$ vi #启动vi:q! #加感叹号是强制退出小贴示：如果你在 vi 中“迷失”了，试着按下 Esc 键两次来找到路（回到普通模式）。编辑模式再次启动 vi，这次传递给 vi 一个不存在的文件名。这也是用 vi 创建新文件的方法。[me@linuxbox ~]$ vi foo.txt当 vi 启动后，进入 的是命令模式。这种模式下，几乎每个按键都是一个命令，所以如果我们打算输入字符，vi 会发疯，弄得一团糟。插入模式为了在文件中添加文本，首先我们必须进入插入模式。按下”i”按键进入插入模式，按下 Esc 按键，返回命令模式。-- INSERT --保存我们的工作在命令模式下输入一个 ex 命令（ex操作为命令模式下的操作）。 通过按下”:”键。为了写入我们修改的文件，我们在冒号之后输入”w”字符，然后按下回车键:w移动光标当在 vi 命令模式（不是插入模式）下时，vi 提供了大量的移动命令| 0 (零按键) | 移动到当前行的行首。 || $ | 移动到当前行的末尾。 || Ctrl-f or Page Down | 向下翻一页 || Ctrl-b or Page Up | 向上翻一页 || G | 移动到文件末尾。 |基本编辑如果我们按下“u” 按键，当在命令模式下，vi 将会撤销你所做的最后一次修改如果我们想要在这个句子的末尾添加一些文本，输入”a”，在光标处，vi 进入插入模式。”A”命令，在行尾进入插入模式。另外还有：| o | 当前行的下方打开一行。 |，进入插入模式| O | 当前行的上方打开一行。 | ，进入插入模式记住按下 Esc 按键来退出插入模式。删除文本表13-3: 文本删除命令| 命令 | 删除的文本 ||-------|---------|| x | 当前字符 || 3x | 当前字符及其后的两个字符。 || dd | 当前行。 || 5dd | 当前行及随后的四行文本。 || d$ | 从光标位置开始到当前行的行尾。 || d0 | 从光标位置开始到当前行的行首。 |剪切，复制和粘贴文本这个 d 命令不仅删除文本，它还“剪切”文本。每次我们使用 d 命令，删除的部分被复制到一个 粘贴缓冲区中（看作剪切板）。过后我们执行小 p 命令把剪切板中的文本粘贴到光标位置之后， 或者是大 P 命令把文本粘贴到光标之前。y 命令用来“拉”（复制）文本。p命令粘贴。表13-4: 复制命令| 命令 | 复制的内容 ||-------|---------|| yy | 当前行。 || 5yy | 当前行及随后的四行文本。 || y$ | 从当前光标位置到当前行的末尾。 || y0 | 从当前光标位置到行首。 |查找和替换查找一行： f 命令查找一行，移动光标到下一个所指定的字符上。例如，命令 fa 会把光标定位到同一行中 下一个出现的”a”字符上。在一行中执行了字符的查找命令之后，通过输入分号来重复这个查找。查找整个文件：移动光标到下一个出现的单词或短语上，使用 / 命令。下一步，输入要查找的单词或短语后， 按下回车。光标就会移动到下一个包含所查找字符串的位置。通过 n 命令来重复先前的查找 全局查找和替代vi 使用 ex 命令来执行查找和替代操作（vi 中叫做“替换”）。把整个文件中的单词“Line”更改为“line”， 我们输入以下命令： 1:%s/Line/line/g 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263需要用户确认的替换命令。通过添加一个”c”字符到这个命令的末尾，来完成 这个替换命令。例如：:%s/line/Line/gc这个命令会把我们的文件恢复先前的模样；然而，在执行每个替换命令之前，vi 会停下来， 通过下面的信息，来要求我们确认这个替换：replace with Line (y/n/a/q/l/^E/^Y)?表13-5: 替换确认按键| 按键 | 行为 ||-------|---------|| y | 执行替换操作 || n | 跳过这个匹配的实例 || a | 对这个及随后所有匹配的字符串执行替换操作。 || q or esc | 退出替换操作。 || l | 执行这次替换并退出。l 是 “last” 的简写。 || Ctrl-e, Ctrl-y | 分别是向下滚动和向上滚动。用于查看建议替换的上下文。 |文件之间转换从这个文件转到下一个文件，使用这个 ex 命令：:n回到先前的文件使用：:N当我们从一个文件移到另一个文件时，如果当前文件没有保存修改，vi 会阻止我们转换文件， 这是 vi 强制执行的政策。在命令之后添加感叹号，可以强迫 vi 放弃修改而转换文件。我们可以查看正在编辑的文件列表，使用:buffers 命令:buffers1 # &quot;foo.txt&quot; line 12 %a &quot;ls-output.txt&quot; line 0Press ENTER or type command to continue注意：你不同通过:n 或:N 命令在由:e 命令加载的文件之间进行切换。这时要使用:buffer 命令， 其后加上缓冲区号码，来转换文件。从一个文件复制内容到另一个文件以打开的两个文件为例，首先转换到缓冲区1（foo.txt） ，输入：:buffer 1我们应该得到以下输出：The quick brown fox jumped over the lazy dog. It was cool.Line 2Line 3Line 4Line 5下一步，把光标移到第一行，并且输入 yy 来复制这一行。转换到第二个缓冲区，输入：:buffer 2现在屏幕会包含一些文件列表（这里只列出了一部分）：total 343700-rwxr-xr-x 1 root root 31316 2007-12-05 08:58 [....移动光标到第一行，输入 p 命令把我们从前面文件中复制的一行粘贴到这个文件中：total 343700The quick brown fox jumped over the lazy dog. It was cool.-rwxr-xr-x 1 root root 31316 2007-12-05 08:58 [....保存工作像 vi 中的其它操作一样，有几种不同的方法来保存我们所修改的文件。我们已经研究了:w 这个 ex 命令，但还有几种方法，可能我们也觉得有帮助。在命令模式下，输入 ZZ 就会保存并退出当前文件 第十四章：自定制shell提示符并不是每个人都会花心思来更改提示符，因为通常默认的提示符就很让人满意 第十五章：软件包管理12345678910111213141516171819202122232425262728293031软件包管理是指系统中一种安装和维护软件的方法，inux 系统中几乎所有的软件都可以在互联网上找到。其中大多数软件由发行商以 包文件的形式提供，剩下的则以源码形式存在，可以手动安装。在后面章节里，我们将会谈谈怎样 通过编译源码来安装软件。查找资源库中的软件包| Red Hat | yum search search_string |例如：yum search emacs从资源库中安装一个软件包| Red Hat | yum install package_name |通过软件包文件来安装软件如果从某处而不是从资源库中下载了一个软件包文件，可以使用底层工具来直接（没有经过依赖解析）安装它。| Red Hat | rpm -i package_file |卸载软件| Red Hat | yum erase package_name |经过资源库来更新软件包| Red Hat | yum update |经过软件包文件来升级软件如果已经从一个非资源库网站下载了一个软件包的最新版本，可以安装这个版本，用它来 替代先前的版本：| Red Hat | rpm -U package_file |列出所安装的软件包| Red Hat | rpm -qa |确定是否安装了一个软件包| Red Hat | rpm -q package_name |查找安装了某个文件的软件包| Red Hat | rpm -qf file_name |]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Linux Command Line学习笔记（一）]]></title>
    <url>%2F2018%2F06%2F07%2FLinux%2FThe%20Linux%20Command%20Line%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[记录读《The Linux Command Line》的学习笔记，因为之前是记在印象笔记里面的，格式有点对不上，这里内容就全部粘贴到代码框里面。 另外一篇此书的学习笔记博客 文件查看1`pwd — 打印出当前工作目录名` 1`ls — 列出目录内容ls -l # 长模式ls -l # 修改时间排序ls -a # 显示所用文件，默认不显示隐藏文件ls -S # 按大小排序me@linuxbox ~]$ ls /usr # 除了当前工作目录以外，也可以指定要列出内容的目录[me@linuxbox ~]$ ls ~ /usr #列出多个指定目录的内容 ,家目录（用字符“~”代表）` 12345cd — 更改目录符号 “.” 指的是工作目录（找下一层使用，一般可省略使用），”..” 指的是工作目录的父目录（找上一层使用）| cd | 更改工作目录到你的家目录。 || cd - | 更改工作目录到先前的工作目录。 || cd ~user_name | 更改工作目录到用户家目录。例如, cd ~bob 会更改工作目录到用户“bob”的家目 1234linux一切皆文件，不依赖后缀名判断使用 file 打印文件描述me@linuxbox ~]$ file picture.jpgpicture.jpg: JPEG image data, JFIF standard 1.01 123456less浏览文本文件内容，以下是常用命令| G | 移动到最后一行 || 1G or g | 移动到开头一行 || /charaters | 向前查找指定的字符串 || q | 退出 less 程序 || h | 显示帮助屏幕 | 文件操作123456接受文件名作为参数的任何命令，都可以使用通配符，要记得一些简单的规则（类似正则）cp item1 item2 — 复制文件和目录mv item1 item2— 移动/重命名文件和目录mkdir dir1 — 创建目录rm item1 — 删除文件和目录（一旦删除不可恢复，慎重使用）ln — 创建硬链接和符号链接 12345678910这里列举了 cp 命令一些有用的选项（短选项和等效的长选项）：其他操作也类同| -a, --archive | 复制文件和目录，以及它们的属性，包括所有权和权限。| -i, --interactive | 在重写已存在文件之前，提示用户确认| -r, --recursive | 递归地复制目录及目录中的内容。当复制目录时， 需要这个选项（或者-a 选项）。 || -u, --update | 当把文件从一个目录复制到另一个目录时，仅复制 目标目录中不存在的文件，或者是文件内容新于目标目录中已经存在的文件。 || -v, --verbose | 显示翔实的命令操作信息 |rm命令参数选项：| -f, --force | 忽视不存在的文件，不显示提示信息。这选项颠覆了“--interactive”选项。 | 1`ln 命令即可创建硬链接，也可以创建符号链接。` 1`重命名：[me@linuxbox playground]$ mv oldname newname` 使用命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950type – 说明怎样解释一个命令名which – 显示会执行哪个可执行程序man – 显示命令手册页 ，完整正式又复杂apropos – 显示一系列适合的命令，基于某个关键字的匹配项搜索info – 显示命令 infowhatis – 显示一个命令的简洁描述alias – 创建命令别名到底什么是命令？命令可以是下面四种形式之一：1. 是一个可执行程序，比如说shell，perl，python，ruby，等等。2. 是一个内建于 shell 自身的命令。cd 命令，就是一个 shell 内部命令。3. 是一个 shell 函数。4. 是一个命令别名。[me@linuxbox ~]$ type lsls is aliased to `ls --color=tty`[me@linuxbox ~]$ which ls/bin/ls[me@linuxbox ~]$ help cdcd: cd [-L|-P] [dir] # 方括号，表示可选的项目。一个竖杠字符 表示互斥选项[me@linuxbox ~]$ mkdir --help # 显示命令所支持的语法和选项说明Usage: mkdir [OPTION] DIRECTORY...[me@linuxbox ~]$ man ls # 手册页的组织形式| 章节 | 内容 |，可以指定章号[me@linuxbox ~]$ apropos floppy #以”floppy”为关键词来搜索参考手册的create_floppy_devices (8) - udev callout to create all possible #输出结果每行的第一个字段是手册页的名字，第二个字段展示章节。注意，man 命令加上”-k”选项，和 apropos 完成一样的功能。许多手册页都很难阅读，whatis 程序显示匹配特定关键字的手册页的名字和一行命令说明。info － GNU 项目提供了一个命令程序手册页的替代物，称为”info”，输入”info”，接着输入程序名称，启动 info。下表中的命令，当显示一个 info 页面时， 用来控制阅读器。表 6-2: info 命令| 命令 | 行为 || --- | --- || ? | 显示命令帮助 || Pg Up or Backspace | 显示上一页 || Pg Dn or Space | 显示下一页 || n | 下一个 - 显示下一个结点 || p | 上一个 - 显示上一个结点 || u | Up - 显示当前所显示结点的父结点，通常是个菜单 || Enter | 激活光标位置下的超级链接 || q | 退出 |[me@linuxbox ~]$ type test # 检测新命名是否与系统重名test is a shell builtin #重名，需要换一个[me@linuxbox ~]$ type foo # 检测bash: type: foo: not found # 不重名，可以使用这个新命名[me@linuxbox ~]$ alias foo=&apos;cd /usr; ls; cd -&apos; #通过 alia 命令 把这一串命令转变为一个命令注意命令结构：alias name=&apos;string&apos;。在命令”alias”之后，输入“name”，紧接着（没有空格）是一个等号，等号之后是 一串用引号引起的字符串，字符串的内容要赋值给 name删除别名，使用 unalias 命令，像这样：[me@linuxbox ~]$ unalias foo[me@linuxbox ~]$ type foobash: type: foo: not found IO重定向1`重定向命令的输入输出，命令的输入来自文件，而输出也存到文件。 也可以把多个命令连接起来组成一个强大的命令管道。为了炫耀这个工具，我们将叙述 以下命令：` 第八章：从shell眼中看世界12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879echo 是一个 shell 内部命令，来完成非常简单的任务。 它在标准输出中打印出它的文本参数。[me@linuxbox ~]$ echo this is a testthis is a test[me@linuxbox ~]$ echo *Desktop Documents ls-output.txt Music Pictures Public Templates Videos为什么 echo 不打印“*”呢，最简单的答案就是在 echo 命令被执行前， shell 把“*”展开成了 另外的东西（在这种情况下，就是在当前工作目录下的文件名字）类似还有[me@linuxbox ~]$ echo this is a testthis is a test[me@linuxbox ~]$ echo The total is $100.00The total is 00.00 # $1的值替换为一个空字符串，因为 1 是没有定义的变量路径名展开这种通配符（即*）工作机制叫做路径名展开（当前工作目录下的文件名字）[me@linuxbox ~]$ echo D*Desktop Documents #列出当前工作目录下的D开头的文件名字波浪线展开波浪线字符(“~”)有特殊的意思。当它用在 一个单词的开头时，它会展开成指定用户的家目录名，如果没有指定用户名，则是当前用户的家目录：[me@linuxbox ~]$ echo ~/home/me算术表达式展开shell 允许算术表达式通过展开来执行。这允许我们把 shell 提示当作计算器来使用：[me@linuxbox ~]$ echo $((2 + 2))4| / | 除（取整数除法，所以结果是整数。） || % | 取余，只是简单的意味着，“余数” || ** | 取幂 |[me@linuxbox ~]$ echo Five divided by two equals $((5/2))Five divided by two equals 2[me@linuxbox ~]$ echo with $((5%2)) left over.with 1 left over.花括号展开可能最奇怪的展开是花括号展开。通过它，你可以从一个包含花括号的模式中 创建多个文本字符串。例子：[me@linuxbox ~]$ echo Front-&#123;A,B,C&#125;-BackFront-A-Back Front-B-Back Front-C-Back[me@linuxbox ~]$ echo &#123;Z..A&#125; #可以一系列整数和字符Z Y X W V U T S R Q P O N M L K J I H G F E D C B A[me@linuxbox ~]$ echo a&#123;A&#123;1,2&#125;,B&#123;3,4&#125;&#125;b #嵌套a A1b a A2b a B3b a B4b命令替换命令替换允许我们把一个命令的输出作为一个展开模式来使用：me@linuxbox ~]$ echo $(ls)Desktop Documents ls-output.txt Music Pictures Public Templates Videos[me@linuxbox ~]$ ls -l $(which cp) #把 which cp的执行结果作为一个参传递给ls命令-rwxr-xr-x 1 root root 71516 2007-12-05 08:58 /bin/cp双引号、单引号双意味着单词分割，路径名展开， 波浪线展开，和花括号展开都被禁止，然而参数展开，算术展开，和命令替换 仍然执行。单引号所有都被禁止。[me@linuxbox ~]$ echo text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USERtext /home/me/ls-output.txt a b foo 4 me[me@linuxbox ~]$ echo &quot;text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER&quot;text ~/*.txt &#123;a,b&#125; foo 4 me[me@linuxbox ~]$ echo &apos;text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER&apos;text ~/*.txt &#123;a,b&#125; $(echo foo) $((2+2)) $USER转义字符&apos;\&apos;| \a | 响铃（”警告”－导致计算机嘟嘟响） || \b | 退格符 || \n | 新的一行。在类 Unix 系统中，产生换行。 || \r | 回车符 || \t | 制表符 | 第九章：键盘高级操作技巧1`clear － 清空屏幕` 第十章：权限1`在这一章中，我们将看看这一系统安全的本质部分，会介绍以下命令：` chmod － 更改文件模式chmod 命令支持两种不同的方法来改变文件模式：1、八进制数字表示法，或 2、 符号表示法。这两种方法都有人用。首先我们讨论一下八进制数字表示法。 123456789101112131415161718192021通过使用3个八进制数字，我们能够设置文件所有者，用户组，和其他人的权限：[me@linuxbox ~]$ chmod 600 foo.txt通过传递参数 “600”，我们能够设置文件所有者的权限为读写权限，而删除用户组和其他人的所有 权限chmod 命令还支持另一种符号表示法，通过字符 “u”，“g”，“o”，和 “a” 的组合来指定 要影响的对象。如果没有指定字符，则假定使用”all”表10-4: chmod 命令符号表示法||||-----|-----------|| u | &quot;user&quot;的简写，意思是文件或目录的所有者。 || g | 用户组。 || o | &quot;others&quot;的简写，意思是其他所有的人。 || a | &quot;all&quot;的简写，是&quot;u&quot;, &quot;g&quot;和“o”三者的联合。 |执行的操作可能是一个“＋”字符，表示加上一个权限， 一个“－”，表示删掉一个权限，或者是一个“＝”，表示只有指定的权限可用，其它所有的权限被删除。表10-5: chmod 符号表示法实例||||-----|-----------|| u+x | 为文件所有者添加可执行权限。 || u-x | 删除文件所有者的可执行权限。 || +x | 为文件所有者，用户组，和其他所有人添加可执行权限。 等价于 a+x。 || o-rw | 除了文件所有者和用户组，删除其他人的读权限和写权限。 || go=rw | 给群组的主人和任意文件拥有者的人读写权限。如果群组的主人或全局之前已经有了执行的权限，他们将被移除。 1234567umask － 设置默认权限[me@linuxbox ~]$ rm -f foo.txt[me@linuxbox ~]$ umask0002运行不带参数的 umask 命令， 看一下当前的掩码值。响应的数值是0002（0022是另一个常用值），这个数值是掩码的八进制表示形式，以下是掩码0002的八进制表达形式 12345这次自己设置掩码[me@linuxbox ~]$ rm foo.txt[me@linuxbox ~]$ umask 0000大多数情况下，你不必修改掩码值，系统提供的默认掩码值就很好了 1`更改身份`]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（十）：统计模型]]></title>
    <url>%2F2018%2F05%2F26%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[R语言基本统计分析方法（包及函数） kaggle R语言实战 —House Prices 其他博主的学习笔记]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（九）：编写函数]]></title>
    <url>%2F2018%2F05%2F26%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%BC%96%E5%86%99%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1`一个函数是通过下面的语句形式定义的，` 1`%anything %将以二元操作符（binary operator）的形式在表达式中使用，而不是函数的形式。例如我们用! 作为中间的字符。函数可以如下定义` 1`注意任何在函数内部的普通赋值都是局部的暂时的，当退出函数时都会丢失。如果想在一个函数里面全局赋值或者永久赋值，可以采用“强赋值”（superassignment）操作符&lt;&lt;- 或者采用函数assign()` 1`在众多泛型函数中，plot() 用于图形化显示对象，summary()用于各种类型的概述分析，以及anova() 用于比较统计模型。可以用函数methods() 得到当前对某个类对象可用的泛型函数列表.`]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（八）：概率分布和条件语句]]></title>
    <url>%2F2018%2F05%2F24%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E5%92%8C%E6%9D%A1%E4%BB%B6%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[R 的统计表 不同的名字前缀表示不同的含义，d表示概率密度函数，p 表示累积分布函数（cumulative distribution function，CDF），q 表示分位函数以及r 表示随机模拟(random deviates)或者随机数发生器 123456789我们可以用很多方法分析一个单变量数据集的分布。最简单的办法就是直接看数字。利用函数summary 和fivenum 会得到两个稍稍有点差异的汇总信息。 此外，stem(“茎叶”图)也会反映整个数据集的数字信息。用函数hist 绘制柱状图。&gt; hist(eruptions)&gt; ## 让箱距缩小，绘制密度图&gt; hist(eruptions, seq(1.6, 5.2, 0.2), prob=TRUE) # seq为范围和箱距，prob为T时显示频率，F时为频数&gt; lines(density(eruptions, bw=0.1))&gt; rug(eruptions) # 显示实际的数据点更为精致的密度图是用函数density 绘制的。在这个例子中，我们加了一条由density 产生的曲线。你可以用试错法（trial-and-error）选择带宽bw（bandwidth）因为默认的带宽值让密度曲线过于平滑 函数ecdf 绘制一个数据集的经验累积分布（empirical cumulativedistribution）函数。 分位比较图（Quantile-quantile (Q-Q) plot），又称QQ图，通常用于检验是否满足正态分布 1234567891011121314151617181920# 检验是否符合正态分布norm.test&lt;- function(input.data,alpha=0.05,pic=TRUE)&#123; if(pic==TRUE)&#123;#画图形 dev.new() # 打开一个新的图形窗口 par(mfrow=c(2,1)) # 一个图版显示2行,3列 qqnorm(input.data,main=&quot;qq图&quot;) qqline(input.data) # 画出QQ图 hist(input.data,frep=F,main=&quot;直方图和密度估计曲线&quot;) lines(density(input.data),col=&quot;blue&quot;) #密度估计曲线 x&lt;- c(round(min(input.data)):round(max(input.data))) lines(x,dnorm(x,mean(input.data),sd(input.data)),col=&quot;red&quot;) #正态分布曲线 &#125; sol&lt;- shapiro.test(input.data) if(sol$p.value&gt;alpha)&#123; print(paste(&quot;success:服从正态分布,p.value=&quot;,sol$p.value,&quot;&gt;&quot;,alpha)) &#125;else&#123; print(paste(&quot;error:不服从正态分布,p.value=&quot;,sol$p.value,&quot;&lt;=&quot;,alpha)) &#125; sol&#125; 1234567891011121314利用Shapiro-Wilk方法进行正态检验&gt; shapiro.test(long)Shapiro-Wilk normality testdata: longW = 0.9793, p-value = 0.01052检验结果，因为W接近1，p值大于0.05，所以数据为正态分布利用R自带的Kolmogorov-Smirnov 检验，Kolmogorov-Smirnov检验需要三个输入变量，及数据本身、均值及标准差&gt; ks.test(long, &quot;pnorm&quot;, mean = mean(long), sd = sqrt(var(long)))One-sample Kolmogorov-Smirnov testdata: longD = 0.0661, p-value = 0.4284alternative hypothesis: two.sided检验结果，因为p值大于0.05，所以数据为正态分布 123456789到现在为止，我们已经学会了单样本的正态性检验。而更常见的操作是比较两个样本的特征。在R 里面，所有“传统”的检验都放在包stats 里面。这个包常常会自动载入。盒状图（boxplot）为这两组数据提供了简单的图形比较。A &lt;- scan()79.98 80.04 80.02 80.04 80.03 80.03 80.04 79.9780.05 80.03 80.02 80.00 80.02B &lt;- scan()80.02 79.94 79.98 79.97 79.97 80.03 79.95 79.97boxplot(A, B) 箱型图解释 通俗理解T检验和F检验 简单来说，通过把所得到的统计检定值与统计学家建立了一些随机变量的概率分布进行比较，证明样本的统计结果不是随机得到的，是有意义的。专业上，P值或sig值为结果可信程度的一个递减指标，如p=0.05提示样本中变量关联有5%的可能是由于偶然性造成的。即假设总体中任意变量间均无关联。 1234567891011121314R 语言的条件语句形式为&gt; if (expr1) expr2else expr3R 语言有下面形式的for 循环架构&gt; for (name in expr 1 ) expr 2其他循环语句包括&gt; repeat expr和语句&gt; while (condition) expr关键字break可以用于结束任何循环，甚至是非常规的。它是结束repeat 循环的唯一办法。关键字next 可以用来结束一次特定的循环，然后直接跳入“下一次”循环]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（七）：从文件中读取数据]]></title>
    <url>%2F2018%2F05%2F22%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%BB%8E%E6%96%87%E4%BB%B6%E4%B8%AD%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[12345678910111213141516read.table()函数为了可以直接读取整个数据框，外部文件常常要求有特定的格式。• 第一行可以有该数据框各个变量的名字。• 随后的行中第一个条目是行标签，其他条目是各个变量的值。有列名字和行标签的输入文件格式：Price Floor Area Rooms Age Cent.heat01 52.00 111.0 830 5 6.2 no02 54.75 128.0 710 5 7.5 no03 57.50 101.0 1000 5 4.2 no04 57.50 131.0 690 6 8.8 no05 59.75 93.0 900 5 1.9 yes...忽略行索引/行标签，可用如下的命令&gt; House Price &lt;- read.table(&quot;houses.data&quot;, header=TRUE) 1`假定有三个数据向量` 12data() 访问内部标准数据集为了访问某个特定R包的数据，可以使用参数package，例如data(package=&quot;rpart&quot;) 1`用edit 调用数据框和矩阵时，R 会产生一个电子表形式的编辑环境。这对在数据集上进行小的修改时非常有用的。它的命令是&gt; xnew &lt;- edit(xold)`]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（六）：列表和数据框]]></title>
    <url>%2F2018%2F05%2F22%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E5%88%97%E8%A1%A8%E5%92%8C%E6%95%B0%E6%8D%AE%E6%A1%86%2F</url>
    <content type="text"><![CDATA[123下面的例子演示怎么创建一个列表：&gt; Lst &lt;- list(name=&quot;Fred&quot;, wife=&quot;Mary&quot;, no.children=3,child.ages=c(4,7,9)) 1`分量常常会被编号的（numbered），并且可以利用这种编号来访问分量。如果列表 Lst 有四个分量，这些分量则可以用Lst[[1]], Lst[[2]], Lst[[3]] 和Lst[[4]]独立访问。如果Lst[[4]] 是一个有下标的数组，那么Lst[[4]][1] 就是该数组的第一个元素。` 12345678910111213列表的分量可以被命名，这种情况下可以通过名字访问。此时，可以把字符串形式的分量名字放在列表名后面的双中括号中，或者干脆采用下面的形式&gt; name$component name此在上面给定的例子中，Lst$name 和Lst[[1]] 返回结果都是&quot;Fred&quot;,Lst$wife 和Lst[[2]] 返回的则是&quot;Mary&quot;,而Lst$child.ages[1] 和Lst[[4]][1] 返回一样的数字4。另外你同样可以在双中括号中使用列表分量的名字，即和Lst$name 等价的Lst[[&quot;name&quot;]]这里特别要注意一下Lst[[1]] 和Lst[1] 的差别。[[. . . ]] 是用来选择单个元素的操作符，而[. . . ] 是一个更为一般的下标操作符。因此前者得到的是列表Lst 中的第一个对象, 并且含有分量名字的命名列表（named list）中的分量名字会被排除在外的。后者得到的则是列表Lst 中仅仅由第一个元素构成的子列表。如果是命名列表，分量名字会传给子列表的。即Lst[[1]] 得到 FredLst[1] 得到name=&quot;Fred&quot; 12345可以通过函数list() 将已有的对象构建成列表。下面的命令&gt; Lst &lt;- list(name 1 =object 1 , . . . , name m=object m)将创建一个含有m 个分量的列表Lst。它的分量分别是object 1, . . . , object m，分量名则是由参数得到（命名没有特定要求）。如果这些名字被忽略了，那么分量只有被编号了。当连接函数c() 的参数中有列表对象时，结果就是一个列表模式的对象&gt; list.ABC &lt;- c(list.A, list.B, list.C) 1`数据框常常会被看作是一个由不同模式和属性的列构成的矩阵。它能以矩阵形式出现，行列可以通过矩阵的索引习惯访问。可以通过函数data.frame 创建符合上面对列(分量)限制的数据框对象：&gt; accountants &lt;- data.frame(home=statef, loot=incomes, shot=incomef)` 1`从外部文件读取一个数据框最简单的方法是使用函数read.table()` 123456789用$ 符号访问对象不是非常的方便，如accountants$statef。一个非常有用的工具将会使列表或者数据框的分量可以通过它们的名字直接调用。而且这种调用是暂时性的，没有必要每次都显式的引用列表名字。函数attach() 除了可以用目录路径作为参数，也可以使用数据框。假定数据框lentils 有三个变量lentils$u, lentils$v, lentils$w，那么&gt; attach(lentils)将把数据框绑定在搜索路径的位置2（position 2）上（注释：R 的搜索路径是一种层状结构，当前搜索位置是1，可以通过函数attach()设置搜索路径的位置2。）。如果位置1没有变量u, v 或w，那么u, v 和w 直接在数据框中访问。因此，下面的命令&gt; u &lt;- v+w际上没有替换数据框中的变量u，而是被处于搜索路径位置1工作空间中的变量u 所屏蔽。为了真正改变数据框中的数据，最简单的办法还是使用$ 符号：&gt; lentils$u &lt;- v+w去除一个数据框的绑定，可以使用&gt; detach() 1`如何使用数据框• 处理问题时，将相应的数据框绑定在位置2上，在第1 层的工作目录中存放操作值和临时变量；• 问题结束时，用$ 形式的赋值命令把任何你想保留的变量加入数据框中，然后利用函数detach() 将绑定去除；• 最后去掉工作目录中所有你不想要的变量，尽可能清空临时变量。`]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（五）：数组和矩阵]]></title>
    <url>%2F2018%2F05%2F21%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E6%95%B0%E7%BB%84%E5%92%8C%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[1`数组和矩阵的联系：一个矩阵只是一个二维的数组` 1`向量只有在定义了 dim 属性后才能作为数组在R 中使用。假定，z是一个含1500个元素的向量。那么&gt; dim(z) &lt;- c(3,5,100)对dim 属性的赋值使得该向量成一个3 ×5 ×100 的数组。` 1`数据向量（data vector）的值在数组中的排列顺序采用 FORTRAN 方式的数组元素次序，即“按列次序”，也就是说第一下标变化最快，最后下标变化最慢。假定数组a的维数向量是c(3,4,2)，则a 中有3×4×2 = 24 元素，依次为a[1,1,1],a[2,1,1], ..., a[2,4,2], a[3,4,2]。` 123延续前面的例子，a[2,,] 是一个4 × 2 的数组。它的维度向量为c(4,2)，数据向量依次包括下面的值c(a[2,1,1], a[2,2,1], a[2,3,1], a[2,4,1],a[2,1,2], a[2,2,2], a[2,3,2], a[2,4,2]) 1a[,,]表示整个数组。这和忽略下标直接使用a 效果是一样的 1234567891011121314151617181920212223242526272829假定我们有一个4 ×5 的数组X，我们可以做如下的事情：• 以向量的格式取出元素X[1,3], X[2,2] 和X[3,1]，• 在数组X 中用0替换这些元素。在这个例子中，我们需要一个3 ×2 的下标数组，见下面的代码。# 注意顺序是按列加上去的&gt; x &lt;- array(1:20, dim=c(4,5)) # 产生一个 4 × 5 的数组。&gt; x[,1] [,2] [,3] [,4] [,5][1,] 1 5 9 13 17[2,] 2 6 10 14 18[3,] 3 7 11 15 19[4,] 4 8 12 16 20&gt; i &lt;- array(c(1:3,3:1), dim=c(3,2))&gt; i # i 是一个 3 × 2 的索引矩阵。[,1] [,2][1,] 1 3[2,] 2 2[3,] 3 1&gt; x[i] # 提取这些元素。[1] 9 6 3&gt; x[i] &lt;- 0 # 用0替换这些元素。&gt; x[,1] [,2] [,3] [,4] [,5][1,] 1 5 0 13 17[2,] 2 0 10 14 18[3,] 0 7 11 15 19[4,] 4 8 12 16 20&gt; 1`cbind和rbind分别是根据列和行合并，例如ib &lt;- cbind(1:5, 2)&gt; ib [,1] [,2][1,] 1 2[2,] 2 2[3,] 3 2[4,] 4 2[5,] 5 2` 12345678910111213141516在R中求单变量的频次，直接使用table函数就可以了，比如：&gt; table(rpois(100,5))0 1 2 3 4 5 6 7 8 9 101 4 9 22 22 11 11 13 3 2 2 同样的，可以使用table函数来求两个或多个变量之间的列联表，比如：&gt; table(rpois(100,2),rpois(100,1)) 0 1 2 3 4 5 0 7 4 3 0 0 0 1 8 7 6 1 1 0 2 14 14 2 2 0 1 3 6 7 1 1 0 0 4 2 3 4 2 0 0 5 1 0 0 0 1 0 6 0 2 0 0 0 0 1`除了用设定一个向量dim 属性的方法来构建数组，它还可直接通过函数array 将向量转换得到，具体格式为&gt; Z &lt;- array(data vector , dim vector )&gt; Z &lt;- array(0, c(3,4,2))这样就会使得Z 是一个所有值都是0的3 ×4 ×2 数组` 1`数组可用于算术表达式中，并且结果就是一个基于数据向量的对应元素运算而得到的数组。所有操作数的属性dim 必须一致` 123456789101112所谓数组(或向量)a和b的外积,指的是a的每一个元素和b的每一个元素搭配在一起相乘得到的新元素.当然运算规则也可自定义.外积运算符为 %o%(注意:百分号中间的字母是小写的字母o)，也可以用&gt; ab &lt;- outer(a, b, &quot;*&quot;)例如:&gt; a &lt;- 1:2&gt; b &lt;- 3:5&gt; d &lt;- a %o% b&gt; d [,1] [,2] [,3] # a的每一个元素和b的每一个元素搭配在一起相乘[1,] 3 4 5[2,] 6 8 10注意维数公式为:dim(d) = c( dim(a) , dim(b) ) 1广义转置：我们可以用命令B &lt;- t(A)表示矩阵A转置，函数nrow(A) 和ncol(A) 将会分别返回矩阵A 的行数和列数。 123456789101112A*B表示矩阵元素对应相乘（要求行列相同），A%*%B表示矩阵乘法，行列式用法（要求满足m,k × k,n）。a &lt;- array(1:9,c(3,3))b &lt;- array(2,c(3,3))a*ba%*%b [,1] [,2] [,3][1,] 2 8 14[2,] 4 10 16[3,] 6 12 18 [,1] [,2] [,3][1,] 24 24 24[2,] 30 30 30[3,] 36 36 36 1`函数crossprod() 可以完成“矢积”（crossproduct）运算，也就是说crossprod(X,y) 和t(X) %*% y 等价，但是在运算上更为高效.如果crossprod() 第二个参数忽略了，它将默认和第一个参数一样，即第一个参数和自己进行运算` 1`函数diag() 的含义依赖于它的参数。当v 是一个向量时，diag(v)返回以该向量元素为对角元素的对角矩阵。当M 是一个矩阵时，diag(M) 返回M的对角元素。这和Matlab 中diag() 的用法完全一致。不过有点混乱的是，如果k 是单个值，那么diag(k) 的结果就是k ×k 的方阵！` 1`求解线性方程组是矩阵乘法的逆运算。当下面的命令运行后，&gt; b &lt;- A %*% x（想象多元方程组形式，A的行数为样本数）。如果仅仅给出A 和b，那么x 就是该线性方程组的根。在R 里面，用命令&gt; solve(A,b)求解线性方程组，并且返回x (可能会有一些精度丢失)。注意，在线性代数里面该值表示为x = A−1b ，其中A−1表示A的逆（inverse）。矩阵的逆可以用下面的命令计算，solve(A)不过一般很少用到。在数学上，用直接求逆的办法解x &lt;- solve(A) %*% b相比solve(A,b)不仅低效而且还有一种潜在的不稳定性。用于多元计算的二次型xA−1x可以通过像x %*% solve(A,x)的方式计算得到，而不是直接计算A 的逆。` 123函数eigen(Sm) 用来计算矩阵Sm 的特征值和特征向量。这个函数的返回值是一个含有values 和vectors 两个分量的列表。ev$val 表示Sm 的特征值向量ev$vec 则是相应特征向量构成的一个矩阵。假定我们仅仅需要特征值，我们可以采用如下的命令：&gt; evals &lt;- eigen(Sm)$values 1`函数svd(M) 可以把任意一个矩阵M作为一个参数, 且对M 进行奇异值分解。这包括一个和M 列空间一致的正交列U 的矩阵，一个和M 行空间一致的正交列V 的矩阵，以及一个正元素D 的对角矩阵，如M = U %*% D %*% t(V)` 123函数lsfit() 返回最小二乘法拟合（Least squares ﬁtting）的结果列表。赋值可以采用入下命令&gt; ans &lt;- lsfit(X, y)但实际上，你在回归分析中可能已经习惯使用lm(.) (见线性模型&lt;页码：70&gt;部分) 而不是lsfit()。 123456单个因子会把各部分数据分成不同的组。类似的是，一对因子可以实现交叉分组等。 函数table() 可以从等长的不同因子中计算出频率表。如果有k 个因子参数，那么结果将是一个k-维的频率分布数组。假定statef 是一个设定数据向量元素个体所在州的因子，那么下面的赋值&gt; statefr &lt;- table(statef)将会把一个样本中各种状态的频率分布表赋给statefr。这些频率会被排序且以因子的水平特性标记。等价但有点烦琐实现方式如下&gt; statefr &lt;- tapply(statef, statef, length)]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（四）：有序因子和无序因子]]></title>
    <url>%2F2018%2F05%2F21%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E6%9C%89%E5%BA%8F%E5%9B%A0%E5%AD%90%E5%92%8C%E6%97%A0%E5%BA%8F%E5%9B%A0%E5%AD%90%2F</url>
    <content type="text"><![CDATA[1`因子（factor）是一个对等长的其他向量元素进行分类（分组）的向量对象。 R同时提供有序（ordered）和无序（unordered）因子。` 123注意在字符向量中，“有序”意味着以字母排序的。因子可以简单地用函数factor() 创建：&gt; statef &lt;- factor(state) 123函数levels() 可以用来得到因子的水平（levels），相当于分类依据因子&gt; levels(statef) 12345678910111213141516171819202122232425使用因子来计算样本中每个州的平均收入的一个例子：&gt; state &lt;- c(&quot;tas&quot;, &quot;sa&quot;, &quot;qld&quot;, &quot;nsw&quot;, &quot;nsw&quot;, &quot;nt&quot;, &quot;wa&quot;, &quot;wa&quot;,&quot;qld&quot;, &quot;vic&quot;, &quot;nsw&quot;, &quot;vic&quot;, &quot;qld&quot;, &quot;qld&quot;, &quot;sa&quot;, &quot;tas&quot;,&quot;sa&quot;, &quot;nt&quot;, &quot;wa&quot;, &quot;vic&quot;, &quot;qld&quot;, &quot;nsw&quot;, &quot;nsw&quot;, &quot;wa&quot;,&quot;sa&quot;, &quot;act&quot;, &quot;nsw&quot;, &quot;vic&quot;, &quot;vic&quot;, &quot;act&quot;)&gt; statef &lt;- factor(state)# 查看&gt; statef[1] tas sa qld nsw nsw nt wa wa qld vic nsw vic qld qld sa[16] tas sa nt wa vic qld nsw nsw wa sa act nsw vic vic actLevels: act nsw nt qld sa tas vic wa&gt; levels(statef)[1] &quot;act&quot; &quot;nsw&quot; &quot;nt&quot; &quot;qld&quot; &quot;sa&quot; &quot;tas&quot; &quot;vic&quot; &quot;wa&quot;&gt; incomes &lt;- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56,61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46,59, 46, 58, 43)我们可以用函数tapply()：&gt; incmeans &lt;- tapply(incomes, statef, mean)这将给出一个均值向量。各个元素都用对应的水平名字标记。act nsw nt qld sa tas vic wa44.500 57.333 55.500 53.600 55.000 60.500 56.000 52.250解释：indomes数据在statef的分类/分组情况下，进行mean操作 1`有时候因子的水平有自己的自然顺序并且这种顺序是有意义的。我们需要记录下来可能在进一步的统计分析中用到。函数ordered() 就是用来创建这种有序因子。在其他方面，函数ordered() 和factor 基本完全一样。大多数情况下，有序和无序因子的唯一差别在于前者显示的时候反应了各水平的顺序。另外, 在线性模型拟合的时候，两种因子对应的对照矩阵的意义是完全不同的。`]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（三）：对象]]></title>
    <url>%2F2018%2F05%2F21%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[1`R 的对象类型包括数值型（numeric），复数型（complex），逻辑型（logical），字符型（character）和原味型（raw）。向量必须保证它的所有元素是一样的模式。因此任何给定的向量必须明确属于逻辑性，数值型，复数型，字符型或者原味型。注意空向量也有自己的模式。例如，空的字符串向量将会被显示character(0) 和空的数值向量显示为numeric(0)。` 1`一个对象的模式（mode）是该对象基本要素的类型，另外一个所有对象都有的特征是长度（length），模式和长度又叫做一个对象的“内在属性”。` 1`例如，如果z 是一个长为100 的复数向量，那么命令mode(z) 就会得到字符串&quot;complex&quot; 而length(z) 对应的是100。` 1有一系列类似as.something() 的函数，这些函数主要用于对象模式数据的强制转换，例如&gt; digits &lt;- as.character(z) # 转为字符形式 1改变对象长度这一点上，一个“空”的对象仍然有其模式的。例如&gt; e &lt;- numeric()创建了一个数值模式的空向量结构e。类似的是，character()是一个空的字符向量，等等。一旦一个任意长度的对象被创建，新元素可以通过给定一个在先前索引范围外的索引值而加入。因此&gt; e[3] &lt;- 17将创建一个长度为3的向量e(此时，前两个元素都是NA)。相反，删减一个对象的大小只需要用赋值操作。因此，如果alpha 是一个长度为10的对象，那么&gt; alpha &lt;- alpha[2 * 1:5]将创建一个由偶数索引位值上的元素构成的长度为5的对象(此时，老的索引将会被抛弃)。我们可以用下面命令仅仅保留起始的三个值&gt; length(alpha) &lt;- 3一个向量也可以用同样的办法扩充（扩充部分用缺损值）。]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（二）：算术操作和向量运算]]></title>
    <url>%2F2018%2F05%2F20%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%AE%97%E6%9C%AF%E6%93%8D%E4%BD%9C%E5%92%8C%E5%90%91%E9%87%8F%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[12345678假如我们要创建一个含有五个数值的向量x，且这五个值分别为10.4，5.6，3.1，6.4 和21.7，则 R 中的命令为&gt; x &lt;- c(10.4, 5.6, 3.1, 6.4, 21.7)赋值也可以用函数assign() 实现。下面的命令和前面的赋值命令等价：&gt; assign(&quot;x&quot;, c(10.4, 5.6, 3.1, 6.4, 21.7))进一步的赋值，会创建一个含有11个元素的向量y，其中包括两份x 拷贝和位于中间的一个0。&gt; y &lt;- c(x, 0, x) 1234在算术表达式中使用向量将会对该向量的每一个元素都进行同样算术运算。出现在同一个表达式中的向量最好是长度一致。如果他们的长度不一样，该表达式的值将是一个和其中最长向量等长的向量。表达式中短的向量会被循环使用（recycled）(可能是部分的元素)以达到最长向量的长度。利用前面例子中的变量，命令&gt; v &lt;- 2*x + y + 1将产生一个新的长度为11的向量v。它由2*x 重复2.2次，y 重复一次，1 重复11次得到的向量相加而成。 1`sum(x)给出x 中元素的累加和， 而prod(x) 则得到它们的乘积。var(x)则计算样本方差` 1如果var() 的参数是一个n×p 的矩阵，则将该矩阵行与行之间看作是相互独立的p-变量的样本向量，从而得到一个p×p 的样本协方差矩阵。 12345函数seq() 是数列生成中最为常用的工具。它有五个参数，起始的两个参数，表示一个数列的首尾。如果只是给定这两个值，则和冒号运算符的效果完全一样了。前两个参数就可以用from=value 和to=value 方式设定；因此seq(1,30)，seq(from=1, to=30)，seq(to=30, from=1) 同1:30 完全一样。seq() 随后的两个参数是by=value 和length=value；它们分别表示这个数列的步长和长度。如果二者没有设定，默认值就是by=1（步长为1）。&gt; seq(-5, 5, by=.2) -&gt; s3 12345还有一个相关的函数是rep()。 它可以用各种复杂的方式重复一个对象。最简单的方式是&gt; s5 &lt;- rep(x, times=5)这种方式先把x 的完整拷贝五次，保持x 的数列顺序，逐一放在s5 中。另一种有用的方式是&gt; s6 &lt;- rep(x, each=5)这种方式把x 中的每个元素都重复五次，然后将重复五次的元素逐一放入 123逻辑向量元素可以被赋予的值有TRUE，FALSE 和NA (“不可得到”)逻辑向量可以由条件式（conditions）产生。例如&gt; temp &lt;- x &gt; 13 # 得到TRUE or FALSE 1`函数is.na(x) 返回一个和x 同等长度的向量。它的某个元素值为TRUE 当且仅当x中对应元素是NA。` 1`特别要注意的是逻辑表达式x == NA 和is.na(x) 完全不同。因为NA 不是一个真实的值而是一个符号以表示某个量是不可得到的, 因此x == NA 得到的是一个长度和x一致的向量。它的所有 元素的值都是NA.` 1`总之，对于NA 和Na N(数学上无法计算得到) 用is.na(xx) 检验都是 TRUE。为了区分它们，is.nan(xx)就只对是Na N 元素显示TRUE。` 1`通过函数c() 可以把几个字符向量连接成一个字符向量；函数paste() 可以有任意多的参数，并且把它们一个接一个连成字符串。这些参数中的任何数字都将被显式地强制转换成字符串，而且以同样的方式在终端显示。默认的分隔符是单个的空格符，不过这可以被指定的参数修改。参数sep=string 就是将分隔符换成string，这个参数可以设为空。例如，&gt; labs &lt;- paste(c(&quot;X&quot;,&quot;Y&quot;), 1:10, sep=&quot;&quot;)使得labs 变成一个字符向量。c(&quot;X1&quot;, &quot;Y2&quot;, &quot;X3&quot;, &quot;Y4&quot;, &quot;X5&quot;, &quot;Y6&quot;, &quot;X7&quot;, &quot;Y8&quot;, &quot;X9&quot;, &quot;Y10&quot;)` 1`负整数向量索引。这种索引向量指定被排除的元素而不是包括进来。&gt; y &lt;- x[-(1:5)] #将x 除开始五个元素外的其他元素都赋给y。` 12345字符串向量索引，这可能仅仅用于一个对象可以用names 属性来识别它的元素。&gt; fruit &lt;- c(5, 10, 1, 20)&gt; names(fruit) &lt;- c(&quot;orange&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;peach&quot;)&gt; lunch &lt;- fruit[c(&quot;apple&quot;,&quot;orange&quot;)]]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R导论学习笔记（一）：基础]]></title>
    <url>%2F2018%2F05%2F17%2FR%E8%AF%AD%E8%A8%80%2FR%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[记录读《R导论》和《153分钟学会R》 的学习笔记 如果你是第一次在UNIX 系统使用 R，我们推荐的操作步骤如下： 创建一个独立的子目录work 来保存你要在这个系统上用 R 分析的数据文件。当你用 R 处理这些数据时，这将是你的工作目录。 12$ mkdir work$ cd work 用命令启动 R 程序。 1$ R 此时，可以键入 R 的命令(见后面的内容)。 退出 R 程序的命令是 1&gt;q() 1为了得到任何特定名字的函数的帮助，如solve，可以使用如下命令 &gt; help(solve) 1在大多数 R 平台中，你可以通过运行下面的命令得到HTML 格式的帮助。 &gt; help.start() 1命令可以被(;)隔开，或者另起一行。基本命令可以通过大括弧(&#123;和&#125;) 放在一起构成一个复合表达式（compound expression）。注释几乎可以放在任何地方。一行中，从井号(#)开始到句子收尾之间的语句就是注释。 1如果一批命令保存在工作目录work 下一个叫commands.R 的文件中，可以用下面的命令在 R 会话中执行这个文件。 &gt; source(&quot;commands.R&quot;) 12如何清除变量？清除单个变量使用 rm() 函数，清除内存中所有的变量： rm( l i s t = l s ( a l l = TRUE)) 12如何得到函数的代码？通常情况你只需要在 R 平台下写出你需要查看的函数名，回车即可。比如：d i s t 1但有时候这个函数可能是一个泛型函数（Generic Function），上面的方法就需要稍稍改进一下：先使用 methods() 函数来查看这个类函数的列表，找到具体需要的函数，写出来，回车 ，问题解决。 123summary # It i s a generic funcitonmethods (summary) # l i s t of the S3 methodssummary . lm # maybe you want to know the l i n e a r models ’ s summary 1想查看一个矩阵的前（后）几行， 么办？可以使用 head() 或 tail() 函数。 12345在 R 中公式的符号都是什么意义？拿常见的 lm，glm 模型来说，y ˜model 是一种特定的格式，表示以 y 为响应变量，模型为model。其中 model 中的变量由+来连接，或者由: 来表示变量间的 “交互作用”。除了+和 : ，我们使用 ∗来表示′a + b + a : b′。(a + b + c)∧2表示(a + b + c)∗(a + b + c)，即主因素 a、b、c 和各个因素的交互作用。−表示去掉之意。(a + b + c)∧2−a : b表示′a + b + c + b : c + a : c′。在公式表达中除了变量和因子名外，运算符号也是可以存在的。如′log(y) a+log(x)′是合法的。符号. 在 update 函数中有特殊的意义，它表示 “已经存在” 之意。1fm &lt;− aov ( Speed ~ Run + Expt)fm0 &lt;− update (fm , . ~ . − Run) # .表示之前的旧模型 1可以将 R 中显示的结果输出到文件么？可以。使用 sink()函数。 123怎样将因子 (factor) 转换为数字as . numeric ( as . character ( f )) #这个点一定要小心，因为对于顺序型 factor 数据，如果强制转化为数值型，会返回的是 factor的顺序信息，而非你看到的 character 信息。 123为什么当我使用 source() 时，不能显示输出结果？对需要显示输出的对象使用 print() ，或者使用 source(ﬁle, echo = TRUE)。如果 R 代码里面包含 sink() 之类的函数，必须使用 source(ﬁle, echo = TRUE) 才能得到正确的输出结果，否则 sink 的对象将为空 1234在 R 里面使用必须使用双反斜杠或单斜杠表示文件路径，比如：d :\\R−2.4.1\\ l i b r a r y \\ xgobi \\ s c r i p t s \\ xgobi . batd : /R−2.4.1 / l i b r a r y / xgobi / s c r i p t s / xgobi . bat 1234如何删掉缺失值？在 R 中使用 NA（not available）表示缺失值，要注意 R（S）语言中 NA 同样是一个逻辑值。使用函数 is.na()来判断是否为缺失值，删除缺失值。x [ ! i s . na (x ) ] 123如何将字符串转变为命令行？这里用到 eval() 和 parse() 函数。首先使用 parse() 函数将字符串转化为表达式（expression），而后使用 eval() 函数对表达式求解` 1如何向一个向量 加元素？参考 append()函数。 12我的数据框有相同的行，如何去掉这些行？参考 unique 函数。 duplicated 函数返回了元素是否重复的逻辑值。 1234如何对数列（array）进行维度变换？使用函数 apermx &lt;− array (1:24 , 2:4)xt &lt;− aperm (x , c (2 ,1 ,3))dim(x) ; dim( xt ) 1234如何删除 list 中的元素？R 中使用 NULL 表示无效的对象。l s t &lt;− l i s t ( ”a”=l i s t ( ”b”=1,”c”=2) ,”b”=l i s t ( ”d”=3,”e”=4))l s t [ [ ”a” ] ] [ ”b” ] &lt;− NULL # or l s t $a$b &lt;− NULL 123456789如何对矩阵 行 (列) 作计算？使用函数 apply()vec =1:20mat=matrix ( vec , ncol =4)veccumsum( vec )matapply (mat ,2 ,cumsum)apply (mat ,1 ,cumsum) 123如何注掉大段的 R 程序i f (FALSE)&#123;something passby&#125; 1一组数中随机抽取数据？参考函数 sample() 123如何根据共有的列将两个数据框合并？我们经常会遇到两个数据框拥有相同的时间或观测值，但这些列却不尽相同。处理的办法就是使用 merge(x, y ,by.x = ,by.y = ,all = ) 函数。 123如何求矩阵各行 (列) 的均值？如果运算量不是很大，当然可以使用 apply() 函数。row Means() 和 col Means() 函数可以更快地得到你要的结果。 123如何计算组合数或得到所有排列组合？choose() 用于计算排列组合数 123如何在 R 里面求（偏）导数？使用函数 D() 123如何模拟高斯（正态）分布数据？使用 rnorm(n , mean , sd) 来产生 n 个来自于均值为 mean，标准差为 sd 的高斯（正态）分布的数据。在 R 里面通过分布前增加字母 ‘d’ 表示概率密度函数，‘p’ 表示累积分布函数，‘q’表示分位数函数，‘r’ 表示产生该分布的随机数 123如何在字符串中选取特定位置的字符？参考 substr()函数。substr ( ” abcdef ” ,2 ,4) 1234这里要区别一下 length，length 函数是返回向量里元素的个数。比如 “你好吗” 是长度为一的向量，但这个元素的字符长度为三，这里就需要使用 nchar 函数：nchar ( ’你好吗 ’ )[ 1 ] 3length ( ’你好吗 ’ )[ 1 ] 1 123如何在同一面出多张图？修改绘图参数，如 par(mfrow = c(2,2)) 或 par(mfcol = c(2,2))； 1234567如何在条形图上显示每个 bar 的数值？如果明白 barplot() 函数其实是由低级绘图命令 rect() 函数构造的，那下面的例子也就不难理解了：x &lt;− 1:10 ; names (x) &lt;− l e t t e r s [ 1 : 1 0 ]b &lt;− barplot (x , col = rev ( heat . c o l o r s (10)) )text (b , x , l a b e l s = x , pos = 3) 1234没有直接计算峰度和偏度的函数？当然自己写一个也费不了太多时间。FBasics 包中提供了可以直接计算偏度和峰度的函数。skewness ()kurtosis () 1234如何得到一个正态总体均值µ的区间估计？很简单，t.test() 函数x &lt;− rnorm (100)t . t e s t (x) 123如何做聚类分析？K 均值聚类 (kmeans() )：层次聚类 (hclust() )： 1234567如何做主成分分析？stats 包中的 princomp 函数（特征根求解），以及 prcomp 函数（奇异值分解）( pc . cr &lt;− princomp ( USArrests , cor = TRUE))plot ( pc . cr , type = ” l i n e s ” # or ” barplot ”) # or s c r e e p l o tloadings ( pc . cr )princomp() 中的参数 cor = TRUE 表示使用样本相关矩阵作主成分分析，反之使用样本协方差矩阵。loadings() 返回因子荷载。screeplot() 绘制碎石图。 123如何对样本数据进行正态检验？比较常见的方法：shapiro.test() ，ks.test()(Kolmogorov-Smirnov 检验) ，jarque.bera.test() 12345如何做配对 t 检验？参考 t.test() 中的 paired 参数require ( s t a t s )## Student ’ s paired t−t e s tt . t e s t ( extra ~ group , data = sleep , paired = TRUE) 123多项式回归应该使用什么函数？使用 I() ，例如：lm(y ~ x + I (x^2) + I (x^3)) 如何使用方差分析（ANOVA）？方差分析同线性回归模型很类似，毕竟它们都是线性模型。最简单实现方差分析的函数为aov()，通过规定函数内公式形式来指定方差分析类型： 123如何求解没有常数项的线性回归模型？只需在公式中引入 0 即可 ：r e s u l t &lt;− lm( smokes ~ 0 + male + female , data = smokerdata ) 1234567回归的命令是？参考 MASS 包中的 lm.ridge() 函数。data ( longley ) # not the same as the S−PLUS datasetnames ( longley ) [ 1 ] &lt;− ”y”lm . ridge (y ~ . , longley )plot (lm . ridge (y ~ . , longley ,lambda = seq ( 0 , 0 . 1 , 0 . 0 0 1 ) ) )s e l e c t (lm . ridge (y ~ . , longley ,lambda = seq ( 0 , 0 . 1 , 0 . 0 0 0 1 ) ) ) 1`如何使用正交多项式回归？在 R 中，使用 poly() 函数：( z &lt;− poly (1:10 , 3))` 1234如何求 Spearman 等级（或 kendall）相关系数？cor() 函数默认为求出 Person 相关系数，修改其 method 参数即可求得 Kendallτ和Spearman 秩相关系数。cor ( longley , method = ”spearman” ) 1`如何做 Decision Tree？基于树型方法的模型（Tree-based model）并不被统计学背景的研究者所熟悉，但它在其他领域却时常被广泛应用。下面是 Modern Applied Statistics With S 中的例子，需要加载 rpart包。l i b r a r y ( rpart )set . seed (123)cpus . rp &lt;− rpart ( log10 ( perf ) ~ . , cpus [ , 2 : 8 ] , cp = 1e−3)plot ( cpus . rp , uniform = T)text ( cpus . rp , d i g i t s = 3)` 1box-cox 变换？MASS 包中的boxcox ()函数。 1R 有类似于 SPSS 的界面么？有！安装包 Rcmdr ，加载包后，使用命令Commander() 1`样来计算函数运行使用时间？使用 system.time() 。proc.time() 可以获得 R 进程存在的时间，system.time() 通过调用两次 proc.time() 来计算函数运行的时间。`]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
</search>
