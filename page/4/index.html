<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="Mosbyllc" type="application/atom+xml">






<meta name="description" content="Sometimes thing have to fall apart to make way for better things.">
<meta property="og:type" content="website">
<meta property="og:title" content="Mosbyllc">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="Mosbyllc">
<meta property="og:description" content="Sometimes thing have to fall apart to make way for better things.">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mosbyllc">
<meta name="twitter:description" content="Sometimes thing have to fall apart to make way for better things.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/4/">





  <title>Mosbyllc</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Mosbyllc</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>

<link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/" itemprop="url">Sklearn 与 TensorFlow 机器学习实用指南（五）：决策树</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-23T11:07:30+08:00">
                2018-07-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sklearn-与-TensorFlow-机器学习实用指南/" itemprop="url" rel="index">
                    <span itemprop="name">Sklearn 与 TensorFlow 机器学习实用指南</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  7,682
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  28
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一些基本概念"><a href="#一些基本概念" class="headerlink" title="一些基本概念"></a>一些基本概念</h1><h2 id="决策树简述"><a href="#决策树简述" class="headerlink" title="决策树简述"></a>决策树简述</h2><p>决策树（Decision Tree）是数据挖掘中一种基本的分类和回归方法，它呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是if−thenif−then规则的集合。决策树模型的主要优点是模型具有可读性，分类速度快。在学习时，利用训练数据，根据损失函数最小化原则建立决策树模型；而在预测时，对新的数据，利用决策树模型进行分类。主要的决策树算法有ID3算法、C4.5算法和CART算法。一个决策树的学习过程包括三个步骤：特征选择、决策树的生成以及决策树的修剪。</p>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息熵是衡量样本纯度的一种指标，嘉定当前样本集合D中第k类样本所占的比例为$p_k(k=1,2,…,|y|)$,则D的信息熵定义为</p>
<script type="math/tex; mode=display">
Ent(D)=-\sum_{k=1}^{\vert y\vert}p_klog_2{p_k}</script><p>Ent(D)的值越小，则D的纯度越高。</p>
<p>以周志华西瓜书P76的西瓜数据集中的17个样本数据为例子。</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/01.jpg" alt></p>
<p>显然，标签类别|y|=2.其中正例占$p_1$=8/17，反例占$p_2$=9/17。于是根节点的信息熵为</p>
<script type="math/tex; mode=display">
Ent(D)=-\sum_{k=1}^2p_klog_2^{p_k}=-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})=0.998</script><h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><p>$Ent(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望</p>
<script type="math/tex; mode=display">
Ent\left( Y|X \right) =\sum_{i=1}^n{p_iEnt\left( Y|X=x_i \right)}</script><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p><strong>信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度</strong>。特征A对训练数据集D的信息增益Gain(D,a)定义为集合D的经验熵$Ent(D)$与特征A给定条件下D的经验条件熵$Ent(Y | X)$之差，即假定离散属性a有n个可能的取值${a^1,a^2,…a^n}$，若使用属性a的取值来对样本集合划分，则会产生n个分支节点子集$D_1,D_2,..,Dn$，$|D_i|$ 为$D_i$的样本个数。(考虑不同的分支节点所包含的样本数不同，用$\frac{|D^i|}{|D|}$代替期望概率$p_i$，即样本越多的分支节点的影响越大)</p>
<script type="math/tex; mode=display">
Gain(D,a)=Ent(D)-\sum_{i=1}^n\frac{|D^i|}{|D|}End(D^i)</script><p>以西瓜数据集的“色泽”属性为例，它有3个可能的取值{青绿，乌黑，浅白}。若使用该属性对D角线划分，则可以得到三个子集，分布记为$D^1$(色泽=青绿)={1,4,6,10,13,17}，正反例率分别为$p_1=\frac{3}{6}$，$p_2=\frac{3}{6}$；$D^2$(色泽=乌黑)={2,3,7,8,9,15}，正反例率分别为$p_1=\frac{4}{6}$，$p_2=\frac{2}{6}$ ；$D^3$(色泽=乌黑)={5,11,12,14,16}，正反例率分别为$p_1=\frac{1}{5}$，$p_2=\frac{4}{5}$。</p>
<p>首先，根据信息熵公式可计算出“色泽”划分之后所获得的3个分支节点的信息熵为</p>
<script type="math/tex; mode=display">
Ent(D^1)=-(\frac{3}{6}log_2\frac{3}{6}+\frac{3}{6}log_2\frac{3}{6})=1.000 \\ \\ \\ Ent(D^2)=-(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})=0.918 \\ \\ \\ Ent(D^3)=-(\frac{1}{5}log_2\frac{1}{5}+\frac{4}{5}log_2\frac{4}{5})=0.722</script><p>之后根据上面公式可计算出属性“色泽”的信息增益为</p>
<script type="math/tex; mode=display">
Gain(D,色泽)=Ent(D)-\sum_{i=1}^3\frac{|D^i|}{|D|}Ent(D^i)  \\ \\ \\ =0.998-((\frac{6}{17}\times1.000+\frac{6}{17}\times0.918+\frac{5}{17}\times0.722)=0.109</script><p>一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。其中ID3决策树就是以信息增益为准则来选择划分属性。根据得到的属性最大信息增益来划分结果示例如下所示。之后再根据子集样本进一步划分，直到只有一个样本个体或者样本个体都为同一类标签为止。</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/02.jpg" alt></p>
<p>若我们把样本编号1-17也作为一个划分属性，则根据信息增益公式可计算出它的信息增益为0.998，远大于其他候选划分属性。这很容易理解，“编号”将产生17个分支，每个分支仅包含一个样本，这些分支节点纯度已打最大。然而这些侧介绍显然不具有泛化能力。</p>
<p><strong>所以实际上，信息增益准则对可取数目较多的属性有所偏好</strong>；为了减少这种偏好，著名的C4.5决策树算法不直接采用信息增益，而是使用增益率来做最优划分属性。</p>
<h2 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h2><p>增益率定义为其信息增益Gain(D，a)与训练数据集D关于特征a样本数的信息熵之比，增益率的数学定义为</p>
<script type="math/tex; mode=display">
Gain_{ratio}(D,a)=\frac{Gain(D,a)}{IV(a)}</script><script type="math/tex; mode=display">
IV(a)=-\sum_{v=1}^n\frac{|D^i|}{|D|}log_2\frac{|D^i|}{|D|}</script><p>（<strong>注意子集的划分是属性a样本的可取类别数目n，而不是标签类别|y|了啊</strong>），IV(a)可以成为属性a的内在信息，若属性a的可取数目越大，则IV(a)的值通常越大。这样就可以一定程度平衡信息增益对可取数目较多的属性的偏好。以“色泽”为例</p>
<script type="math/tex; mode=display">
IV(色泽)=-\frac{6}{17}log_2\frac{6}{17}-\frac{6}{17}log_2\frac{6}{17}-\frac{5}{17}log_2\frac{5}{17}=1.580</script><p>另外，需要注意的是，增益率准则对可取数值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率的最高的。</p>
<h2 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h2><p>数据集D的纯度可以用基尼指数来度量，Gini（D）越小，则数据集D的纯度越高。假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼系数定义为</p>
<script type="math/tex; mode=display">
Gini\left( p \right) =\sum_{k=1}^K{p_k\left( 1-p_k \right) =1-\sum_{k=1}^K{p_k^2}}</script><p>根据基尼指数定义，可以得到样本集合D的基尼指数，其中$D_k$表示数据集D中属于第k类的样本子集</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^{K}\left(\frac{|D_k|}{|D|} \right)^2</script><p>若样本集合D根据特征A是否取某一可能值a被分割成$D_1$和$D_2$两部分，即</p>
<script type="math/tex; mode=display">
D_1=\left\{ \left( x,y \right) \in D|A\left( x \right) =0 \right\} \mathrm{，}D_2=D-D_1</script><p>则在特征A的条件下，集合D的基尼指数定义为</p>
<script type="math/tex; mode=display">
Gini\left( D,A \right) =\frac{|D_1|}{|D|}Gini\left( D_1 \right) +\frac{|D_2|}{|D|}Gini\left( D_2 \right)</script><p>基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。<strong>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini</strong>，选取其中的最小值，作为属性A得到的<strong>最优二分方案</strong>。<strong>然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案</strong>。</p>
<script type="math/tex; mode=display">
\min_{i\epsilon A}(Gain\_Gini(D,A))</script><script type="math/tex; mode=display">
\min_{A\epsilon Attribute}(\min_{i\epsilon A}(Gain\_Gini(D,A)))</script><h1 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h1><p>剪枝是决策树学习算法对付“过拟合”的主要手段。在决策树学习中，为了尽可能正确分类样本，节点划分过程不断重复，有时会造成决策树分支过多，这时有时候把自身特点当做所有数据都具有的一般性质而导致过拟合，因此，可以通过主动去掉一些分支来降低过拟合的风险</p>
<p>基本策略有预剪枝和后剪枝，预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分当前节点标记为叶节点；后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的字数替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点。</p>
<h1 id="CART决策树的生成"><a href="#CART决策树的生成" class="headerlink" title="CART决策树的生成"></a>CART决策树的生成</h1><p> ID3算法和C4.5按照信息增益和增益率最大的特征作为节点特征，然后递归构建。比较简单，还要注意这两种方法都容易过拟合。这里主要讲一下CART分类与回归树。</p>
<p>分类树与回归树（classification and regression tree，CART）模型（Breiman）由特征选择、树生成及剪枝组成，既可用于分类也可用于回归。CART算法采用<strong>二分递归分割的技术</strong>将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。</p>
<ul>
<li>如果待预测分类是离散型数据，则CART生成分类决策树。</li>
<li>如果待预测分类是连续性数据，则CART生成回归决策树。</li>
</ul>
<h2 id="CART分类树"><a href="#CART分类树" class="headerlink" title="CART分类树"></a>CART分类树</h2><p>对分类树用基尼系数（Gini index）最小化准则，进行特征选择，生成二叉树。</p>
<p>具体算法步骤如下：</p>
<ul>
<li>1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为”是”或者“否”将D分割为D1和D2两部分，计算其基尼系数。</li>
<li>2）在所有可能的特征A以及他们所有可能的切分点a中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</li>
<li>3）对两个子结点递归地调用上述两个步骤，直至满足停止条件。</li>
<li>4）生成CART决策树</li>
</ul>
<p>以生物特征分类数据为例</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/03.jpg" alt></p>
<p>针对上述离散型数据，按照<strong>体温为恒温和非恒温</strong>(多类别属性也按’非’分成两类别)进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算以体温为划分特征D1,D2的基尼指数。</p>
<script type="math/tex; mode=display">
Gini(D_1)=1-[ (\frac{5}{7})^2+(\frac{2}{7})^2]=\frac{20}{49}</script><script type="math/tex; mode=display">
Gini(D_2)=1-[ (\frac{3}{8})^2+(\frac{3}{8})^2+(\frac{2}{8})^2]=\frac{42}{64}</script><p>然后计算得到特征<strong>体温</strong>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分</p>
<script type="math/tex; mode=display">
Gain\_Gini(D,体温)=\frac{7}{15}*\frac{20}{49}+\frac{8}{15}*\frac{42}{64}</script><p>在所有可能的特征A以及他们所有可能的切分点a中的二分划分中，选择基尼系数最小的特征及其对应的切分点作为最优特征与最优切分点，依次递归。</p>
<h2 id="CART回归树"><a href="#CART回归树" class="headerlink" title="CART回归树"></a>CART回归树</h2><p>回归树衡量最好的标准不再是最大熵，而是<strong>最小化均方差</strong>。而且在每个节点（不一定是叶子节点）都会得一个预测值，这个预测值可为所有样本的平均值。我们利用最小二乘回归树生成算法来生成回归树f(x)，即在训练数据集所在的输入空间中，递归地将每个区域分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>
<p><strong>回归树算法流程：j为选定的某个特征属性，s为在这个特征上的切分数值（需要遍历所有特征和切分点来选到最小化均方差），R1R2为切分的样本，C1C2为切分区域样本中的特征均值。</strong></p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/04.jpg" alt></p>
<p><strong>实例详解：</strong></p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/05.jpg" alt></p>
<p>考虑如上所示的连续性变量，根据给定的数据点，考虑<strong>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</strong>共9个自定均分的切分点。对各切分点依次求出<strong>R1,R2,c1,c2及m(s)</strong>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示</p>
<script type="math/tex; mode=display">
c_1=\frac{1}{N_m}\sum_{x_i\epsilon R_m(j,s)}y_i=\frac{1}{1}\sum_{x_i\epsilon R_1(1,1.5)}5.56=5.56</script><script type="math/tex; mode=display">
c_2=\frac{1}{N_m}\sum_{x_i\epsilon R_m(j,s)}y_i=\frac{1}{9}\sum_{x_i\epsilon R_2(1,1.5)}(5.70+5.91+...+9.05)=7.50</script><script type="math/tex; mode=display">
m(s)=\min_{j,s}[\min_{c_1}\sum _{x_i\epsilon R_i(j,s)}(y_i-c_1)^2+\min_{c_2}\sum _{x_i\epsilon R_i(j,s)}(y_i-c_1)^2]=0+15.72=15.72</script><p><strong>依次改变(j,s)对，可以得到s及m(s)的计算结果</strong>，如下表所示。</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/06.jpg" alt></p>
<p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<strong>回归树T1(x)</strong>为</p>
<script type="math/tex; mode=display">
T_1(x)=\begin{cases}
 & 6.24,x<6.5 \\ 
 & 8.91,x\ge 6.5
\end{cases}</script><script type="math/tex; mode=display">
f_1(x)=T_1(x)</script><p><strong>然后我们利用f1(x)拟合训练数据的残差</strong>(各目标值减去对应的c1,c2)，如下表所示</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/07.jpg" alt></p>
<p><strong>用f1(x)拟合训练数据得到平方误差</strong></p>
<script type="math/tex; mode=display">
L(y,f_1(x))=\sum_{i=1}^{10}(y_i-f_1(x_i))^2=1.93</script><p>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到</p>
<script type="math/tex; mode=display">
T_2(x)=\begin{cases}
 & -0.52,x<3.5 \\ 
 & 0.22,x\ge 3.5
\end{cases}</script><script type="math/tex; mode=display">
f_2(x)=f_1(x)+T_2(x)=
\begin{cases}
 & 5.72,x<3.5 \\ 
 & 6.46,3.5\le x \le 6.5 \\
 & 9.13,x\ge 6.5
\end{cases}</script><p>用f2(x)拟合训练数据的平方误差</p>
<script type="math/tex; mode=display">
L(y,f_2(x))=\sum_{i=1}^{10}(y_i-f_2(x_i))^2=0.79</script><p>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示</p>
<script type="math/tex; mode=display">
T_3(x)=\begin{cases}
 & 0.15,x<6.5 \\ 
 & -0.22,x\ge 6.5
\end{cases}
L(y,f_3(x))=0.47</script><script type="math/tex; mode=display">
T_4(x)=\begin{cases}
 & -0.16,x<4.5 \\ 
 & 0.11,x\ge 4.5
\end{cases}
L(y,f_4(x))=0.30</script><script type="math/tex; mode=display">
T_5(x)=\begin{cases}
 & 0.07,x<6.5 \\ 
 & -0.11,x\ge 6.5
\end{cases}
L(y,f_5(x))=0.23</script><script type="math/tex; mode=display">
T_6(x)=\begin{cases}
 & -0.15,x<2.5 \\ 
 & 0.04,x\ge 2.5
\end{cases}</script><script type="math/tex; mode=display">
f_6(x)=f_5(x)+T_6(x)=T_1(x)+...+T_6(x)=
\begin{cases}
 & 5.63,x<2.5 \\ 
 & 5.82,2.5\le x \le 3.5 \\
  & 6.56,3.5\le x \le 4.5 \\
 & 6.83,4.5\le x \le 6.5 \\
  & 8.95,x\ge 6.5 
\end{cases}</script><p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。</p>
<script type="math/tex; mode=display">
L(y,f_6(x))=\sum_{i=1}^{10}(y_i-f_6(x_i))^2=0.71</script><h2 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h2><p>们将一颗充分生长的树称为<strong>T0</strong> ，希望减少树的大小来防止过拟化。但同时<strong>去掉一些节点后预测的误差可能会增大</strong>，<strong>即完整树T0拟合的损失是最小的</strong>。那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下</p>
<script type="math/tex; mode=display">
C_\alpha(T)=C(T)+\alpha|T|</script><ul>
<li>T为任意子树，|T|为子树T的叶子节点个数。</li>
<li>α是参数，权衡拟合程度与树的复杂度。</li>
<li>C(T)为<strong>预测误差</strong>，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。</li>
</ul>
<p><strong>那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？</strong>准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得$C_\alpha(T)$最小的最优子树T(α)。</p>
<ul>
<li>当α很小的时候(相当于弱化正则)，T0 完整树是这样的最优子树.</li>
<li>当α很大的时候(相当于强化正则)，单独一个根节点就是最优子树。</li>
</ul>
<p>Breiman等人证明：可以用递归地方法对树进行剪枝。将a从小增大，$0=a_0&lt;a_1&lt;…..a_n&lt;+∞$产生一系列的区间$[a_i,a_i+1),i=0,1,…,n$剪枝得到的子树序列对应着区间$a∈[a_i,a_i+1)，i=0,1,2,…,n$的最优子树序列为${T_0,T_1,T_2,…,T_n}$,序列的子树是嵌套的。</p>
<p>具体地，从整体树$T_0$开始剪枝，对$T_0$的人以内部结点t，<strong>以t为单结点树</strong>的损失函数是</p>
<script type="math/tex; mode=display">
C_a\left( t \right) =C\left( t \right) +a</script><p><strong>以t为根结点的子树</strong>$T_t$的损失函数是</p>
<script type="math/tex; mode=display">
C_a(T_t)=C(T_t)+a|T_t|</script><p>到这里，我们的目的就变得很明确了，当t为单节点树的损失比以t为根节点的子树$T_t$损失相等的时候，损失相同，而t的节点少，就进行剪枝操作。其过程会随a由0逐渐增大，单节点树t损失一开始大于子树$T_t$，随后不断减少，直到大于子树损失。具体的，当$\alpha =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}$ ，$T_t$和t有相同的损失。</p>
<p>为此，对$T_0$中的每一个内部结点t，计算</p>
<script type="math/tex; mode=display">
g\left( t \right) =\frac{C\left( t \right) -C\left( T_t \right)}{|T_t|-1}</script><p>它表示剪枝后整体损失函数减少的程度。在$T_0$中减去g(t)最小的$T_t$，将得到的子树作为$T_1$，同时将最小的g(t)设为$a_1$，$T_1$为区间$[a_1,a_2)$的最优子树。如此剪枝下去，直至得到根结点。在这一过程中，不断得增加a的值，产生新的区间。</p>
<p>然后，在剪枝得到的子树序列$T_0,T_1,…,T_n$中通过交叉验证选取最优子树$T_a$.</p>
<p>具体地，利用独立的验证数据集，测试子树序列$T_0,T_1,…,T_n$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$T_0,T_1,…,T_n$都对应一个参数$a_1,a_2,…,a_n$。所以当最优子树$T_k$确定时，对应的$a_k$也就确定了，即得到最由决策树$T_a$.</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><h2 id="决策树分类边界"><a href="#决策树分类边界" class="headerlink" title="决策树分类边界"></a>决策树分类边界</h2><p>另外，决策树所形成的分类边界有一个明显的特点：轴平行，即它的分类边界有若干个与坐标轴平行的分段组成。</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/08.png" alt></p>
<h2 id="决策树的训练和可视化"><a href="#决策树的训练和可视化" class="headerlink" title="决策树的训练和可视化"></a>决策树的训练和可视化</h2><p>下面的代码就是在我们熟知的鸢尾花数据集上进行一个决策树分类器的训练</p>
<blockquote>
<p>决策树的众多特性之一就是， 它不需要太多的数据预处理， 尤其是不需要进行特征的缩放或者归一化。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data[:, <span class="number">2</span>:] <span class="comment"># petal length and width y = iris.target</span></span><br><span class="line">    tree_clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>)</span><br><span class="line">    tree_clf.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>你可以通过使用export_graphviz()方法，通过生成一个叫做iris_tree.dot的图形定义文件将一个训练好的决策树模型可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line">    export_graphviz(</span><br><span class="line">            tree_clf,</span><br><span class="line">            out_file=image_path(<span class="string">"iris_tree.dot"</span>),</span><br><span class="line">            feature_names=iris.feature_names[<span class="number">2</span>:],</span><br><span class="line">            class_names=iris.target_names,</span><br><span class="line">            rounded=<span class="literal">True</span>,</span><br><span class="line">            filled=<span class="literal">True</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>然后，我们可以利用graphviz package 中的dot命令行，将.dot文件转换成 PDF 或 PNG 等多种数据格式。例如，使用命令行将.dot文件转换成.png文件的命令如下：</p>
<blockquote>
<p>Graphviz是一款开源图形可视化软件包，<a href="http://www.graphviz.org/" target="_blank" rel="noopener">http://www.graphviz.org/</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dot -Tpng iris_tree.dot -o iris_tree.png</span><br></pre></td></tr></table></figure>
<p>我们的第一个决策树如图</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/09.png" alt></p>
<p>节点的samples属性统计出它应用于多少个训练样本实例，例如，我们有一百个训练实例是花瓣长度大于 2.45 里面的（深度为 1， 右侧），在这 100 个样例中又有 54 个花瓣宽度小于 1.75cm（深度为 2，左侧）；节点的value属性告诉你这个节点对于每一个类别的样例有多少个，例如：右下角的节点中包含 0 个 Iris-Setosa，1 个 Iris-Versicolor 和 45 个 Iris-Virginica；最后，节点的Gini属性用于测量它的纯度：如果一个节点包含的所有训练样例全都是同一类别的，我们就说这个节点是纯的（Gini=0）</p>
<p>下面公式显示了训练算法如何计算第i个节点的 gini 分数 。例如， 深度为 2 的左侧节点基尼指数为：</p>
<script type="math/tex; mode=display">G_i=\sum_{k=1}^nP_{i,k}^2</script><p> $p_{i,k}$ 是第i个节点中训练实例为的k类实例的比例</p>
<blockquote>
<p>Scikit-Learn 用的是 CART 算法， CART 算法仅产生二叉树：每一个非叶节点总是只有两个子节点（只有是或否两个结果）。然而，像 ID3 这样的算法可以产生超过两个子节点的决策树模型</p>
</blockquote>
<p>下图显示了决策树的决策边界。粗的垂直线代表根节点（深度为 0）的决定边界：花瓣长度为 2.45 厘米。由于左侧区域是纯的（只有 Iris-Setosa），所以不能再进一步分裂。然而，右边的区域是不纯的，所以深度为 1 的右边节点在花瓣宽度为 1.75 厘米处分裂（用虚线表示）。又由于max_depth设置为 2，决策树在那里停了下来。但是，如果将max_depth设置为 3，两个深度为 2 的节点，每个都将会添加另一个决策边界（用虚线表示）</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/10.png" alt></p>
<h2 id="估计分类概率"><a href="#估计分类概率" class="headerlink" title="估计分类概率"></a>估计分类概率</h2><p>决策树还可以估计某个实例属于特定类k的概率：首先遍历树来查找此实例的叶节点，然后它返回此节点中类k的训练实例的比例。</p>
<p>例如，假设你发现了一个花瓣长 5 厘米，宽 1.5 厘米的花朵。相应的叶节点是深度为 2 的左节点，因此决策树应该输出以下概率：Iris-Setosa 为 0%（0/54），Iris-Versicolor 为 90.7%（49/54），Iris-Virginica 为 9.3%（5/54）。当然，如果你要求它预测具体的类，它应该输出 Iris-Versicolor（类别 1），因为它具有最高的概率。我们了测试一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tree_clf.predict_proba([[<span class="number">5</span>, <span class="number">1.5</span>]])</span><br><span class="line">array([[ <span class="number">0.</span> , <span class="number">0.90740741</span>, <span class="number">0.09259259</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tree_clf.predict([[<span class="number">5</span>, <span class="number">1.5</span>]])</span><br><span class="line">array([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h2 id="CART-训练算法"><a href="#CART-训练算法" class="headerlink" title="CART 训练算法"></a>CART 训练算法</h2><p>Scikit-Learn 用分裂回归树（Classification And Regression Tree，简称 CART）算法训练决策树（也叫“增长树”）。这种算法思想真的非常简单：</p>
<p>首先使用单个特征k和阈值$t_k$ (例如，“花瓣长度≤2.45cm”）将训练集分成两个子集。它如何选择k和$t_k$ 呢？它寻找到能够产生最纯粹的子集一对(k,$t_k$) ，然后通过子集大小加权计算.</p>
<p>算法会尝试最小化成本函数。方法如公式</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/11.png" alt></p>
<p>当它成功的将训练集分成两部分之后， 它将会继续使用相同的递归式逻辑继续的分割子集，然后是子集的子集。当达到预定的最大深度之后将会停止分裂（由max_depth超参数决定），或者是它找不到可以继续降低不纯度的分裂方法的时候。几个其他超参数（之后介绍）控制了其他的停止生长条件（min_samples_split，min_samples_leaf，min_weight_fraction_leaf，max_leaf_nodes）。</p>
<blockquote>
<p>正如您所看到的，CART 算法是一种贪婪算法：它贪婪地搜索最高级别的最佳分割方式，然后在每个深度重复该过程。 它不检查分割是否能够在几个级别中的全部分割可能中找到最佳方法。贪婪算法通常会产生一个相当好的解决方法，但它不保证这是全局中的最佳解决方案</p>
</blockquote>
<h2 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h2><p>在建立好决策树模型后， 做出预测需要遍历决策树， 从根节点一直到叶节点。决策树通常近似左右平衡，因此遍历决策树需要经历大致 $O(log_2^m)$ 个节点。由于每个节点只需要检查一个特征的值，因此总体预测复杂度仅为$O(log_2^m)$ ，与特征的数量无关。 所以即使在处理大型训练集时，预测速度也非常快.</p>
<p>然而，训练算法的时候（训练和预测不同）需要比较所有特征（如果设置了max_features会更少一些）<br>在每个节点的所有样本上。就有了$O(nmlog_2^m)$的训练复杂度。对于小型训练集（少于几千例），Scikit-Learn 可以通过预先设置数据（presort = True）来加速训练，但是这对于较大训练集来说会显着减慢训练速度。</p>
<h2 id="基尼不纯度或是信息熵"><a href="#基尼不纯度或是信息熵" class="headerlink" title="基尼不纯度或是信息熵"></a>基尼不纯度或是信息熵</h2><p>通常，算法使用 Gini 不纯度来进行检测， 但是你也可以通过将标准超参数设置为”entropy”来使用熵不纯度进行检测。这里熵的概念是源于热力学中分子混乱程度的概念，当分子井然有序的时候，熵值接近于 0</p>
<p>那么我们到底应该使用 Gini 指数还是熵(ID3,C4.5决策树)呢？ 事实上大部分情况都没有多大的差别：他们会生成类似的决策树。 基尼指数计算稍微快一点，所以这是一个很好的默认值。但是，也有的时候它们会产生不同的树，基尼指数会趋于在树的分支中将最多的类隔离出来，而熵指数趋向于产生略微平衡一些的决策树模型。</p>
<h2 id="正则化超参数"><a href="#正则化超参数" class="headerlink" title="正则化超参数"></a>正则化超参数</h2><p>决策树几乎不对训练数据做任何假设（于此相反的是线性回归等模型，这类模型通常会假设数据是符合线性关系的）。</p>
<p>如果不添加约束，树结构模型通常将根据训练数据调整自己，使自身能够很好的拟合数据，而这种情况下大多数会导致模型过拟合。</p>
<p>这一类的模型通常会被称为非参数模型，这不是因为它没有任何参数（通常也有很多），而是因为在训练之前没有确定参数的具体数量，所以模型结构可以根据数据的特性自由生长。</p>
<p>DecisionTreeClassifier类还有一些其他的参数用于限制树模型的形状:</p>
<blockquote>
<p>min_samples_split（节点在被分裂之前必须具有的最小样本数），min_samples_leaf（叶节点必须具有的最小样本数），min_weight_fraction_leaf（和min_samples_leaf相同，但表示为加权总数的一小部分实例），max_leaf_nodes（叶节点的最大数量）和max_features（在每个节点被评估是否分裂的时候，具有的最大特征数量）。增加min_hyperparameters或者减少max_hyperparameters会使模型正则化</p>
<p>预剪枝与后剪枝</p>
</blockquote>
<p>下图显示了对moons数据集进行训练生成的两个决策树模型，左侧的图形对应的决策树使用默认超参数生成（没有限制生长条件），右边的决策树模型设置为min_samples_leaf=4。很明显，左边的模型过拟合了，而右边的模型泛用性更好。</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/12.png" alt></p>
<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>决策树也能够执行回归任务，让我们使用 Scikit-Learn 的DecisionTreeRegressor类构建一个回归树，让我们用max_depth = 2在具有噪声的二次项数据集上进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">tree_reg = DecisionTreeRegressor(max_depth=<span class="number">2</span>)</span><br><span class="line">tree_reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>结果如图：</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/13.png" alt></p>
<p>这棵树看起来非常类似于你之前建立的分类树，它的主要区别在于，它不是预测每个节点中的样本所属的分类，而是预测一个具体的数值。例如，假设您想对  的新实例进行预测。从根开始遍历树，最终到达预测值等于 0.1106 的叶节点。该预测仅仅是与该叶节点相关的 110 个训练实例的平均目标值。而这个预测结果在对应的 110 个实例上的均方误差（MSE）等于 0.0151</p>
<p>下图的左侧显示的是模型的预测结果，如果你将max_depth=3设置为 3，模型就会如 6-5 图右侧显示的那样.注意每个区域的预测值总是该区域中实例的平均目标值。算法以一种使大多数训练实例尽可能接近该预测值的方式分割每个区域。</p>
<blockquote>
<p>译者注：图里面的红线就是训练实例的平均目标值，对应上图中的value</p>
</blockquote>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/14.png" alt></p>
<p>CART 算法的工作方式与之前处理分类模型基本一样，不同之处在于，现在不再以最小化不纯度的方式分割训练集，而是试图以最小化 MSE 的方式分割训练集</p>
<p>下面公式显示了成本函数，该算法试图最小化这个成本函数。</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/15.png" alt></p>
<p>和处理分类任务时一样，决策树在处理回归问题的时候也容易过拟合。如果不添加任何正则化（默认的超参数），你就会得到图 6-6 左侧的预测结果，显然，过度拟合的程度非常严重。而当我们设置了min_samples_leaf = 10，相对就会产生一个更加合适的模型了，就如图 6-6 所示的那样</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/16.png" alt></p>
<h2 id="不稳定性"><a href="#不稳定性" class="headerlink" title="不稳定性"></a>不稳定性</h2><p>它很容易理解和解释，易于使用且功能丰富而强大。然而，它也有一些限制，首先，你可能已经注意到了，决策树很喜欢设定正交化的决策边界，（所有边界都是和某一个轴相垂直的），这使得它对训练数据集的旋转很敏感，例如图 6-7 显示了一个简单的线性可分数据集。在左图中，决策树可以轻易的将数据分隔开，但是在右图中，当我们把数据旋转了 45° 之后，决策树的边界看起来变的格外复杂。尽管两个决策树都完美的拟合了训练数据，右边模型的泛化能力很可能非常差。</p>
<p>解决这个难题的一种方式是使用 PCA 主成分分析，这样通常能使训练结果变得更好一些。</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/17.png" alt></p>
<p>更加通俗的讲，决策时的主要问题是它对训练数据的微小变化非常敏感，举例来说，我们仅仅从鸢尾花训练数据中将最宽的 Iris-Versicolor 拿掉（花瓣长 4.8 厘米，宽 1.8 厘米），然后重新训练决策树模型，你可能就会得到图 6-8 中的模型。正如我们看到的那样，决策树有了非常大的变化（原来的如图 6-2），事实上，由于 Scikit-Learn 的训练算法是非常随机的，即使是相同的训练数据你也可能得到差别很大的模型（除非你设置了随机数种子）</p>
<p><img src="/2018/07/23/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（五）：决策树/18.png" alt></p>
<h1 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h1><ol>
<li>在 100 万例训练集上训练（没有限制）的决策树的近似深度是多少？</li>
<li>节点的基尼指数比起它的父节点是更高还是更低？它是通常情况下更高/更低，还是永远更高/更低？</li>
<li>如果决策树过拟合了，减少最大深度是一个好的方法吗？</li>
<li>如果决策树对训练集欠拟合了，尝试缩放输入特征是否是一个好主意？</li>
<li>如果对包含 100 万个实例的数据集训练决策树模型需要一个小时，在包含 1000 万个实例的培训集上训练另一个决策树大概需要多少时间呢？</li>
<li>如果你的训练集包含 100,000 个实例，设置<code>presort=True</code>会加快训练的速度吗？</li>
</ol>
<hr>
<p>1、$log_210^6\approx20$</p>
<p>2、子节点的基尼指数通常比父节点更低，这是由CART训练树损失函数决定的，它只会每次减少子节点权重求和后的Gini指数。然而子节点的Gini并不会永远小于父节点，比如现在根节点包括{A B A A A}五个样本，其Gini为$1-\frac{1}{5}^2-\frac{4}{5}^2=0.32$，划分样本后为{A B}和 {A A A} ，Gini分别为0.5和0。权重求和后的Gini为$\frac{2}{5}\times0.5+\frac{3}{5}\times0=0.2$，其Gini指数划分后只会比原来的更小。</p>
<p>3、如果决策树过度拟合训练集，那么减少max_depth是个不错的选择。</p>
<p>4、决策树不关心训练数据是否缩放或居中，这是他们的优势之一。 因此，如果决策树如果欠拟合，缩放输入功能只会浪费时间。</p>
<p>5、训练决策树的时间复杂度为$O(nmlog_2m)$，m为样本数，n为特征数。所以训练样本增加十倍，其训练时间会增加$K=(n\cdot 10m\cdot log_2(10m))=10\times log(10m)/log(m)$.如果m=10^6,则K$\approx$11.7</p>
<p>6、仅当数据集小于几千个实例时，预先训练训练集能加速训练。 如果它包含100,000个实例，则设置presort = True将大大减慢训练速度。</p>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/" itemprop="url">Sklearn 与 TensorFlow 机器学习实用指南（四）：支持向量机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-18T14:18:23+08:00">
                2018-07-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sklearn-与-TensorFlow-机器学习实用指南/" itemprop="url" rel="index">
                    <span itemprop="name">Sklearn 与 TensorFlow 机器学习实用指南</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9,756
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  36
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h1><h2 id="支持向量机条件描述"><a href="#支持向量机条件描述" class="headerlink" title="支持向量机条件描述"></a>支持向量机条件描述</h2><h3 id="样本分类"><a href="#样本分类" class="headerlink" title="样本分类"></a>样本分类</h3><p>Y∈{+1， -1}是样本的标签，分别代表两个不同的类。这里我们需要用这些样本去训练学习一个线性分类器（超平面）：$f(x)=sgn(w^Tx + b)$，也就是$w^Tx + b$大于0的时候，输出+1，小于0的时候，输出-1。sgn()表示取符号。而$g(x) =w^Tx + b=0$就是我们要寻找的分类超平面</p>
<p>也就是，对于任何一个正样本$y_i$=+1，它都要处于超平面的一边，也就是要保证：$y= w^Tx + b&gt;0$。对于任何一个负样本$y_i=-1$，它都要处于超平面的另一边，也就是要保证：$y = w^Tx + b&lt;0$这两个约束，其实可以合并成同一个式子：$y_i (w^Tx_i+b)\geq0$</p>
<h3 id="最大间隔"><a href="#最大间隔" class="headerlink" title="最大间隔"></a>最大间隔</h3><p>点到直线距离距离引理：设直线 L 的方程为Ax+By+C=0，点 P 的坐标为（Xo，Yo），则点 P 到直线 L 的距离为</p>
<script type="math/tex; mode=display">
\frac{\vert Ax_0+By_0+c\vert}{\sqrt{A^2+B^2}}</script><p><strong>间隔:间隔表示距离划分超平面最近的样本到划分超平面距离的两倍</strong>,即($x_i$为离直线最近的样本i)</p>
<script type="math/tex; mode=display">
\gamma=2\min_i \frac{1}{\Vert w \Vert}\vert w^Tx_i+b \vert</script><h3 id="约束条件"><a href="#约束条件" class="headerlink" title="约束条件"></a>约束条件</h3><p>所以，线性支持向量机的目标是找到一组合适的参数(<strong>w</strong>, b), 在满足分割样本的条件下，使得上诉间隔最大</p>
<script type="math/tex; mode=display">
\max_{w,b} \min_i \frac{2}{\Vert w \Vert}\vert w^Tx_i+b \vert \\ \\ \\s.t. \quad y_i(w^T+b)>0, i=1,2,...,m</script><p><strong>上面的优化问题十分复杂, 难以处理. 为了能在现实中应用, 我们希望能对其做一些简化, 使其变为可以求解的, 经典的凸二次规划 (QP) 问题</strong></p>
<blockquote>
<p>凸二次规划的优化问题是指目标函数是凸二次函数, 约束是线性约束的一类优化问题</p>
</blockquote>
<p>支持向量机的缩放引理：若（w* ,b*）是上面优化问题的解，那么对任何的r&gt;0,(rw*,rb*)仍是该问题的解。</p>
<p>由于对 (w, b) 的放缩不影响解, 为了简化优化问题,我们约束 (w, b) 使得最短距离固定为1.</p>
<script type="math/tex; mode=display">
\min_i \quad \vert w^Tx_i+b \vert = 1</script><p><strong>所以最后我们的优化目标可以等价为下面优化目标</strong>：</p>
<script type="math/tex; mode=display">
\min_{w,b} \quad \frac{1}{2}w^Tw \\ \\ \\s.t. \quad y_i(w^T+b)\geq1, i=1,2,...,m</script><p><strong>这是一个凸二次规划问题，除了用解决QP问题的常规方法之外，还可以应用拉格朗日对偶性，通过求解对偶问题得到最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一是对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题</strong>。</p>
<h2 id="拉格朗日函数与对偶形式"><a href="#拉格朗日函数与对偶形式" class="headerlink" title="拉格朗日函数与对偶形式"></a>拉格朗日函数与对偶形式</h2><h4 id="拉格朗日函数"><a href="#拉格朗日函数" class="headerlink" title="拉格朗日函数"></a>拉格朗日函数</h4><p>对于优化问题</p>
<script type="math/tex; mode=display">
\min_{u} \quad f(u) \\ \\ \\s.t. \quad g_i(u)\leq 0, i=1,2,...,m \\ \\ \\ \qquad  \ \ \ h_j(u)=0, j=1,2,...,n \qquad（6）</script><p>定义其拉格朗日函数为</p>
<script type="math/tex; mode=display">
L(u,\alpha,\beta):=f(u)+\sum_{i=1}^m\alpha_ig_i(u)+\sum_{j=1}^n\beta_jh_j(u)\\ \\ \\s.t.\quad \alpha_i\geq0</script><p><strong>公式6的优化问题等价于</strong></p>
<script type="math/tex; mode=display">
\min \limits_{u} \max \limits_{\alpha,\beta} \quad L(u,\alpha,\beta) \\ \\ \\ s.t. \alpha_i\geq0, \ \ i=1,2,...,m \qquad(8)</script><p>其证明过程如下</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/08.jpg" alt></p>
<p>其中，当$g_i$不满足约束时，即$g_i(u)&gt;0$,我们可以取$\alpha_i=\infty $;当$h_j$不满足约束时, 即$h_j(u)\neq 0$, 我们可以取$\beta_j=sign(h_j(u))\infty$，使得$\beta_jh_j(u)=\infty$. 当u满足约束时，由于$\alpha_i\geq 0,g_i(u)\leq 0$,则$\alpha_ig_i(u) \leq 0$.因此$\alpha_ig_i(u)$的最大值为0.</p>
<h4 id="KKT条件和对偶形式"><a href="#KKT条件和对偶形式" class="headerlink" title="KKT条件和对偶形式"></a>KKT条件和对偶形式</h4><p>公式8描述的优化问题在最优值必须满足如下条件(KKT完整条件放后面)</p>
<ul>
<li>主问题可行：$g_i(u)\leq 0,h_i(u)=0$;</li>
<li>对偶问题可行：$\alpha_i \geq 0$;</li>
<li>互补松弛：$\alpha_i g_i(u)=0$.</li>
</ul>
<p>主问题可行和对偶问题可行是公式6和公式8描述的优化问题的约束项。松弛互补是在主问题和对偶问题都可行的条件下的最大值</p>
<p>在对偶问题里，公式6描述的优化问题，其等价形式公式8称为主问题，其对偶问题为(最大最小条件互换)</p>
<script type="math/tex; mode=display">
\max \limits_{\alpha,\beta} \min \limits_{u}  \quad L(u,\alpha,\beta) \\ \\ \\ s.t. \alpha_i\geq0, \ \ i=1,2,...,m</script><ul>
<li>引理一：对偶问题是主 (primal) 问题的下界（最大值里取最小值肯定会大于最小值里取最大值）</li>
</ul>
<script type="math/tex; mode=display">
\max \limits_{\alpha,\beta} \min \limits_{u}  \quad L(u,\alpha,\beta)\leq \min \limits_{u} \max \limits_{\alpha,\beta} \quad L(u,\alpha,\beta)</script><ul>
<li>引理二(Slater条件)：当主问题为凸优化问题，即$f$和$g_i$ 为凸函数，$h_j$为仿射函数(类似于kx+b的一类函数)，且可行域中至少有一点使不等式约束严格成立时，对偶问题等价于原问题。(支持向量机的优化函数和约束函数都为凸函数)</li>
</ul>
<h2 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h2><p>线性支持向量机的拉格朗日函数为</p>
<script type="math/tex; mode=display">
L(w,b,\alpha):=\frac{1}{2}w^tw+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b))</script><p>其对偶问题为（并非主问题）</p>
<script type="math/tex; mode=display">
\max \limits_{\alpha}\min\limits_{w,b}\frac{1}{2}w^Tw+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b)) \\ \\ \\ s.t.\quad \alpha_i\geq0,\quad i=1,2,...,m. \qquad(12)</script><p>因为其内层的(w,b)属于无约束优化问题，我们可以通过令偏导等于0的方法得到(w,b)的最优值</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^m{a_iy_ix_i}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^m{a_iy_i=0}</script><p>将上面公式带入(12)消去(w,b)即可得(变为min是因为化简过程有个负号，转为求最小)</p>
<script type="math/tex; mode=display">
L\left( w,b,a \right) =\frac{1}{2}\sum_{i=1}^m{\sum_{j=1}^m{a_ia_jy_iy_j\left( x_i·x_j \right) -\sum_{i=1}^m{a_iy_i\left( \left( \sum_{j=1}^m{a_jy_jx_j} \right) ·x_i+b \right) +\sum_{i=1}^m{a_i}}}}  \\ \\ \\ =-\frac{1}{2}\sum_{i=1}^m{\sum_{j=1}^m{a_ia_jy_iy_j\left( x_i·x_j \right) +\sum_{i=1}^m{a_i}}}</script><p><strong>故最终化简得到线性支持向量机对偶型</strong></p>
<script type="math/tex; mode=display">
\min \limits_{\alpha} \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum_{i=1}^m\alpha_i \\ \\ \\ s.t.\sum_{i=1}^m\alpha_iy_i=0, \\ \\ \\ \alpha_i\geq 0,\quad i=1,2,...,m.</script><p>上诉为线性支持向量机的最终化简对偶问题，其优化模型还要满足以下的KKT条件</p>
<ul>
<li>主问题可行：$1-y_i(w^Tx_i+b)\leq0$</li>
<li>对偶问题可行：$\alpha_i\geq0$</li>
<li>互补松弛：$\alpha_i(1-y_i(w^Tx_i+b))=0$</li>
</ul>
<p>根据KKT条件，我们可以得到两个非常重要的结论（这么多证明得出两个结论，不容易啊）</p>
<ul>
<li>结论一：落在间隔边界上的支持向量，其对应的样本的对偶变量$\alpha_i&gt;0$</li>
</ul>
<p>证明：由KKT可知，根据$\alpha_i(1-y_i(w^Tx_i+b))=0$ 当;$\alpha_i&gt;0$时，有$1-y_i(w^Tx_i+b)=0$ ，即$y_i(w^Tx_i+b)=1$</p>
<ul>
<li>结论二：支持向量机的参数(w,b)仅仅由支持向量决定，与其他样本无关</li>
</ul>
<p>证明：由于对偶变量$\alpha_i&gt;0$对应的样本是支持向量，</p>
<script type="math/tex; mode=display">
w=\sum_{i=1}^m\alpha_iy_ix_i=\sum_{i:\alpha_i=0}^m0\cdot y_ix_i+\sum_{i:\alpha_i>0}^m\alpha_iy_ix_i=\sum_{i\in SV}\alpha_iy_ix_i</script><p>其中SV代表所有支持向量的集合.b可以由互补松弛算出，对于某一支持向量$x_s$ 及其标记$y_s$，由于$y_s(w^Tx_s+b)=1$,</p>
<p>则有</p>
<script type="math/tex; mode=display">
b=y_s-w^Tx_s=y_s-\sum_{i\in SV}\alpha_iy_ix_i^Tx_s</script><p><strong>根据对偶公式(16)先计算最优解$\alpha_1,\alpha_2,…,\alpha_m$， 然后可以得到w和b，这样我们就可以写出分类超平面$w^\ast\cdot x+b^\ast=0$ 和分类决策函数$f(x)=sign(w^\ast·x+b^\ast)$</strong> </p>
<blockquote>
<p> 在实践中，为了得到对b更稳健的估计,通常使用对所有支持向量求解得到b的平均值</p>
</blockquote>
<h2 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h2><p>对每个样本点引进一个松弛变量$\xi \geqslant 0 $，使函数间隔加上松弛变量大于等于1.这样，<strong>约束条件</strong>变成</p>
<script type="math/tex; mode=display">
y_i\left( w·x_i+b \right) \geqslant 1-\xi _i</script><p>当然，如果我们允许$\xi \geqslant 0 $任意大的话，那任意的超平面都是符合条件的了。这里我们引入松弛变量$\xi$，松弛变量值越大，当样本违背约束的程度越大，当松弛变量为0时，样本分类正确。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi \geqslant 0 $的总和也要最小，<strong>优化最大间隔和违背约束程度越小</strong>，<strong>目标函数</strong>由原来的$\frac{1}{2}||w||^2$变成</p>
<script type="math/tex; mode=display">
\frac{1}{2}||w||^2+C\sum_{i=1}^N{\xi _i}</script><p>这里，$C&gt;0$称为惩罚参数，一般事先由应用问题决定，用于权衡优化间隔和少量样本违背大间隔约束这两个目标，C越大时(看重这一项)对误分类的惩罚增大，，有更多的样本满足大间隔约束。C值小时对误分类的惩罚减小，允许有一些样本不满足大间隔约束.。最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量最小间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。</p>
<p><strong>则有软间隔支持向量机基本型</strong>：软间隔支持向量机旨在找到一组合适的参数 (w, b), 使得</p>
<script type="math/tex; mode=display">
\underset{w,b,\xi}{\min}\ \frac{1}{2}||w||^2+C\sum_{i=1}^m{\xi _i} \\ \\ \\ s.t.\ \ y_i\left( w^T·x_i+b \right) \geqslant 1-\xi _i\ ,\ i=1,2,···,m \\ \\ \\ \xi _i\geqslant 0,\ i=1,2,···\mathrm{，}m  \qquad(21)</script><p>可证明w的解是唯一的，但b的解不唯一，b的解存在于一个区间。</p>
<p>用之前的方法将限制加入到目标函数中，得到如下<strong>原始最优化问题的拉格朗日函数</strong>(不等式为大于转为负号形式)：</p>
<script type="math/tex; mode=display">
L\left( w,b,\xi ,\alpha,\beta \right) =\frac{1}{2}||w||^2+C\sum_{i=1}^m{\xi _i-\sum_{i=1}^m{\alpha_i\left( y_i\left( w^T·x_i+b \right) -1+\xi _i \right) -\sum_{i=1}^m{\beta_i\xi _i}}} \\ \\ \\ s.t.\alpha\geq0 ,\beta\geq0.</script><p>其对偶问题为</p>
<script type="math/tex; mode=display">
\max_\limits{\alpha,\beta}\min_\limits{w,b,\xi} L\left( \alpha,\beta,w,b,\xi \right) \qquad(23)</script><p>首先求拉格朗日函数针对$w,b,\xi$求极小</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^m{a_iy_ix_i} \\ \\ \\ \frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^m{\alpha_iy_i=0} \\ \\ \\ \frac{\partial L}{\partial \xi _i}=0\Rightarrow C-\alpha_i-\beta_i=0，i=1,2,3···,m \qquad(24)</script><p>其中，有$\alpha\geq0,\beta\geq0$。因为$\beta_i=C-\alpha_i\geq0$，不失一般性，我们可以约束$0\leq\alpha_i\leq C$，从而<strong>去掉变量</strong>$\beta_i$.将这些求解变量带入拉格朗日对偶型即公式(23)，得到和原来一样的目标函数，唯一的区别就是现在拉格朗日乘子$\alpha$多了一个上限C。</p>
<p><strong>最终化简的软间隔支持向量机对偶型</strong>问题(软间隔支持向量机的对偶问题等价于找到一组合适的$\alpha$使得)</p>
<script type="math/tex; mode=display">
\underset{\alpha}{\min}\ \ \frac{1}{2}\sum_{i=1}^m{\sum_{j=1}^m{\alpha_i\alpha_jy_iy_j⟨x_i·x_j ⟩-\sum_{i=1}^m{\alpha_i}}} \\ \\ \\ s.t.\ \sum_{i=1}^m{a_iy_i=0} \\ \\ \\ 0\le a_i\le C\ ,\ \ i=1,2,···,m</script><p>上诉为最终化简的软间隔支持向量机对偶型问题，其优化模型还要满足以下的KKT条件</p>
<ul>
<li>主问题可行：$1-\xi_i-y_i(w^Tx_i+b)\leq0$,$-\xi\leq0$;</li>
<li>对偶问题可行：$\alpha_i\geq0$, $\beta_i\geq0$;</li>
<li>互补松弛：$\alpha_i(1-\xi_i-y_i(w^Tx_i+b))=0$,$\beta_i\xi_i=0$.</li>
</ul>
<p>根据KKT条件，我们也可以得到两个结论：</p>
<ul>
<li>结论一：软间隔支持向量机中, 支持向量落在最大间隔边界, 内部, 或被错误分类的样本</li>
</ul>
<p>证明：由软间隔支持向量机的 KKT 条件可知，$\alpha_i(1-\xi_i-y_i(w^Tx_i+b))=0$且$\beta_i\xi_i=0$。当$\alpha_i&gt;0$时，$1-\xi_i-y_i(w^Tx_i+b)=0$，进一步可以分为两种情况：</p>
<p>第一：$0&lt;\alpha_i&lt;’C$， 此时$\beta_i$=C-$\alpha_i$&gt;0(公式24的偏导约束)。因此$\xi_i$=0，即样本恰好落在最大间隔边界上。</p>
<p>第二：$\alpha_i=C$， 此时$\beta_i=C-\alpha_i=0$. 若$\xi$ &lt;1’则分类正确，该样本落在最大间隔内部(间隔边界与分离超平面之间)；若$\xi$=1，该样本在分隔超面上。若$\xi$&gt;0样本位于分离超平面误分的一侧。</p>
<ul>
<li>结论二：支持向量机的参数 (w, b) 仅由支持向量决定,与其他样本无关。</li>
</ul>
<p>证明：和线性支持向量机证明方式相同.</p>
<p>求解w，b的方式也与线性支持向量机相同。注意：对任一适合条件0&lt;a&lt;C都可求得一个$b^∗$，即这些点正好是位于分隔边界上的点来求出b点的。但是由于原始问题对b的求解并不唯一，所以实际计算时可以取在所有符合条件的样本点上的平均值。</p>
<script type="math/tex; mode=display">
w^*=\sum_{i=1}^N{a_{i}^{*}y_ix_i}</script><p>选择a的一个分量的一个分量$a_i$适合约束条件适合约束条件0&lt;$a_i$&lt;C,计算</p>
<script type="math/tex; mode=display">
b^*=y_j-\sum_{i=1}^N{a_{i}^{*}y_i⟨x_i·x_j ⟩}</script><h2 id="Hinge损失函数"><a href="#Hinge损失函数" class="headerlink" title="Hinge损失函数"></a>Hinge损失函数</h2><p>线性支持向量机学习除了原始最优化问题，还有另外一种解释，就是最优化以下目标函数.下标”+”表示以下取正值的函数z=max(0，z)：</p>
<script type="math/tex; mode=display">
\sum_i^{m}[1-y_i(w·x_i+b)]_++\lambda||w||^2</script><p>目标函数的第一项是经验损失或经验风险，函数$L(y·(w·x+b))=[1-y(w·x+b)]_+$ ,称为<strong>合页损失函数（hinge loss function）</strong>.这就是说，当样本点$(x_i,y_i)$被分类正确且函数间隔（确信度）$y_i(w\cdot x_i+b)$大于1时，损失是0，否则损失是$1-y_i(w\cdot x_i+b)$。目标函数目标函数的第二项是系数的L2范数，是正则项。下面要证明上面的公式等价于线性支持向量机原始最优化问题，即(公式21)</p>
<p>证明：先令$[1-y<em>i(w·x_i+b)]</em>+=\xi_i$， 则可以写成$\xi_i=\max(0,\ 1-y_i(w·x_i+b) )$ 。当样本满足约束分类正确时，$y_i(w\cdot x_i+b)&gt;1$， 有$1-y_i(w\cdot x_i+b)\leq0$，得$\xi_i=0$; 当当样本不满足约束时分类错误时， 有$\xi_i=1-y_i(w\cdot x_i+b)$ </p>
<p>故公式21的两个约束条件满足，其最优问题可以写作</p>
<script type="math/tex; mode=display">
\underset{w,b}{min}\sum_{i=1}^m\xi_i+\lambda||w||^2</script><p>若取$\lambda =\frac{1}{2C}$， 则$\underset{w,b}{min} \frac{1}{C}(\frac{1}{2} ||w||^2+C\sum_{i=1}^N \xi_i)$ 与原始最优问题等价</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/09.png" alt></p>
<p>图中还画出了0-1损失函数，可以认为它是一个二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate function）。</p>
<p>图中虚线显示的是感知机的损失函数，相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0，也就是说，合页损失函数对学习有更高的要求。</p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>线性可分问题：既然在原始的特征空间$R^d$不是线性可分的, 支持向量机希望通过一个映射 $\phi$ :$R^d$ →<br> $R^{\widetilde{d}}$,使得数据在新的空间 $R^{\widetilde{d}}$ 是线性可分的.</p>
<p>令 $\phi$(x)代表将样本 x 映射到 $R^{\widetilde{d}}$中的特征向量,参数 w 的维数也要相应变为 $\widetilde{d}$维.</p>
<p>在上面的支持向量机对偶公式中，注意到，被映射到高维的特征向量总是以成对内积的形式存在，即 $\phi (x_i)^T\phi(x_j)$ ,如果先计算特征在$R^{\widetilde{d}}$ 空间的映射, 再计算内积, 复杂度是O($\widetilde{d}$) ,当特征被映射到非常高维的空间, 甚至是无穷维空间时, 这将会是沉重的存储和计算负担.</p>
<p>核技巧旨在将特征映射和内积这两步运算压缩为一步, 并且使复杂度由O($\widetilde{d}$)降为O($d$),即, 核技巧希望构造一个核函数 $\kappa (x_i,x_j) $ ,使得</p>
<script type="math/tex; mode=display">
\kappa(x_i,x_j)=\phi (x_i)^T\phi(x_j)</script><p>并且，$\kappa (x_i,x_j) $ 的计算复杂度是 O($d$) .</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><h2 id="软间隔分类"><a href="#软间隔分类" class="headerlink" title="软间隔分类"></a>软间隔分类</h2><p>如果我们严格地规定所有的数据都不在“街道”上，都在正确地两边，称为<strong>硬间隔分类</strong>，硬间隔分类有两个问题，第一，只对线性可分的数据起作用，第二，对异常点敏感。</p>
<p>为了避免上述的问题，我们更倾向于使用更加软性的模型。目的在保持“街道”尽可能大和避免间隔违规（例如：数据点出现在“街道”中央或者甚至在错误的一边）之间找到一个良好的平衡。这就是软间隔分类。</p>
<p><strong>在 Scikit-Learn 库的 SVM 类，你可以用C超参数（惩罚系数）来控制这种平衡：较小的C会导致更宽的“街道”，但更多的间隔违规</strong>。下图显示了在非线性可分隔的数据集上，两个软间隔SVM分类器的判定边界。左边图中，使用了较大的C值，导致更少的间隔违规，但是间隔较小。右边的图，使用了较小的C值，间隔变大了，但是许多数据点出现在了“街道”上。然而，<strong>第二个分类器似乎泛化地更好</strong>：事实上，在这个训练数据集上减少了预测错误，因为实际上大部分的间隔违规点出现在了判定边界正确的一侧</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/01.jpg" alt></p>
<blockquote>
<p>如果你的 SVM 模型过拟合，你可以尝试通过减小超参数C去调整(街道更宽)</p>
<p>SVM 特别适合复杂的分类，而中小型的数据集分类中很少用到。另外，SVM 对特征缩放比较敏感，要先做数据缩放处理。</p>
</blockquote>
<p>以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型（使用<code>LinearSVC</code>类，超参数<code>C=1</code>，hinge 损失函数）来检测 Virginica 鸢尾花。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)] <span class="comment"># petal length, petal width</span></span><br><span class="line">y = (iris[<span class="string">"target"</span>] == <span class="number">2</span>).astype(np.float64) <span class="comment"># Iris-Virginica</span></span><br><span class="line"></span><br><span class="line">svm_clf = Pipeline((</span><br><span class="line">        (<span class="string">"scaler"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"linear_svc"</span>, LinearSVC(C=<span class="number">1</span>, loss=<span class="string">"hinge"</span>)),</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">svm_clf.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line">Then, <span class="keyword">as</span> usual, you can use the model to make predictions:</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>svm_clf.predict([[<span class="number">5.5</span>, <span class="number">1.7</span>]])</span><br><span class="line">array([ <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>不同于 Logistic 回归分类器，SVM 分类器不会输出每个类别的概率。</p>
<p>SVM的分类器为SVC类，回归则为SVR类。</p>
</blockquote>
<p>作为一种选择，你可以在 SVC 类，使用<code>SVC(kernel=&quot;linear&quot;, C=1)</code>，但是它比较慢，尤其在较大的训练集上，所以一般不被推荐。另一个选择是使用<code>SGDClassifier</code>类，即<code>SGDClassifier(loss=&quot;hinge&quot;, alpha=1/(m*C))</code>。它应用了随机梯度下降（SGD 见第四章）来训练一个线性 SVM 分类器。尽管它不会和<code>LinearSVC</code>一样快速收敛，但是对于处理那些不适合放在内存的大数据集是非常有用的，或者处理在线分类任务同样有用。</p>
<blockquote>
<p><code>LinearSVC</code>要使偏置项规范化，首先你应该集中训练集减去它的平均数，完成数据中心化。如果你使用了<code>StandardScaler</code>，那么它会自动处理。此外，确保你设置<code>loss</code>参数为<code>hinge</code>，因为它不是默认值。最后，为了得到更好的效果，你需要将<code>dual</code>参数设置为<code>False</code>，除非特征数比样本量多。</p>
</blockquote>
<h2 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h2><ol>
<li><strong>铰链损失（Hinge Loss）</strong>：主要用于支持向量机（SVM） 中， 二分类<em>SVM</em>等于Hinge损失+ L2正则化。</li>
<li><strong>互熵损失 （Cross Entropy Loss，Softmax Loss ）</strong>：用于Logistic 回归与Softmax 分类中； </li>
<li><strong>平方损失（Square Loss）</strong>：主要是最小二乘法（OLS）中； </li>
<li><strong>指数损失（Exponential Loss）</strong> ：主要用于Adaboost 集成学习算法中； </li>
</ol>
<p>Hinge loss 的叫法来源于其损失函数的图形，为一个折线，通用的函数表达式为：</p>
<script type="math/tex; mode=display">
L(m_i) = max(0,1-m_i(w)) =\max(0, 1-y\tilde{y}), 其中y=\pm 1</script><p>表示如果被正确分类，损失是0，否则损失就是 $1-m_i(w)​$</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/02.JPG" alt></p>
<p>在机器学习中，Hing 可以用来解 <strong>间距最大化</strong> 的问题，最有代表性的就是SVM 问题，最初的SVM 优化函数如下： </p>
<script type="math/tex; mode=display">
\underset{w,\zeta}{argmin} \frac{1}{2}||w||^2+ C\sum_i \zeta_i \\
st.\quad \forall y_iw^Tx_i \geq 1- \zeta_i \\
\zeta_i \geq 0</script><p>将约束项进行变形，则为： </p>
<script type="math/tex; mode=display">
\zeta_i \geq 1-y_iw^Tx_i</script><p>则损失函数可以进一步写为： </p>
<script type="math/tex; mode=display">
\begin{equation}\begin{split}J(w)&=\frac{1}{2}||w||^2 + C\sum_i max(0,1-y_iw^Tx_i) \\
&= \frac{1}{2}||w||^2 + C\sum_i max(0,1-m_i(w)) \\
&= \frac{1}{2}||w||^2 + C\sum_i L_{Hinge}(m_i)
\end{split}\end{equation}</script><p>因此， <strong>SVM 的损失函数可以看作是 L2-norm 和 Hinge loss 之和</strong>。</p>
<h2 id="非线性支持向量机分类"><a href="#非线性支持向量机分类" class="headerlink" title="非线性支持向量机分类"></a>非线性支持向量机分类</h2><h3 id="创建多项式特征"><a href="#创建多项式特征" class="headerlink" title="创建多项式特征"></a>创建多项式特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">polynomial_svm_clf = Pipeline((</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">3</span>)),</span><br><span class="line">        (<span class="string">"scaler"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"svm_clf"</span>, LinearSVC(C=<span class="number">10</span>, loss=<span class="string">"hinge"</span>))</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">polynomial_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>
<h3 id="多项式核"><a href="#多项式核" class="headerlink" title="多项式核"></a>多项式核</h3><p>添加多项式特征很容易实现，不仅仅在 SVM，在各种机器学习算法都有不错的表现，但是<strong>低次数的多项式不能处理非常复杂的数据集，而高次数的多项式却产生了大量的特征，会使模型变得慢.</strong></p>
<p>幸运的是，当你使用 SVM 时，你可<strong>以运用一个被称为“核技巧”（kernel trick）的神奇数学技巧。它可以取得就像你添加了许多多项式，甚至有高次数的多项式，一样好的结果。所以不会大量特征导致的组合爆炸，因为你并没有增加任何特征</strong>。超参数<code>coef0</code>控制了高阶多项式与低阶多项式对模型的影响。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">poly_kernel_svm_clf = Pipeline((</span><br><span class="line">        (<span class="string">"scaler"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"svm_clf"</span>, SVC(kernel=<span class="string">"poly"</span>, degree=<span class="number">3</span>, coef0=<span class="number">1</span>, C=<span class="number">5</span>))</span><br><span class="line">    ))</span><br><span class="line">poly_kernel_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>
<h3 id="增加相似特征"><a href="#增加相似特征" class="headerlink" title="增加相似特征"></a>增加相似特征</h3><p><strong>另一种解决非线性问题的方法是使用相似函数（similarity funtion）计算每个样本与特定地标（landmark）的相似度。</strong>例如，让我们来看看前面讨论过的一维数据集，<strong>并在x1=-2和x1=1之间增加两个地标</strong>。接下来，我们定义一个相似函数，即高斯径向基函数（Gaussian Radial Basis Function，RBF），设置γ = 0.3。</p>
<script type="math/tex; mode=display">
RBF:\quad \phi_{\gamma}(x, \ell) = exp(-\gamma \|x - \ell \|^2)</script><p>它是个从 0 到 1 的钟型函数，值为 0 的离地标很远，值为 1 的在地标上。<strong>现在我们准备计算新特征。例如，我们看一下样本x0=-1：它距离第一个地标距离(x1=-2)是 1，距离第二个地标(x1=1)是 2。因此它的新特征为x2=exp((-0.3 × 1)^2)≈0.74和x3=exp((-0.3 × 2)^2)≈0.30</strong>。右边的图显示了特征转换后的数据集（删除了原始特征），正如你看到的，它现在是线性可分了。</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/03.jpg" alt></p>
<p><strong>你可能想知道如何选择地标。最简单的方法是在数据集中的每一个样本的位置创建地标。这将产生更多的维度从而增加了转换后数据集是线性可分的可能性</strong>。<strong>但缺点是，m个样本，n个特征的训练集被转换成了m个实例，m个特征的训练集（假设你删除了原始特征）。这样一来，如果你的训练集非常大，你最终会得到同样大的特征</strong></p>
<h3 id="高斯-RBF-核"><a href="#高斯-RBF-核" class="headerlink" title="高斯 RBF 核"></a>高斯 RBF 核</h3><p>就像多项式特征法一样，相似特征法对各种机器学习算法同样也有不错的表现。但是在所有额外特征上的计算成本可能很高，特别是在大规模的训练集上。然而，“核” 技巧再一次显现了它在 SVM 上的神奇之处：<strong>高斯核让你可以获得同样好的结果成为可能，就像你在相似特征法添加了许多相似特征一样</strong>，但事实上，你并不需要在RBF添加它们。我们使用 SVC 类的高斯 RBF 核来检验一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rbf_kernel_svm_clf = Pipeline((</span><br><span class="line">        (<span class="string">"scaler"</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">"svm_clf"</span>, SVC(kernel=<span class="string">"rbf"</span>, gamma=<span class="number">5</span>, C=<span class="number">0.001</span>))</span><br><span class="line">    ))</span><br><span class="line">rbf_kernel_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>下图显示了用不同的超参数gamma (γ)和C训练的模型<strong>。增大γ使钟型曲线更窄，导致每个样本的影响范围变得更小：即判定边界最终变得更不规则，在单个样本周围环绕。相反的，较小的γ值使钟型曲线更宽，样本有更大的影响范围，判定边界最终则更加平滑。所以γ是可调整的超参数：如果你的模型过拟合，你应该减小γ值，若欠拟合，则增大γ（与超参数C相似）。</strong></p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/04.JPG" alt></p>
<p>还有其他的核函数，但很少使用。例如，一些核函数是专门用于特定的数据结构。在对文本文档或者 DNA 序列进行分类时，有时会使用字符串核（String kernels）（例如，使用 SSK 核（string subsequence kernel）或者基于编辑距离（Levenshtein distance）的核函数）</p>
<blockquote>
<p>这么多可供选择的核函数，你如何决定使用哪一个？一般来说，你应该先尝试线性核函数（记住LinearSVC比SVC(kernel=”linear”)要快得多），尤其是当训练集很大或者有大量的特征的情况下。如果训练集不太大，你也可以尝试高斯径向基核（Gaussian RBF Kernel），它在大多数情况下都很有效。如果你有空闲的时间和计算能力，你还可以使用交叉验证和网格搜索来试验其他的核函数，特别是有专门用于你的训练集数据结构的核函数</p>
</blockquote>
<h2 id="计算复杂性"><a href="#计算复杂性" class="headerlink" title="计算复杂性"></a>计算复杂性</h2><p><strong>LinearSVC类基于liblinear库，它实现了线性 SVM 的优化算法。它并不支持核技巧</strong>，但是它样本和特征的数量几乎是线性的：训练时间复杂度大约为O(m × n)。</p>
<p>如果你要非常高的精度，这个算法需要花费更多时间。这是由容差值超参数ϵ（在 Scikit-learn 称为tol）控制的。大多数分类任务中，使用默认容差值的效果是已经可以满足一般要求。</p>
<p><strong>SVC 类基于libsvm库，它实现了支持核技巧的算法</strong>。训练时间复杂度通常介于O(m^2 × n)和O(m^3 × n)之间。不幸的是，<strong>这意味着当训练样本变大时，它将变得极其慢</strong>（例如，成千上万个样本）。这个算法对于复杂但小型或中等数量的数据集表现是完美的。然而，它能对特征数量很好的缩放，尤其对稀疏特征来说（sparse features）（即每个样本都有一些非零特征）。在这个情况下，算法对每个样本的非零特征的平均数量进行大概的缩放。下表对 Scikit-learn 的 SVM 分类模型进行比较。</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/05.jpg" alt></p>
<h2 id="SVM-回归"><a href="#SVM-回归" class="headerlink" title="SVM 回归"></a>SVM 回归</h2><p>正如我们之前提到的，<strong>SVM 算法应用广泛：不仅仅支持线性和非线性的分类任务，还支持线性和非线性的回归任务</strong>。技巧在于逆转我们的目标：限制间隔违规的情况下，<strong>不是试图在两个类别之间找到尽可能大的“街道”（即间隔）。SVM 回归任务是限制间隔违规情况下，尽量放置更多的样本在“街道”上</strong>。“街道”的宽度由超参数ϵ控制。下图显示了在一些随机生成的线性数据上，两个线性 SVM 回归模型的训练情况。一个有较大的间隔（ϵ=1.5），另一个间隔较小（ϵ=0.5）</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/06.jpg" alt></p>
<p>添加更多的数据样本在间隔之内并不会影响模型的预测，因此，这个模型认为是不敏感的（ϵ-insensitive）。</p>
<p>你可以使用 Scikit-Learn 的LinearSVR类去实现线性 SVM 回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVR</span><br><span class="line">svm_reg = LinearSVR(epsilon=<span class="number">1.5</span>)</span><br><span class="line">svm_reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<p><strong>处理非线性回归任务，你可以使用核化的 SVM 模型</strong>。比如，图显示了在随机二次方的训练集，使用二次方多项式核函数的 SVM 回归。左图是较小的正则化（即更大的C值），右图则是更大的正则化（即小的C值）。</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/07.jpg" alt></p>
<p>下面的代码使用了 Scikit-Learn 的SVR类（支持核技巧）。在回归任务上，SVR类和SVC类是一样的，并且LinearSVR是和LinearSVC等价。LinearSVR类和训练集的大小成线性（就像LinearSVC类），当训练集变大，SVR会变的很慢（就像SVC类）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"></span><br><span class="line">svm_poly_reg = SVR(kernel=<span class="string">"poly"</span>, degree=<span class="number">2</span>, C=<span class="number">100</span>, epsilon=<span class="number">0.1</span>)</span><br><span class="line">svm_poly_reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>SVM 也可以用来做异常值检测，详情见 Scikit-Learn 文档</p>
</blockquote>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="拉格朗日子乘与KKT"><a href="#拉格朗日子乘与KKT" class="headerlink" title="拉格朗日子乘与KKT"></a>拉格朗日子乘与KKT</h2><h3 id="等式约束问题"><a href="#等式约束问题" class="headerlink" title="等式约束问题"></a>等式约束问题</h3><p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/10.png" alt></p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/11.png" alt></p>
<p><strong>可以得到一个重要的结论：▽f(x)一定与▽h(x)平行，故其关系可以写成▽f(x) =λ ▽h(x).</strong></p>
<p>扩展到高维度等式：f(x)为目标优化函数，hi(x)为约束等式，其表达式恒等于0。F(x)为等价的拉格朗日子乘函数。</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/12.png" alt></p>
<p>计算 F 对x与$\lambda$ 的偏导数并令其为零，可得最优解的必要条件：</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/13.png" alt></p>
<p><strong>其中第一式为定常方程式(stationary equation)</strong>，第二式为约束条件。也就是之前已经证明过得结果</p>
<p><strong>故等式约束要满足的条件</strong>为</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/14.png" alt></p>
<h3 id="不等式约束问题"><a href="#不等式约束问题" class="headerlink" title="不等式约束问题"></a>不等式约束问题</h3><p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/15.jpg" alt></p>
<p>这两种情况的最佳解具有两种不同的必要条件</p>
<ol>
<li>内部解：h(x)&lt;0，不满足h(x)的等式约束，故不在边界上，在可行域的内部。在约束条件无效的情形下， h(x)不起作用，约束优化问题退化为无约束优化问题，因此驻点(最优解)<strong>满足▽f(x)=0且$\lambda$=0</strong> ($\lambda$为0才能消去F(x)的约束项)。</li>
<li>边界解：在约束条件有效的情形下，约束不等式变成等式约束，有h(x)=0。和等式约束做相同的处理。另外，存在$\lambda$ 使得$\bigtriangledown f=-\lambda \bigtriangledown g$，但这里$\lambda$ 的正负号是有其意义的。<strong>因为我们希望最小化</strong> f，梯度$\bigtriangledown f$ <strong>(函数</strong> f<strong>在点</strong>x<strong>的最陡上升方向)应该指向可行域的内部(因为你的最优解最小值是在边界取得的)，但</strong> $\bigtriangledown g$<strong>指向可行域的的外部(即 g(x)&gt;0的区域，因为你的约束是小于等于0)，因此$\lambda \geq0$，称为对偶可行性(dual feasibility)</strong>。</li>
</ol>
<p>所以，不论是内部解或边界解，$\lambda h(x)=0$恒成立(不同情况总有一个为0)，称为互补松弛性(complementary slackness)。整合上述两种情况，最佳解的必要条件包括拉格朗日函数常定方程式、主问题可行，对偶可行，和互补松弛。</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/16.jpg" alt></p>
<h2 id="对偶性"><a href="#对偶性" class="headerlink" title="对偶性"></a>对偶性</h2><p>一个优化问题，通过求出它的 dual problem ，在只有 weak duality 成立的情况下，我们至少可以得到原始问题的一个下界。而如果 strong duality 成立，则可以直接求解 dual problem 来解决原始问题，就如同经典的 SVM 的求解过程一样。有可能 dual problem 比 primal problem 更容易求解，或者 dual problem 有一些优良的结构（例如 SVM 中通过 dual problem 我们可以将问题表示成数据的内积形式从而使得 kernel trick 的应用成为可能）。此外，还有一些情况会同时求解 dual 和 primal problem ，比如在迭代求解的过程中，通过判断 duality gap 的大小，可以得出一个有效的迭代停止条件.</p>
<h2 id="支持向量机的其他变体"><a href="#支持向量机的其他变体" class="headerlink" title="支持向量机的其他变体"></a>支持向量机的其他变体</h2><h3 id="Prob-SVM"><a href="#Prob-SVM" class="headerlink" title="Prob SVM."></a>Prob SVM.</h3><p><strong>Prob SVM</strong>. 对数几率回归可以估计出样本属于正类的概率, 而支持向量机只能判断样本属于正类或负类,无法得到概率.Prob SVM 先训练一个支持向量机, 得到参数(w,b)。再令$s_i:=y_iw^Tx_i+b$，将${(s_1,y_1),(s_2,y_2),…,(s_m,y_m)}$当做新的训练数据训练一个对数几率回归模型，得到参数$(\theta_1,\theta_0)$.因此，ProbSVM 的假设函数为</p>
<script type="math/tex; mode=display">
h(x):=sigm(\theta_1(w^Tx_i+b)+\theta_0)</script><p>对数几率回归模型可以认为是对训练得到的支持向量机的微调, 包括尺度 (对应$\theta_1)$和平移(对应$\theta_0$).</p>
<h3 id="多分类支持向量机"><a href="#多分类支持向量机" class="headerlink" title="多分类支持向量机"></a>多分类支持向量机</h3><p>支持向量机也可以扩展到多分类问题中. 对于 K 分类问题, 多分类支持向量机有 K组参数 ${(w_1,b_1),(w_2,b_2),…,(w_K,b_K)}$并希望模型对于属于正确标记的结果以 1 的间隔高于其他类的结果, 形式化如下</p>
<script type="math/tex; mode=display">
\min \limits_{W,b} \frac{1}{m}\sum_{i=1}^m\sum_{k=1}^Kmax(0,(w_{yi}^Tx_i+b_{yi})-(w_{k}^Tx_i+b_{k}+1)+\frac{2}{\lambda}\sum_{k=1}^Kw_k^Tw_k</script><h3 id="支持向量回归-SVR"><a href="#支持向量回归-SVR" class="headerlink" title="支持向量回归 (SVR)"></a>支持向量回归 (SVR)</h3><p>支持向量回归 (SVR). 经典回归模型的损失函数度量了模型的预测 $h(x_i)$与$y_i$的差别, 支持向量回归能够容忍$h(x_i)$与$y_i$之间小于$\epsilon$ 的偏差.令$s:=y-w^Tx+b$，我们定义$\epsilon$不敏感损失为</p>
<p><img src="/2018/07/18/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（四）：支持向量机/17.jpg" alt></p>
<h2 id="支持向量机和LR的异同"><a href="#支持向量机和LR的异同" class="headerlink" title="支持向量机和LR的异同"></a>支持向量机和LR的异同</h2><h3 id="SVM与LR的相同点"><a href="#SVM与LR的相同点" class="headerlink" title="SVM与LR的相同点"></a>SVM与LR的相同点</h3><ol>
<li>LR和SVM都是分类算法，都是监督学习算法。</li>
<li>如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？</li>
<li>LR和SVM都是判别模型。判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别。</li>
<li>LR和SVM在学术界和工业界都广为人知并且应用广泛。</li>
</ol>
<h3 id="SVM与LR的不同点"><a href="#SVM与LR的不同点" class="headerlink" title="SVM与LR的不同点"></a>SVM与LR的不同点</h3><p>1.损失函数</p>
<p>SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重,两者的根本目的都是一样的。即支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。</p>
<p>影响SVM决策面的样本点只有少数的支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果</p>
<p>2.核技巧</p>
<p>在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。</p>
<p>这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM转化为对偶问题后，只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的），这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。</p>
<p>3.正则项</p>
<p>根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。</p>
<p>4.异常值</p>
<p>两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。</p>
<p>5.normalization</p>
<p>两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？</p>
<p>因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。</p>
<h1 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h1><ol>
<li>支持向量机背后的基本思想是什么</li>
<li>什么是支持向量</li>
<li>当使用 SVM 时，为什么标准化输入很重要？</li>
<li>分类一个样本时，SVM 分类器能够输出一个置信值吗？概率呢？</li>
<li>在一个有数百万训练样本和数百特征的训练集上，你是否应该使用 SVM 原始形式或对偶形式来训练一个模型？</li>
<li>假设你用 RBF 核来训练一个 SVM 分类器，如果对训练集欠拟合：你应该增大或者减小<code>γ</code>吗？调整参数<code>C</code>呢？</li>
</ol>
<hr>
<p>1、支持向量机背后的基本目标是在训练实例中分隔两个类的决策边界之间具有最大可能的间隔。 当执行软间隔分类时，SVM在完全分离两个类和具有尽可能宽的街道之间搜索折衷。 另一个关键思想是在训练非线性数据集时使用核技巧。</p>
<p>2、在训练SVM之后，支持向量是位于“街道”上的任何实例，包括其边界。 决策边界完全由支持向量决定。 任何不是支持向量的实例（即街道外）都没有任何影响; 你可以删除它们，添加更多实例或移动它们，只要它们离开街道它们就不会影响决策边界。 计算预测仅涉及支持向量，而不是整个训练集。</p>
<p>3、SVM尝试适应类之间最大可能的“街道”，因此如果训练集未缩放，SVM将倾向于忽略数值小的特征。</p>
<p>4、SVM分类器可以输出测试实例与决策边界之间的距离，您可以将其用作置信度分数。 但是，这个分数不能直接转换为类概率的估计。 如果在Scikit-Learn中创建SVM时设置probability = True，则在训练之后，它将使用SVM分数的Logistic回归校准概率（通过对训练数据进行额外的5折交叉验证进行训练）。 这会将predict_proba（）和predict_log_proba（）方法添加到SVM。</p>
<p>5、此问题仅适用于线性SVM，因为进行核化kernelized需要用到对偶形式(对偶问题就是为了可以进行核化)。 SVM问题的原始形式的计算复杂度与训练实例m的数量成比例，而对偶形式的计算复杂度与m2和m3之间的数量成比例。 所以如果有数百万个实例，你肯定应该使用原始形式，因为对偶形式会太慢。</p>
<p>6、如果使用RBF内核训练的SVM分类器不适合训练集，则可能存在正则化强度过高。 要减少它，您需要增加gamma或C（或两者）。</p>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/" itemprop="url">Sklearn 与 TensorFlow 机器学习实用指南（三）：回归</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-14T10:36:11+08:00">
                2018-07-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sklearn-与-TensorFlow-机器学习实用指南/" itemprop="url" rel="index">
                    <span itemprop="name">Sklearn 与 TensorFlow 机器学习实用指南</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  10,121
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  38
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="线性回归："><a href="#线性回归：" class="headerlink" title="线性回归："></a>线性回归：</h1><h2 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h2><script type="math/tex; mode=display">
\hat{y} = h _{\theta} (\mathbf{x})= \theta^T  \cdot \mathbf{x} = \theta _{0} + \theta _{1}x _{1}+\theta _{2}x _{2}+\dots+\theta _{n}x _{n}</script><ul>
<li><strong>x</strong> 为每个样例中特征值的向量形式，包括 $x_1$ 到 $x_n$  ，而且$x_0$ 恒为1。<strong>矩阵点乘，相同规模对应相乘</strong>。</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><script type="math/tex; mode=display">
MSE (\mathbf{X},h _{\theta}) = \frac{1}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}^2</script><h2 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h2><p>一行为一个实例</p>
<script type="math/tex; mode=display">
\left(\begin{matrix}
    1&        x_{1}^{\left(1\right)}&        x_{1}^{\left(2\right)}&        ···&        x_{1}^{\left(n\right)}\\
    1&        x_{2}^{\left(1\right)}&        x_{2}^{\left(2\right)}&        ···&        x_{2}^{\left(n\right)}\\
    ···&        ···&        ···&        &        \\
    1&        x_{m}^{\left(1\right)}&        x_{m}^{\left(2\right)}&        ···&        x_{m}^{\left(n\right)}\\
\end{matrix}\right)</script><p>优化线性回归损失函数的两种方法：1）最小二乘法原理的正态方程 2）梯度下降</p>
<h2 id="正态方程"><a href="#正态方程" class="headerlink" title="正态方程"></a>正态方程</h2><script type="math/tex; mode=display">
\frac{\partial }{\partial \theta}MSE(\theta) = \frac{1}{m}(2X^TX\theta-2X^Ty)</script><script type="math/tex; mode=display">
\hat{\theta} = ({\mathbf{X}}^T\cdot\mathbf{X})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}</script><p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/01.png" alt></p>
<blockquote>
<p>这里的$X^TX$要满足满秩矩阵，然而，现实大多数任务不会满足这个条件。好像只有线性回归能用正态方程求解。优点是一次计算；缺点是矩阵的逆计算慢，尤其是特征数量很多的情况下就更糟糕了，但是一旦你得到了线性回归模型（通过解正态方程或者其他的算法），进行预测是非常快的。</p>
</blockquote>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p><strong>批量梯度下降：使用梯度下降的过程中，你需要计算每一个</strong>θj 下代价函数的梯度<br>代价函数的偏导数: 利用公式2对θj求导，其余 θ看做常数。</p>
<script type="math/tex; mode=display">
\frac{\partial }{\partial \theta_j}MSE(\theta)=\frac{2}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}{x_j}^{(i)}</script><p>更新：</p>
<script type="math/tex; mode=display">
\theta :=\theta_j-\lambda\frac{\partial }{\partial \theta_j}MSE(\theta)</script><p>为了避免单独计算每一个梯度，你也可以使用下面的公式来一起计算它们。梯度向量记为$\nabla_{\theta}MSE(\theta) $ ，其包含了代价函数所有的偏导数(每个模型参数只出现一次)。利用正态方程最后的推导即可）</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/02.jpg" alt></p>
<blockquote>
<p><strong>在这个方程中每一步计算时都包含了整个训练集X ，这也是为什么这个算法称为批量梯度下降：</strong>每一次训练过程都使用所有的的训练数据。因此，在大数据集上，其会变得相当的慢（但是我们接下来将会介绍更快的梯度下降算法）。然而，梯度下降的运算规模和特征的数量成正比。训练一个数千数量特征的线性回归模型使用梯度下降要比使用正态方程快的多.</p>
</blockquote>
<p>更新：</p>
<script type="math/tex; mode=display">
\theta^{(next\ step)}=\theta - \eta\nabla_{\theta}MSE(\theta)</script><p>我们来看一下这个算法的应用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">eta = <span class="number">0.03</span></span><br><span class="line">n_iterations = <span class="number">15000</span></span><br><span class="line">m = <span class="number">100</span>    <span class="comment"># 样本数目，</span></span><br><span class="line"></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)    <span class="comment"># 产生100行1列的0~2的数值</span></span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># np.c_表示按列操作拼接，np.r_表示按行操作拼接</span></span><br><span class="line">X_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), X]     <span class="comment"># x0 = 1</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    gradients = <span class="number">2</span>/m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">    theta = theta - eta * gradients    <span class="comment"># 最后输出的是所有系数矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>theta</span><br><span class="line">array([[<span class="number">4.11509616</span>],[<span class="number">2.87011339</span>]])	<span class="comment"># 理论θ_0=4, θ_3, 由于噪声会有点误差</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>梯度下降的一些要点：</p>
<ul>
<li>应该确保所有的特征有着相近的尺度范围（例如：使用Scikit_Learn的 StandardScaler类）</li>
<li>学习率$\lambda $ 要自适应</li>
</ul>
</blockquote>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。<strong>与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样例</strong>。</p>
<p><strong>虽然随机性可以很好的跳过局部最优值，但同时它却不能达到最小值。解决这个难题的一个办法是逐渐降低学习率。</strong>开始时，走的每一步较大（这有助于快速前进同时跳过局部最小值），然后变得越来越小，从而使算法到达全局最小值。 <strong>这个过程被称为模拟退火</strong></p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/03.jpg" alt></p>
<p>下面的代码使用一个简单的learning schedule来实现<strong>随机梯度下降</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">50</span> </span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">50</span>  <span class="comment">#learning_schedule的超参数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_schedule</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># np.random.randn返回2行1列符合标准正态分布的数；</span></span><br><span class="line"><span class="comment"># np.random.rand返回[0,1）的随机数；</span></span><br><span class="line"><span class="comment"># randint返回范围内的整数</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        random_index = np.random.randint(m)    <span class="comment"># m个样本随机选一个样本</span></span><br><span class="line">        xi = X_b[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">        yi = y[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">        gradients = <span class="number">2</span> * xi.T.dot(xi.dot(theta)-yi)	<span class="comment"># 单个个体视为批量</span></span><br><span class="line">        eta = learning_schedule(epoch * m + i)    <span class="comment"># 根据迭代情况调整学习速率</span></span><br><span class="line">        theta = theta - eta * gradiens</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>theta</span><br><span class="line">array([[<span class="number">3.96100095</span>],[<span class="number">3.0580351</span> ]])</span><br></pre></td></tr></table></figure>
<p>通过使用Scikit-Learn完成<strong>线性回归的随机梯度下降，你需要使用SGDRegressor类，这个类默认优化的是均方差代价函数</strong>。下面的代码迭代了50代，其学习率eta为0.1，<strong>使用默认的learning schedule(与前面的不一样)</strong>，同时也没有添加任何正则项（penalty = None）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为这个函数需要的y是一个行向量，所以压扁;</span></span><br><span class="line"><span class="comment"># 另外，numpy.flatten() 与 numpy.ravel()将多维数组降位一维，前者会进行拷贝处理</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRregressor(n_iter=<span class="number">50</span>, penalty=<span class="literal">None</span>, eta=<span class="number">0.1</span>)</span><br><span class="line">sgd_reg.fit(X,y.ravel())</span><br></pre></td></tr></table></figure>
<p>结果很接近正态方程的解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_reg.intercept_, sgd_reg.coef_</span><br><span class="line">(array([<span class="number">4.18380366</span>]),array([<span class="number">2.74205299</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p><strong>小批量梯度下降中，它则使用一个随机的小型实例集，小批量梯度下降在参数空间上的表现比随机梯度下降要好的多，尤其在有大量的小型实例集时，主要利用了矩阵运算的硬件优化</strong></p>
<p>也看一下这个算法的应用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">theta_path_mgd = []</span><br><span class="line"></span><br><span class="line">n_iterations = <span class="number">50</span></span><br><span class="line">minibatch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line">t0, t1 = <span class="number">200</span>, <span class="number">1000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_schedule</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line"></span><br><span class="line">t = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    shuffled_indices = np.random.permutation(m)</span><br><span class="line">    X_b_shuffled = X_b[shuffled_indices]	<span class="comment"># 打乱所有样本顺序</span></span><br><span class="line">    y_shuffled = y[shuffled_indices]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, m, minibatch_size):</span><br><span class="line">        t += <span class="number">1</span></span><br><span class="line">        xi = X_b_shuffled[i:i+minibatch_size]</span><br><span class="line">        yi = y_shuffled[i:i+minibatch_size]</span><br><span class="line">        gradients = <span class="number">2</span>/minibatch_size * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        eta = learning_schedule(t)</span><br><span class="line">        theta = theta - eta * gradients</span><br><span class="line">        theta_path_mgd.append(theta)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/04.jpg" alt></p>
<h1 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h1><p>如果你的数据实际上比简单的直线更复杂呢？ 令人惊讶的是，<strong>你依然可以使用线性模型来拟合非线性数据</strong>。 <strong>一个简单的方法是对每个特征进行加权后作为新的特征，然后训练一个线性模型在这个扩展的特征集。 这种方法称为多项式回归。</strong></p>
<p>于是，<strong>我们使用Scikit-Learning的PolynomialFeatures类进行训练数据集的转换，让训练集中每个特征的平方（2次多项式）作为新特征（在这种情况下，仅存在一个特征）</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poly_features = PolynomialFeatures(degree=<span class="number">2</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_poly = poly_features.fit_transform(X)    <span class="comment"># 转换特征，包含原始特征和二次项特征</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X[<span class="number">0</span>]</span><br><span class="line">array([<span class="number">-0.75275929</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_poly[<span class="number">0</span>]</span><br><span class="line">array([<span class="number">-0.75275929</span>, <span class="number">0.56664654</span>])</span><br></pre></td></tr></table></figure>
<p>现在包含原始特X并加上了这个特征的平方X^2。<strong>现在你可以在这个扩展训练集上使用LinearRegression模型进行拟合</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_reg = LinearRegression()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_reg.fit(X_poly, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_reg.intercept_, lin_reg.coef_</span><br><span class="line">(array([ <span class="number">1.78134581</span>]), array([[ <span class="number">0.93366893</span>, <span class="number">0.56456263</span>]]))</span><br><span class="line"><span class="comment"># 模型预测函数y=0.56*x_1^2+0.93*x_1+1.78</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>请注意，当存在多个特征时，多项式回归能够找出特征之间的关系（这是普通线性回归模型无法做到的）。 这是因为LinearRegression会自动添加当前阶数下特征的所有组合。例如，如果有两个特征a,b，使用3阶（degree=3）的LinearRegression时，不仅仅只有a2,a3,b2,同时也会有它们的其他组合项ab,a2b,ab2。</p>
</blockquote>
<h1 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h1><p>我们可以使用交叉验证来估计一个模型的泛化能力。<strong>如果一个模型在训练集上表现良好，通过交叉验证指标却得出其泛化能力很差，那么你的模型就是过拟合了。如果在这两方面都表现不好，那么它就是欠拟合了</strong>。这种方法可以告诉我们，你的模型是太复杂了还是太简单了。</p>
<p><strong>另一种方法是观察学习曲线：画出模型在训练集上的表现，同时画出以训练集规模为自变量的训练集函数。为了得到图像，需要在训练集的不同规模子集上进行多次训练</strong>。下面的代码定义了一个函数，用来画出给定训练集后的模型学习曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curves</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">    train_errors, val_errors = [], []</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">1</span>, len(X_train)):    <span class="comment"># 根据样本规模画出模型的表现</span></span><br><span class="line">        model.fit(X_train[:m], y_train[:m])</span><br><span class="line">        y_train_predict = model.predict(X_train[:m])</span><br><span class="line">        y_val_predict = model.predict(X_val)</span><br><span class="line">        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val_predict, y_val))</span><br><span class="line">plt.plot(np.sqrt(train_errors), <span class="string">"r-+"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"train"</span>)    <span class="comment"># 训练损失</span></span><br><span class="line">plt.plot(np.sqrt(val_errors), <span class="string">"b-"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"val"</span>)    <span class="comment"># 验证损失</span></span><br></pre></td></tr></table></figure>
<p>我们一起看一下简单线性回归模型的学习曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lin_reg = LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/05.jpg" alt></p>
<p><strong>上面的曲线表现了一个典型的欠拟合模型，两条曲线都到达高原地带并趋于稳定，并且最后两条曲线非常接近，同时误差值非常大。</strong></p>
<blockquote>
<p>如果你的模型在训练集上是欠拟合的，添加更多的样例是没用的。你需要使用一个更复杂的模型或者找到更好的特征。</p>
</blockquote>
<p>现在让我们看一个在相同数据上10阶多项式模型拟合的学习曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">polynomial_regression = Pipeline((</span><br><span class="line">    (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">    (<span class="string">"sgd_reg"</span>, LinearRegression()),</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/06.jpg" alt></p>
<ul>
<li>在训练集上，误差要比线性回归模型低的多。</li>
<li>图中的两条曲线之间有间隔，这意味模型在训练集上的表现要比验证集上好的多，这也是模型过拟合的显著特点。当然，如果你使用了更大的训练数据，这两条曲线最后会非常的接近。</li>
</ul>
<blockquote>
<p>改善模型过拟合的一种方法是提供更多的训练数据，直到训练误差和验证误差相等</p>
</blockquote>
<p>在统计和机器学习领域有个重要的理论：一个模型的泛化误差由三个不同误差的和决定：</p>
<ul>
<li>偏差：泛化误差的这部分误差是由于错误的假设决定的。例如实际是一个二次模型，你却假设了一个线性模型。一个高偏差的模型最容易出现欠拟合。</li>
<li>方差：这部分误差是由于模型对训练数据的微小变化较为敏感，一个多自由度的模型更容易有高的方差（例如一个高阶多项式模型），因此会导致模型过拟合。</li>
<li>不可约误差：这部分误差是由于数据本身的噪声决定的。降低这部分误差的唯一方法就是进行数据清洗（例如：修复数据源，修复坏的传感器，识别和剔除异常值）。</li>
</ul>
<p>下图依次为欠拟合，过拟合，较合适。</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/07.jpg" alt></p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><p>器学习中有几个常用的范数，分别是：</p>
<ul>
<li>$L<em>1$−范数：$\Vert x\Vert_1 =\sum</em>{i=1}^n\vert x_i\vert$ </li>
<li>$L<em>2$−范数：$\Vert x\Vert</em> 2=(\sum_{i=1}^n\vert x_i^2\vert)^{\frac{1}{2}}$ </li>
<li>$L<em>p$−范数：$\Vert x\Vert_p =(\sum</em>{i=1}^n\vert x_i^p\vert)^{\frac{1}{p}}$ </li>
<li>$L<em>∞$−范数：$\Vert x\Vert</em>∞=lim<em>{p→∞}(\sum</em>{i=1}^n\vert x_i^p\vert)^{\frac{1}{p}}$</li>
</ul>
<h2 id="岭回归-Ridge"><a href="#岭回归-Ridge" class="headerlink" title="岭回归(Ridge)"></a>岭回归(Ridge)</h2><p>岭回归（也称为Tikhonov正则化）是线性回归的正则化版，是L2正则的基础，注意到这个正则项只有在训练过程中才会被加到代价函数。当得到完成训练的模型后，我们应该使用没有正则化的测量方法去评价模型的表现。</p>
<blockquote>
<p><strong>一般情况下，训练过程使用的代价函数和测试过程使用的评价函数不一样样的。除了正则化，还有一个不同：训练时的代价函数应该在优化过程中易于求导，而在测试过程中，评价函数更应该接近最后的客观表现</strong>。一个好的例子：在分类训练中我们使用对数损失（马上我们会讨论它）作为代价函数，但是我们却使用精确率/召回率来作为它的评价函数。</p>
</blockquote>
<p>岭回归代价函数:</p>
<script type="math/tex; mode=display">
J(\theta)=MSE(\theta)+\alpha\frac{1}{2}\sum\limits_{i=1}^n\theta_i^2</script><p><strong>超参数α 决定了你想正则化这个模型的强度,正则化强度越大，模型会越简单。如果α=0 那此时的岭回归便变为了线性回归。如果α 非常的大，所有的权重最后都接近与零，最后结果将是一条穿过数据平均值的水平直线</strong></p>
<p>值得注意的是偏差 $\theta_0$是没有被正则化的（累加运算的开始是 i=1而不是i=0）。如我定义<strong>$w$</strong>作为特征的权重向量($\theta_1$到$\theta_n$)，那么正则项可以简写成$\frac{1}{2} (\Vert w\Vert_2)^2$, 其中$\Vert \cdot \Vert_2$ 表示权重向量的L2范数。对于梯度下降来说仅仅在均方差梯度向量加上一项$\alpha w$ ,加上$\alpha\theta$是$1/2∗\alpha∗\theta^2$求偏导的结果</p>
<blockquote>
<p>在使用岭回归前，对<strong>数据进行放缩（可以使用StandardScaler）是非常重要的</strong>,算法对于输入特征的数值尺度（scale）非常敏感。<strong>大多数的正则化模型都是这样的</strong>。</p>
</blockquote>
<p>对线性回归来说，对于岭回归，我们可以使用封闭方程去计算，也可以使用梯度下降去处理.</p>
<p>岭回归的封闭方程的解</p>
<p>令</p>
<script type="math/tex; mode=display">
MES(\theta)=(X\theta-y)^T(X\theta-y)+\lambda \theta^T\theta</script><script type="math/tex; mode=display">
\frac{\partial }{\partial \theta}MSE(\theta) =X^TX\theta-X^Ty +\alpha\theta=0</script><p>求出</p>
<script type="math/tex; mode=display">
\hat{\theta} =({\mathbf{X}}^T\cdot\mathbf{X}+\alpha\mathbf{I})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}</script><blockquote>
<p> 矩阵$I$是是一个除了左上角有一个0的n×n的<strong>单位矩阵</strong>，这个0代表偏差项。偏差$\theta_0$不被正则化的。</p>
</blockquote>
<p>下面是如何使用 Scikit-Learn 来进行封闭方程的求解（使用 Cholesky 法进行矩阵分解对上面公式进行变形）:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ridge_reg = Ridge(alpha=<span class="number">1</span>, solver=<span class="string">"cholesky"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ridge_reg.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ridge_reg.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([[ <span class="number">1.55071465</span>]]</span><br></pre></td></tr></table></figure>
<p>使用随机梯度法进行求解：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_reg = SGDRegressor(penalty=<span class="string">"l2"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_reg.fit(X, y.ravel())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_reg.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([[ <span class="number">1.13500145</span>]])</span><br></pre></td></tr></table></figure>
<p><code>penalty</code>参数指的是正则项的惩罚类型。指定“l2”表明你要在损失函数上添加一项：权重向量 L2范数平方的一半，这就是简单的岭回归。</p>
<h2 id="Lasso-回归"><a href="#Lasso-回归" class="headerlink" title="Lasso 回归"></a>Lasso 回归</h2><p>Lasso 回归（也称 Least Absolute Shrinkage，或者 Selection Operator Regression）是另一种正则化版的线性回归：L1正则的基础，就像岭回归那样，它也在损失函数上添加了一个正则化项，但是它使用权重向量的L1范数而不是权重向量L2范数的一半。</p>
<p>Lasso回归的代价函数:</p>
<script type="math/tex; mode=display">
J(\theta)=MSE(\theta)+\alpha\sum\limits_{i=1}^n\left|\theta_i \right|</script><blockquote>
<p><strong>Lasso回归的一个重要特征是它倾向于完全消除最不重要的特征的权重（即将它们设置为零）</strong></p>
</blockquote>
<p>下面是一个使用Lasso类的小Scikit-Learn示例。你也可以使用SGDRegressor(penalty=”l1”)来代替它</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lasso_reg = Lasso(alpha=<span class="number">0.1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lasso_reg.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lasso_reg.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([ <span class="number">1.53788174</span>]</span><br></pre></td></tr></table></figure>
<h2 id="弹性网络-ElasticNet"><a href="#弹性网络-ElasticNet" class="headerlink" title="弹性网络(ElasticNet)"></a>弹性网络(ElasticNet)</h2><p><strong>弹性网络介于Ridge回归和Lasso回归之间。它的正则项是Ridge回归和Lasso回归正则项的简单混合，同时你可以控制它们的混合率r，当r=0时，弹性网络就是Ridge回归，当r=1时，其就是Lasso回归</strong></p>
<p>弹性网络代价函数：</p>
<script type="math/tex; mode=display">
J(\theta)=MSE(\theta)+r\alpha\sum\limits_{i=1}^n\left|\theta_i \right|+\frac{1-r}{2}\alpha\sum\limits_{i=1}^n\theta_i^2</script><p>那么我们该如何选择线性回归，岭回归，Lasso回归，弹性网络呢？一般来说有一点正则项的表现更好，因此通常你应该避免使用简单的线性回归。<strong>岭回归是一个很好的首选项，但是如果你的特征仅有少数是真正有用的，你应该选择Lasso和弹性网络</strong>。就像我们讨论的那样，它两能够将无用特征的权重降为零。<strong>一般来说，弹性网络的表现要比Lasso好，因为当特征数量比样例的数量大的时候，或者特征之间有很强的相关性时，Lasso可能会表现的不规律</strong>。下面是一个使用Scikit-Learn 弹性网络ElasticNet（l1_ratio指的就是混合率r）的简单样例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>elastic_net = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>elastic_net.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>elastic_net.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([ <span class="number">1.54333232</span>])</span><br></pre></td></tr></table></figure>
<h2 id="正则化的作用"><a href="#正则化的作用" class="headerlink" title="正则化的作用"></a>正则化的作用</h2><p>那为什么正则化能起作用呢？首先L0范数（元素非零个数，严格上来说不能算是范数）和L1范数都可以实现权重稀疏。<strong>L1范数和L0范数可以实现稀疏，</strong>L1范数是L0范数的最优凸近似，<strong>L1因具有比L0更好的优化求解特性而被广泛应用</strong>。L1会趋向于产生少量的特征，而其他的特征都是0，<strong>而L2会选择更多的特征，这些特征都会接近于0</strong>。</p>
<p>L1范数的主要作用的实现稀疏特征，那么L2范数可以起什么样作用呢？</p>
<p>执行 L2 正则化对模型具有以下影响</p>
<ul>
<li>使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布。</li>
</ul>
<ul>
<li>使权重值接近于 0（但并非正好为 0）</li>
</ul>
<p><strong>L2 正则化可能会导致对于某些信息缺乏的特征，模型会学到适中的权重。L2 正则化降低较大权重的程度高于降低较小权重的程度。随着权重越来越接近于 0.0，L2 将权重“推”向 0.0 的力度越来越弱。L2 正则化会使相似度高(存在噪点)两个特征的权重几乎相同。按照我自己的理解，不同的权重会有不同程度的拟合效果，权重较小，低阶的w控制曲线的整体走势，权重较大，高阶的w控制曲线的局部形态，以此类推。这样看来L2正则项的作用就很明显了，要改变预测曲线的整体细节走势肯地会造成损失函数的不满，但是把曲线的形态熨平似乎并没有什么不妥，会降低过拟合的风险。</strong></p>
<p>L2除了能防止过拟合，提升模型的泛化能力。还有另外的一点好处：优化计算。  从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果一个系统是ill-conditioned的，它的输出结果就不要太相信了。</p>
<p>然而，如果当我们的样本X的数目比每个样本的维度还要小的时候，矩阵XTX将会不是满秩的，也就是XTX会变得不可逆，所以w*就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。</p>
<p>但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：</p>
<script type="math/tex; mode=display">
\hat{\theta} =({\mathbf{X}}^T\cdot\mathbf{X}+\alpha\mathbf{I})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}</script><p> 这里面，专业点的描述是：要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。考虑没有规则项的时候，也就是λ=0的情况，如果矩阵XTX的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善condition number。</p>
<h1 id="早期停止法（Early-Stopping）"><a href="#早期停止法（Early-Stopping）" class="headerlink" title="早期停止法（Early Stopping）"></a>早期停止法（Early Stopping）</h1><p>随着训练的进行，算法一直学习，它在训练集上的预测误差（RMSE）自然而然的下降。然而一段时间后，验证误差停止下降，并开始上升。这意味着模型在训练集上开始出现过拟合。一旦验证错误达到最小值，便提早停止训练.</p>
<p>随机梯度和小批量梯度下降不是平滑曲线，你可能很难知道它是否达到最小值。 一种解决方案是，只有<strong>在验证误差高于最小值一段时间后（你确信该模型不会变得更好了），才停止</strong>，<strong>之后将模型参数回滚到验证误差最小值</strong>。</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/08.png" alt></p>
<p>下面是一个早期停止法的基础应用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</span><br><span class="line">sgd_reg = SGDRegressor(n_iter=<span class="number">1</span>, warm_start=<span class="literal">True</span>, penalty=<span class="literal">None</span>,learning_rate=<span class="string">"constant"</span>, eta0=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line">minimum_val_error = float(<span class="string">"inf"</span>)</span><br><span class="line">best_epoch = <span class="literal">None</span></span><br><span class="line">best_model = <span class="literal">None</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sgd_reg.fit(X_train_poly_scaled, y_train)    <span class="comment"># 训练多项式的新特征，拟合非线性</span></span><br><span class="line">    y_val_predict = sgd_reg.predict(X_val_poly_scaled)</span><br><span class="line">    val_error = mean_squared_error(y_val_predict, y_val)</span><br><span class="line">    <span class="keyword">if</span> val_error &lt; minimum_val_error:</span><br><span class="line">        minimum_val_error = val_error</span><br><span class="line">        best_epoch = epoch</span><br><span class="line">        best_model = clone(sgd_reg)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：当warm_start=True时，调用fit()方法后，训练会从停下来的地方继续，而不是从头重新开始</p>
</blockquote>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p><strong>逻辑回归</strong>会生成一个<strong>介于 0 到 1 之间（不包括 0 和 1）的概率值</strong>，而不是确切地预测结果是 0 还是 1。以用于检测垃圾邮件的逻辑回归模型为例。如果此模型推断某一特定电子邮件的值为 0.932，则意味着该电子邮件是垃圾邮件的概率为 93.2%。<strong>更准确地说，这意味着在无限训练样本的极限情况下，模型预测其值为 0.932 的这组样本实际上有 93.2% 是垃圾邮件，其余的 6.8% 不是垃圾邮件。</strong></p>
<p>逻辑回归模型的概率估计（向量形式）：</p>
<script type="math/tex; mode=display">
\hat{p}=h_\theta(\mathbf{x})=\sigma(\theta^T  \cdot \mathbf{x})</script><p>Logistic函数（也称为logit），用σ() 表示，其是一个sigmoid函数（图像呈S型），它的输出是一个介于0和1之间的数字<br>逻辑函数(S函数)</p>
<script type="math/tex; mode=display">
\sigma(t)=\frac{1}{1+exp(-t)}</script><p>Logistic函数（也称为logit），用σ() 表示，其是一个sigmoid函数（图像呈S型），它的输出是一个介于0和1之间的数字<br>逻辑函数(S函数)</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/09.jpg" alt></p>
<p>逻辑回归预测模型(σ() 概率输出以0.5作为二分类门槛):</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/10.jpg" alt></p>
<p>单个样例的代价函数:</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/11.jpg" alt></p>
<p><strong>这个代价函数是合理的，因为当t接近0时，-log(t)变得非常大，所以如果模型估计一个正例概率接近于0，那么代价函数将会很大，同时如果模型估计一个负例的概率接近1，那么代价函数同样会很大。 另一方面，当t接近于1时， -log(t)接近0，所以如果模型估计一个正例概率接近于0，那么代价函数接近于0，同时如果模型估计一个负例的概率接近0，那么代价函数同样会接近于0， 这正是我们想的.（简单来说,y=1时，概率p越接近1损失越小；相反y=0时，概率p越接近0时损失越小）</strong></p>
<p>整个训练集的代价函数只是所有训练实例的平均值。可以用一个表达式（你可以很容易证明）来统一表示，称为对数损失</p>
<p><strong>逻辑回归的代价函数（对数损失）：</strong></p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}\sum\limits_{i=1}^m\left[y^{(i)}log\left(\hat{p}^{(i)}\right)+\left(1-y^{(i)}\right)log\left(1-\hat{p}^{(i)}\right)\right]</script><p>但是这个代价函数对于求解最小化代价函数的θ 是没有公式解的（<strong>没有等价的正态方程</strong>）。 <strong>但好消息是，这个代价函数是凸的，所以梯度下降</strong>（或任何其他优化算法）一定能够找到全局最小值（如果学习速率不是太大，并且你等待足够长的时间）。下面公式给出了代价函数关于第j个模型参数θj 的偏导数。</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/20.png" alt></p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/22.jpg" alt></p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/21.jpg" alt></p>
<p>逻辑回归代价函数的偏导数:</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_j}J(\theta_j)=\frac{1}{m} \sum\limits_{i=1}^m{\left(\sigma\left(\theta^T \cdot \mathbf{x}^{(i)}\right)-y^{(i)}\right)}{x_j}^{(i)}</script><p>这个公式首先计算每个样例的预测误差，然后误差项乘以第j项特征值，最后求出所有训练样例的平均值。 一旦你有了包含所有的偏导数的梯度向量，你便可以在梯度向量上使用批量梯度下降算法。 也就是说：你已经知道如何训练Logistic回归模型。 对于随机梯度下降，你当然只需要每一次使用一个实例，对于小批量梯度下降，你将每一次使用一个小型实例集。</p>
<h1 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h1><p>我们使用鸢尾花数据集来分析Logistic回归。 这是一个著名的数据集，其中包含150朵三种不同的鸢尾花的萼片和花瓣的长度和宽度。这三种鸢尾花为：Setosa，Versicolor，Virginica</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/12.png" alt></p>
<p>让我们尝试建立一个分类器，仅仅<strong>使用花瓣的宽度特征来**</strong>识别Virginica**，首先让我们加载数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>iris = datasets.load_iris()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>list(iris.keys())</span><br><span class="line">[<span class="string">'data'</span>, <span class="string">'target_names'</span>, <span class="string">'feature_names'</span>, <span class="string">'target'</span>, <span class="string">'DESCR'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = iris[<span class="string">"data"</span>][:, <span class="number">3</span>:] <span class="comment"># petal width</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = (iris[<span class="string">"target"</span>] == <span class="number">2</span>).astype(np.int)</span><br></pre></td></tr></table></figure>
<p>接下来，我们训练一个逻辑回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X, y) <span class="comment"># 训练模型</span></span><br></pre></td></tr></table></figure>
<p>我们来看看模型估计的花瓣宽度从0到3厘米的概率估计</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_new = np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1000</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)    <span class="comment"># 构造花瓣宽度从0到3厘米的所有特征</span></span><br><span class="line">y_proba = log_reg.predict_proba(X_new)    <span class="comment"># 预测概率</span></span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">1</span>], <span class="string">"g-"</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">0</span>], <span class="string">"b--"</span>, label=<span class="string">"Not Iris-Virginica"</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/13.png" alt></p>
<p>Virginica花的花瓣宽度（用三角形表示）在1.4厘米到2.5厘米之间，而其他种类的花（由正方形表示）通常具有较小的花瓣宽度，范围从0.1厘米到1.8厘米。注意，它们之间会有一些重叠。在大约2厘米以上时，分类器非常肯定这朵花是Virginica花（分类器此时输出一个非常高的概率值），而在1厘米以下时，它非常肯定这朵花不是Virginica花（不是Virginica花有非常高的概率）。在这两个极端之间，分类器是不确定的。但是，如果你使用它进行预测（使用predict()方法而不是predict_proba()方法），它将返回一个最可能的结果。<strong>因此，在1.6厘米左右存在一个决策边界，这时两类情况出现的概率都等于50％</strong>：如果花瓣宽度大于1.6厘米，则分类器将预测该花是Virginica，否则预测它不是（即使它有可能错了）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>log_reg.predict([[<span class="number">1.7</span>], [<span class="number">1.5</span>]])</span><br><span class="line">array([<span class="number">1</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>下图的线性决策边界表示相同的数据集，<strong>但是这次使用了两个特征进行判断：花瓣的宽度和长度</strong>。 一旦训练完毕，Logistic回归分类器就可以根据这两个特征来估计一朵花是Virginica的可能性。 <strong>虚线表示这时两类情况出现的概率都等于50％：这是模型的决策边界。</strong> <strong>请注意，它是一个线性边界。每条平行线都代表一个分类标准下的两两个不同类的概率，从15％（左下角）到90％（右上角）。</strong>越过右上角分界线的点都有超过90％的概率是Virginica花</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/14.png" alt></p>
<p>就像其他线性模型，逻辑回归模型也可以ℓ1或者ℓ2 惩罚使用进行正则化。Scikit-Learn默认添加了ℓ2 惩罚</p>
<blockquote>
<p>在Scikit-Learn的LogisticRegression模型中控制正则化强度的超参数不是α （与其他线性模型一样），而是是它的逆：C. C的值越大，模型正则化强度越低</p>
</blockquote>
<h1 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h1><p>Logistic回归模型可以直接推广到支持多类别分类，不必组合和训练多个二分类器， 其称为Softmax回归或多类别Logistic回归.</p>
<p>这个想法很简单：<strong>当给定一个实例x 时，Softmax回归模型首先计算k类的分数sk(x) ，然后将分数应用在Softmax函数（也称为归一化指数）上，估计出每类的概率</strong>。 计算sk(x) 的公式看起来很熟悉，因为它就像线性回归预测的公式一样</p>
<blockquote>
<p>k类的Softmax得分: $s_k(x)=θ^T⋅x$</p>
</blockquote>
<p>注意，每个类都有自己独一无二的参数向量θk 。 所有这些向量通常作为行放在参数矩阵Θ 中</p>
<p><strong>一旦你计算了样例x 的每一类的得分，你便可以通过Softmax函数估计出样例属于第k类的概率p^k ：通过计算e的sk(x) 次方，然后对它们进行归一化（除以所有分子的总和）</strong>。</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/15.png" alt></p>
<p>和Logistic回归分类器一样，Softmax回归分类器将估计概率最高（它只是得分最高的类）的那类作为预测结果，如公式4-21所示</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/16.png" alt></p>
<blockquote>
<p>Softmax回归分类器一次只能预测一个类（即它是多类的，但不是多输出的），因此它只能用于判断互斥的类别，如不同类型的植物。 你不能用它来识别一张照片中的多个人。</p>
</blockquote>
<p>现在我们知道这个模型如何估计概率并进行预测，接下来将介绍如何训练。<strong>我们的目标是建立一个模型在目标类别上有着较高的概率（因此其他类别的概率较低），最小化公式4-22可以达到这个目标，其表示了当前模型的代价函数，称为交叉熵，当模型对目标类得出了一个较低的概率，其会惩罚这个模型。 交叉熵通常用于衡量待测类别与目标类别的匹配程度（我们将在后面的章节中多次使用它）</strong></p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/17.png" alt></p>
<h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>熵的本质是香农信息量$log\frac{1}{p}$的期望。<strong>信息熵</strong>代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小（猜题次数、编码长度等），就是用<strong>交叉熵</strong>来衡量的。</p>
<p>现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布。按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为$H(p)=\sum \limits<em>{i=1}^n p(i)\cdot log\frac{1}{p(i)}$ 。如果使用错误分布q来表示来自真实分布p的平均编码长度，则应该是$H(p,q)=\sum\limits</em>{i=1}^n p(i)\cdot log\frac{1}{q(i)}$ 。因为用q来编码的样本来自分布p，所以期望H(p,q)中概率是p(i)。<strong>H(p,q)我们称之为“交叉熵”</strong>。<strong>当q为真实分布p时，交叉熵达到最小值1，否则将会大于1</strong>。我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“<strong>相对熵</strong>”：$D(p\Vert q)=H(p,q)-H(p)=\sum\limits_{i=1}^n p(i)\cdot log\frac{p(i)}{q(i)}$  ,其又被称为KL散度(Kullback–Leibler divergence，KLD)。<strong>它表示两个概率分布的差异性</strong>：差异越大则相对熵越大，差异越小则相对熵越小，特别地，若2者相同则熵为0。</p>
<p>另外，<strong>通常“相对熵”也可称为“交叉熵”</strong>，因为真实分布p是固定的，D(p||q)由H(p,q)决定。<strong>所以他们得到的相对效果是一样程度的</strong>。当然也有特殊情况，彼时两者须区别对待。</p>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/18.png" alt></p>
<p><strong>上面这个公式由公式4-22求导得到，过程和逻辑回归损失函数一样，只不过将每个类别都纳入计算而已，当k=2则计算正负两类，与逻辑回归一模一样。现在你可以计算每一类的梯度向量，然后使用梯度下降（或者其他的优化算法）找到使得代价函数达到最小值的参数矩阵Θ</strong>。</p>
<p>让我们使用Softmax回归对三种鸢尾花进行分类。当你使用LogisticRregression对模型进行训练时，<strong>Scikit_Learn默认使用的是一对多模型，但是你可以设置multi_class参数为“multinomial”来把它改变为Softmax回归。你还必须指定一个支持Softmax回归的求解器，例如“lbfgs”求解器</strong>（有关更多详细信息，请参阅Scikit-Learn的文档）。<strong>其默认使用ℓ12 正则化，你可以使用超参数C控制它。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)] <span class="comment"># petal length, petal width</span></span><br><span class="line">y = iris[<span class="string">"target"</span>]</span><br><span class="line"></span><br><span class="line">softmax_reg = LogisticRegression(multi_class=<span class="string">"multinomial"</span>,solver=<span class="string">"lbfgs"</span>, C=<span class="number">10</span>)</span><br><span class="line">softmax_reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>所以下次你发现一个花瓣长为5厘米，宽为2厘米的鸢尾花时，你可以问你的模型你它是哪一类鸢尾花，它会回答94.2％是Virginica花（第二类），或者5.8％是其他鸢尾花</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>softmax_reg.predict([[<span class="number">5</span>, <span class="number">2</span>]])</span><br><span class="line">array([<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>softmax_reg.predict_proba([[<span class="number">5</span>, <span class="number">2</span>]])</span><br><span class="line">array([[ <span class="number">6.33134078e-07</span>, <span class="number">5.75276067e-02</span>, <span class="number">9.42471760e-01</span>]])是</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/14/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（三）：回归/19.png" alt></p>
<p>图4-25用不同背景色表示了结果的决策边界。注意，任何两个类之间的决策边界是线性的。 该图的曲线表示Versicolor类的概率（例如，用0.450标记的曲线表示45％的概率边界）。注意模型也可以预测一个概率低于50％的类。 例如，在所有决策边界相遇的地方，所有类的估计概率相等，分别为33％。</p>
<h1 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h1><ol>
<li>如果你有一个数百万特征的训练集，你应该选择哪种线性回归训练算法？</li>
<li>假设你训练集中特征的数值尺度（scale）有着非常大的差异，哪种算法会受到影响？有多大的影响？对于这些影响你可以做什么？</li>
<li>训练 Logistic 回归模型时，梯度下降是否会陷入局部最低点？</li>
<li>在有足够的训练时间下，是否所有的梯度下降都会得到相同的模型参数？</li>
<li>假设你使用批量梯度下降法，画出每一代的验证误差。当你发现验证误差一直增大，接下来会发生什么？你怎么解决这个问题？</li>
<li>当验证误差升高时，立即停止小批量梯度下降是否是一个好主意？</li>
<li>哪个梯度下降算法（在我们讨论的那些算法中）可以最快到达解的附近？哪个的确实会收敛？怎么使其他算法也收敛？</li>
<li>假设你使用多项式回归，画出学习曲线，在图上发现学习误差和验证误差之间有着很大的间隙。这表示发生了什么？有哪三种方法可以解决这个问题？</li>
<li>假设你使用岭回归，并发现训练误差和验证误差都很高，并且几乎相等。你的模型表现是高偏差还是高方差？这时你应该增大正则化参数$\alpha$ 还是降低它？</li>
<li>你为什么要这样做：</li>
</ol>
<ul>
<li>使用岭回归代替线性回归？</li>
<li>Lasso 回归代替岭回归？</li>
<li>弹性网络代替 Lasso 回归？</li>
</ul>
<ol>
<li>假设你想判断一副图片是室内还是室外，白天还是晚上。你应该选择二个逻辑回归分类器，还是一个 Softmax 分类器？</li>
</ol>
<hr>
<p>1、如果您拥有具有数百万个功能的训练集，则可以使用随机梯度下降或小批量梯度下降，如果计算内存足够的话，则可使用批量梯度下降。 但是你不能使用正态方程，因为计算复杂度随着特征数量的增长而快速增长（超过二次方），求矩阵特征的逆非常花时间。</p>
<p>2、如果训练集中的特征具有非常不同的比例，则损失函数将具有细长碗的形状，因此梯度下降优化将花费很长时间来收敛。 要解决此问题，您应该在训练模型之前缩放数据。 另外，正态方程在没有缩放的情况下可以正常工作。</p>
<p>3、在训练Logistic回归模型时，梯度下降不会陷入在局部最小值，因为它的损失函数是凸函数的。</p>
<p>4、如果优化问题是凸函数的（例如线性回归或逻辑回归），并且假设学习速率不是太高，则所有梯度下降算法将接近全局最优并最终产生相当类似的模型。 但是，除非你逐渐降低学习率，否则随机梯度下降和小批量GD将永远不会真正收敛; 相反，他们将继续围绕全局最佳状态来回跳跃。 这意味着即使你让它们运行很长时间，这些Gradient Descent算法也会产生略微不同的模型。</p>
<p>5、如果验证误差在每个时期之后一直上升，则一种可能性是学习速率太高并且算法发散。如果训练误差也会增加，那么这显然是问题，你应该降低学习率。 但是，如果训练错误没有增加，那么您的模型将过度拟合训练集，您应该停止训练。</p>
<p>6、由于随机性，随机梯度下降和小批量梯度下降都不能保证在每次训练迭代中都取得进展。 因此，如果在验证损失增加时立即停止训练，你可能会在达到最佳值之前过早停止。 更好的选择是定期保存模型，当它长时间没有改进时（意味着它可能永远不会超过记录），你可以恢复到最佳保存模型。</p>
<p>7、随机梯度下降具有最快的训练迭代，因为它一次只考虑一个训练实例，因此它通常是第一个到达全局最优值（或具有非常小的小批量大小的Minibatch GD）附近。 但是，如果有足够的训练时间，只有批量梯度下降实际上会收敛。 如上所述，除非你逐渐降低学习速度，否则随机指标GD和小批量GD将在最佳状态下反弹。</p>
<p>8、如果验证误差远远高于训练误差，则可能是因为你的模型过度拟合了训练集。 尝试解决此问题的一种方法是降低多项式度：具有较少自由度的模型不太可能过度拟合。 你可以尝试的另一件事是加入正则项，例如，通过在成本函数中添加ℓ2惩罚（岭）或ℓ1惩罚（Lasso）。 这也会降低模型的自由度。 最后，你还可以尝试增加训练集的大小。</p>
<p>9、如果训练误差和验证误差几乎相等且相当高，则模型可能欠拟合训练集，这意味着它具有高偏差。 你应该尝试减少正则化超参数α。</p>
<p>10、</p>
<ul>
<li>具有一些正则化的模型通常比没有任何正则化的模型表现更好，因此通常应该优先选择岭回归而不是简单的线性回归。</li>
<li>Lasso回归使用ℓ1惩罚，这往往会将权重降低到恰好为零。 这导致稀疏模型，除了最重要的权重之外，所有权重都为零。 这是一种自动执行特征选择的方法，如果你怀疑只有少数特征真正重要，这是很好的。 当你不确定时，你应该更偏向岭回归。</li>
<li>弹性网络常比Lasso更受欢迎，因为Lasso在某些情况下可能表现不稳定（当有些特征强烈相关或者特征数量比训练样本数量还要多）。 但是，它确实添加了一个额外的超参数来调整。 如果你想要具有稳定行为的Lasso，你可以使用弹性网络，并设置比率r接近1。</li>
</ul>
<p>11、如果你想将图片分类为室外/室内和白天/夜晚，因为这些不是专属类别（即，所有四种组合都是可能的），你应该训练两个Logistic回归分类器。</p>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/" itemprop="url">Sklearn 与 TensorFlow 机器学习实用指南（二）：分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-11T10:57:46+08:00">
                2018-07-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sklearn-与-TensorFlow-机器学习实用指南/" itemprop="url" rel="index">
                    <span itemprop="name">Sklearn 与 TensorFlow 机器学习实用指南</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  7,758
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  31
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="MNIST：手写数字分类数据集"><a href="#MNIST：手写数字分类数据集" class="headerlink" title="MNIST：手写数字分类数据集"></a>MNIST：手写数字分类数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_mldata</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mnist = fetch_mldata(<span class="string">'MNIST original'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mnist</span><br><span class="line">&#123;<span class="string">'COL_NAMES'</span>: [<span class="string">'label'</span>, <span class="string">'data'</span>],</span><br><span class="line"><span class="string">'DESCR'</span>: <span class="string">'mldata.org dataset: mnist-original'</span>,   <span class="comment"># DESCR键描述数据集</span></span><br><span class="line"><span class="string">'data'</span>: array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],    <span class="comment"># 数组的一行表示一个样例，一列表示一个特征</span></span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                ...,</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ..., <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], dtype=uint8),</span><br><span class="line"><span class="string">'target'</span>: array([ <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, ..., <span class="number">9.</span>, <span class="number">9.</span>, <span class="number">9.</span>])&#125;    <span class="comment"># target键存放一个标签数组</span></span><br><span class="line">X, y = mnist[<span class="string">"data"</span>], mnist[<span class="string">"target"</span>] <span class="comment"># 获取样本或标签</span></span><br></pre></td></tr></table></figure>
<p>MNIST 有 70000 张图片，每张图片有 784 个特征。<strong>这是因为每个图片都是28×28像素的，并且每个像素的值介于 0~255 之间</strong>。让我们看一看数据集的某一个数字。你只需要将某个实例的特征向量，reshape为28*28的数组，然后使用 Matplotlib 的imshow函数展示出来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">some_digit = X[<span class="number">36000</span>]    </span><br><span class="line">some_digit_image = some_digit.reshape(<span class="number">28</span>, <span class="number">28</span>)    <span class="comment"># 将样本转为28大小的像素矩阵</span></span><br><span class="line"><span class="comment"># 按‘0’‘1’数值转为灰度图像 </span></span><br><span class="line"><span class="comment"># interpolation当小图像放大时,interpolation ='nearest'效果很好，否则用None。</span></span><br><span class="line">plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation=<span class="string">"nearest"</span>)    </span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/01.jpg" alt></p>
<p>MNIST 数据集已经事先被分成了一个训练集（前 6000 张图片）和一个测试集（最后 10000 张图片）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = X[:<span class="number">60000</span>], X[<span class="number">60000</span>:], y[:<span class="number">60000</span>], y[<span class="number">60000</span>:]</span><br></pre></td></tr></table></figure>
<p>打乱数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">shuffle_index = np.random.permutation(<span class="number">60000</span>)</span><br><span class="line">X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]</span><br></pre></td></tr></table></figure>
<h2 id="训练一个二分类器"><a href="#训练一个二分类器" class="headerlink" title="训练一个二分类器"></a>训练一个二分类器</h2><p>现在我们简化一下问题，只尝试去识别一个数字，比如说，数字 5。这个“数字 5 检测器”就是一个二分类器，能够识别两类别，“是 5”和“非 5”。让我们为这个分类任务创建目标向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在训练和测试集上区分是否为5转为0,1标签矩阵</span></span><br><span class="line">y_train_5 = (y_train == <span class="number">5</span>) <span class="comment"># True for all 5s, False for all other digits.</span></span><br><span class="line">y_test_5 = (y_test == <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p><strong>采用随机梯度下降分类器</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line">sgd_clf = SGDClassifier(random_state=<span class="number">42</span>)    <span class="comment">#如果你想重现结果，你应该固定参数random_state </span></span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br></pre></td></tr></table></figure>
<p>输出预测结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.predict([some_digit])</span><br><span class="line">array([ <span class="literal">True</span>], dtype=bool)</span><br></pre></td></tr></table></figure>
<p>分类器猜测这个数字代表 5（True）。看起来在这个例子当中，它猜对了。现在让我们评估这个模型的性能。</p>
<h2 id="使用交叉验证测量准确性"><a href="#使用交叉验证测量准确性" class="headerlink" title="使用交叉验证测量准确性"></a>使用交叉验证测量准确性</h2><p>评估一个模型的好方法是使用交叉验证，像之前提过一样。<strong>但有时为了有更好的控制权，可以写自己版本的交叉验证，以下代码粗略地做了和cross_val_score()相同的事情，并且输出相同的结果</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</span><br><span class="line">skfolds = StratifiedKFold(n_splits=<span class="number">3</span>, random_state=<span class="number">42</span>)    <span class="comment"># 三组</span></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> skfolds.split(X_train, y_train_5):</span><br><span class="line">    clone_clf = clone(sgd_clf)</span><br><span class="line">    X_train_folds = X_train[train_index]</span><br><span class="line">    y_train_folds = (y_train_5[train_index])</span><br><span class="line">    X_test_fold = X_train[test_index]</span><br><span class="line">    y_test_fold = (y_train_5[test_index])</span><br><span class="line">    clone_clf.fit(X_train_folds, y_train_folds)</span><br><span class="line">    y_pred = clone_clf.predict(X_test_fold)</span><br><span class="line">    n_correct = sum(y_pred == y_test_fold)</span><br><span class="line">    print(n_correct / len(y_pred)) <span class="comment"># prints 0.9502, 0.96565 and 0.96495</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>StratifiedKFold类实现了分层采样，生成的折（fold）包含了各类相应比例的样例。在每一次迭代，上述代码生成分类器的一个克隆版本，在训练折（training folds）的克隆版本上进行训，在测试折（test folds）上进行预测。然后它计算出被正确预测的数目和输出正确预测的比例。</p>
</blockquote>
<p><strong>这里使用sklearn提供的cross_val_score()函数来评估SGDClassifier模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">array([ <span class="number">0.9502</span> , <span class="number">0.96565</span>, <span class="number">0.96495</span>]</span><br></pre></td></tr></table></figure>
<p>有大于 95% 的精度（accuracy），特别高！但要注意这是一个有数据偏差的数据集，这是因为只有 10% 的图片是数字 5，所以你总是猜测某张图片不是 5，你也会有90%的可能性是对的。处理这类问题，要回归到之前讲的准确率和召回率和ORC曲线了。</p>
<h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>对分类器来说，一个好得多的性能评估指标是混淆矩阵，<strong>为了计算混淆矩阵，首先你需要有一系列的预测值，这样才能将预测值与真实值做比较</strong>。你或许想在测试集上做预测。但是我们现在先不碰它。（记住，只有当你处于项目的尾声，当你准备上线一个分类器的时候，你才应该使用测试集）。<strong>相反，你应该使用cross_val_predict()函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><strong>就像 cross_val_score()，cross_val_predict()也使用 K 折交叉验证。它不是返回一个评估分数，而是返回基于每一个测试折做出的一个预测值</strong>。这意味着，对于每一个训练集的样例，你得到一个干净的预测（“干净”是说一个模型在训练过程当中没有用到测试集的数据）。</p>
<p><strong>现在使用 confusion_matrix()函数，你将会得到一个混淆矩阵</strong>。传递目标类(y_train_5)和预测类（y_train_pred）给它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>confusion_matrix(y_train_5, y_train_pred)</span><br><span class="line">array([[<span class="number">53272</span>, <span class="number">1307</span>],</span><br><span class="line">        [ <span class="number">1077</span>, <span class="number">4344</span>]])</span><br></pre></td></tr></table></figure>
<p><strong>混淆矩阵中的每一行表示一个实际的类, 而每一列表示一个预测的类</strong>。该矩阵的第一行认为“非 5”（反例）中的 53272 张被正确归类为 “非 5”（他们被称为真反例，true negatives）, 而其余 1307 被错误归类为”是 5” （假正例，false positives）。第二行认为“是 5” （正例）中的 1077 被错误地归类为“非 5”（假反例，false negatives），其余 4344 正确分类为 “是 5”类（真正例，true positives）。一个完美的分类器将只有真反例和真正例，所以混淆矩阵的非零值仅在其主对角线（左上至右下）。</p>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/12.png" alt></p>
<p>Scikit-Learn 提供了一些函数去计算分类器的指标，包括精确率和召回率（之前的文章是tensorflow，这里主要讲Scikit-Learn）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>precision_score(y_train_5, y_pred) <span class="comment"># == 4344 / (4344 + 1307)</span></span><br><span class="line"><span class="number">0.76871350203503808</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recall_score(y_train_5, y_train_pred) <span class="comment"># == 4344 / (4344 + 1077)</span></span><br><span class="line"><span class="number">0.79136690647482011</span></span><br></pre></td></tr></table></figure>
<p>通常结合精确率和召回率会更加方便，这个指标叫做“F1 值”，特别是当你需要一个简单的方法去比较两个分类器的优劣的时候。F1 值是精确率和召回率的调和平均。普通的平均值平等地看待所有的值，而调和平均会给小的值更大的权重。<strong>所以，要想分类器得到一个高的 F1 值，需要召回率和精确率</strong>。</p>
<script type="math/tex; mode=display">F1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} = 2 * \frac{precison * recall}{precison + recall} = \frac{TP}{TP + \frac{FN + FP}{2}}</script><p>为了计算 F1 值，简单调用f1_score()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f1_score(y_train_5, y_pred)</span><br><span class="line"><span class="number">0.78468208092485547</span></span><br></pre></td></tr></table></figure>
<p>F1 支持那些有着相近精确率和召回率的分类器。这不会总是你想要的。有的场景你会绝大程度地关心精确率，而另外一些场景你会更关心召回率。不幸的是，你不能同时拥有两者。增加精确率会降低召回率，反之亦然。这叫做精确率与召回率之间的折衷<strong>.</strong>  <strong>一般来说，提高分类阈值会减少假正例，从而提高精确率。降低分类阈值会提高召回率。</strong></p>
<p><strong>Scikit-Learn 不让你直接设置阈值，但是它给你提供了设置决策分数的方法，这个决策分数可以用来产生预测。它不是调用分类器的predict()方法，而是调用decision_function()方法。这个方法返回每一个样例的分数值，然后基于这个分数值，使用你想要的任何阈值做出预测。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_scores</span><br><span class="line">array([ <span class="number">161855.74572176</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>threshold = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line">array([ <span class="literal">True</span>], dtype=bool)</span><br></pre></td></tr></table></figure>
<p>SGDClassifier用了一个等于 0 的阈值，所以前面的代码返回了跟predict()方法一样的结果（都返回了true）。<strong>让我们提高这个阈值：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>threshold = <span class="number">200000</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred</span><br><span class="line">array([<span class="literal">False</span>], dtype=bool)</span><br></pre></td></tr></table></figure>
<p>这证明了提高阈值会降调召回率。这个图片实际就是数字 5，当阈值等于 0 的时候，分类器可以探测到这是一个 5，当阈值提高到 20000 的时候，分类器将不能探测到这是数字 5。</p>
<p><strong>那么，你应该如何使用哪个阈值呢？首先，你需要再次使用cross_val_predict()得到每一个样例的分数值，但是这一次指定返回一个决策分数，而不是预测值。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, </span><br><span class="line">                            method=<span class="string">"decision_function"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>现在有了这些分数值。对于任何可能的阈值，使用precision_recall_curve(),你都可以计算精确率和召回率:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>
<p>最后，你可以使用 Matplotlib 画出精确率和召回率，这里把精确率和召回率当作是阈值的一个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_recall_vs_threshold</span><span class="params">(precisions, recalls, thresholds)</span>:</span></span><br><span class="line">    plt.plot(thresholds, precisions[:<span class="number">-1</span>], <span class="string">"b--"</span>, label=<span class="string">"Precision"</span>)</span><br><span class="line">    plt.plot(thresholds, recalls[:<span class="number">-1</span>], <span class="string">"g-"</span>, label=<span class="string">"Recall"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Threshold"</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/02.png" alt></p>
<blockquote>
<p>你也许会好奇为什么精确率曲线比召回率曲线更加起伏不平（右上部分）。原因是精确率有时候会降低，尽管当你提高阈值的时候，通常来说精确率会随之提高。另一方面，当阈值提高时候，召回率只会降低。这也就说明了为什么召回率的曲线更加平滑。</p>
</blockquote>
<p>现在你可以选择适合你任务的最佳阈值。<strong>另一个选出好的精确率/召回率折衷的方法是直接画出精确率对召回率的曲线(PR曲线)</strong>，如图所示。</p>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/03.png" alt></p>
<p><strong>我们假设你决定达到 90% 的准确率，在 70000 附近找到一个阈值。为了作出预测（目前为止只在训练集上预测），你可以运行以下代码，而不是运行分类器的predict()方法。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train_pred_90 = (y_scores &gt; <span class="number">70000</span>)</span><br></pre></td></tr></table></figure>
<p>检查这些预测的准确率和召回率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>precision_score(y_train_5, y_train_pred_90)</span><br><span class="line"><span class="number">0.8998702983138781</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>recall_score(y_train_5, y_train_pred_90)</span><br><span class="line"><span class="number">0.63991883416343853</span></span><br></pre></td></tr></table></figure>
<h2 id="ROC-曲线"><a href="#ROC-曲线" class="headerlink" title="ROC 曲线"></a>ROC 曲线</h2><p>受试者工作特征（ROC）曲线是另一个二分类器常用的工具。它非常类似与准确率/召回率曲线（PR曲线），但不是画出准确率对召回率的曲线，<strong>ROC 曲线是真正例率（true positive rate，另一个名字叫做召回率）对假正例率（false positive rate, FPR）的曲线</strong>。FPR 是反例被错误分成正例的比率。它等于 1 减去真反例率（true negative rate， TNR）。TNR是反例被正确分类的比率。TNR也叫做特异性。所以 ROC 曲线画出召回率对（1 减特异性）的曲线。</p>
<script type="math/tex; mode=display">TPR = \frac{TP}{P} = \frac{TP}{TP+FN}</script><script type="math/tex; mode=display">
FPR = \frac{FP}{N} = \frac{FP}{FP+TN} = 1-TNR</script><script type="math/tex; mode=display">
TNR = \frac{TN}{N} = \frac{TN}{TN+FP}</script><p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/04.png" alt></p>
<p><strong>为了画出 ROC 曲线，你首先需要计算各种不同阈值下的 TPR、FPR，使用roc_curve()函数：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>
<p>然后你可以使用 matplotlib，画出 FPR 对 TPR 的曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc_curve</span><span class="params">(fpr, tpr, label=None)</span>:</span></span><br><span class="line">    plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=label)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'k--'</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</span><br><span class="line">plot_roc_curve(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/05.png" alt></p>
<p>一个比较分类器之间优劣的方法是：测量ROC曲线下的面积（AUC）**。一个完美的分类器的 ROC AUC 等于 1，而一个纯随机分类器的 ROC AUC 等于 0.5。Scikit-Learn 提供了一个函数来计算 ROC AUC：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>roc_auc_score(y_train_5, y_scores)</span><br><span class="line"><span class="number">0.97061072797174941</span></span><br></pre></td></tr></table></figure>
<p><strong>因为 ROC 曲线跟准确率/召回率曲线（或者叫 PR）很类似，你或许会好奇如何决定使用哪一个曲线呢？一个笨拙的规则是，优先使用 PR 曲线当正例很少，或者当你关注假正例多于假反例的时候。其他情况使用 ROC 曲线</strong>。举例子，回顾前面的 ROC 曲线和 ROC AUC 数值，你或许人为这个分类器很棒。但是这几乎全是因为只有少数正例（“是 5”），而大部分是反例（“非 5”）。相反，PR 曲线清楚显示出这个分类器还有很大的改善空间（PR 曲线应该尽可能地靠近右上角）。</p>
<p>我们训练一个RandomForestClassifier，然后拿它的的ROC曲线和ROC AUC数值去跟SGDClassifier的比较。<strong>首先你需要得到训练集每个样例的数值</strong>。<strong>但是由于随机森林分类器的工作方式，RandomForestClassifier不提供decision_function()方法。相反，它提供了predict_proba()方法</strong>。Skikit-Learn分类器通常二者中的一个。<strong>predict_proba()方法返回一个数组，数组的每一行代表一个样例，每一列代表一个类。数组当中的值的意思是：给定一个样例属于给定类的概率。比如，70%的概率这幅图是数字 5。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>,</span><br><span class="line">                                    method=<span class="string">"predict_proba"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>但是要画 ROC 曲线，你需要的是样例的分数，而不是概率</strong>。一个简单的解决方法是使用正例的概率当作样例的分数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_scores_forest = y_probas_forest[:, <span class="number">1</span>] <span class="comment"># score = proba of positive class 预测为正例概率</span></span><br><span class="line">fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)</span><br></pre></td></tr></table></figure>
<p>现在你即将得到 ROC 曲线。<strong>将前面一个分类器的 ROC 曲线一并画出来是很有用的，可以清楚地进行比较</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(fpr, tpr, <span class="string">"b:"</span>, label=<span class="string">"SGD"</span>)</span><br><span class="line">plot_roc_curve(fpr_forest, tpr_forest, <span class="string">"Random Forest"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"bottom right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/06.png" alt></p>
<p>如你所见，RandomForestClassifier的 ROC 曲线比SGDClassifier的好得多：<strong>它更靠近左上角。所以，它的 ROC AUC 也会更大。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>roc_auc_score(y_train_5, y_scores_forest)</span><br><span class="line"><span class="number">0.99312433660038291</span></span><br></pre></td></tr></table></figure>
<p>现在你知道如何训练一个二分类器，选择合适的标准，<strong>使用交叉验证去评估你的分类器，选择满足你需要的准确率/召回率折衷方案，和比较不同模型的 ROC 曲线和 ROC AUC 数值</strong>。现在让我们检测更多的数字，而不仅仅是一个数字 5。</p>
<h2 id="多类别分类"><a href="#多类别分类" class="headerlink" title="多类别分类"></a>多类别分类</h2><p><strong>一些算法（比如随机森林分类器或者朴素贝叶斯分类器）可以直接处理多类分类问题。其他一些算法（比如 SVM 分类器或者线性分类器）则是严格的二分类器。然后，有许多策略可以让你用二分类器去执行多类分类。</strong></p>
<ul>
<li>一个方法是：训练10个二分类器，每一个对应一个数字（探测器 0，探测器 1，探测器 2，以此类推）。然后当你想对某张图片进行分类的时候，让每一个分类器对这个图片进行分类，选出决策分数最高的那个分类器（One vs all 里面分数最高的One）。这叫做“一对所有”（OvA）策略</li>
<li>另一个策略是对每一对数字都训练一个二分类器：一个分类器用来处理数字 0 和数字 1，一个用来处理数字 0 和数字 2，一个用来处理数字 1 和 2，以此类推。这叫做“一对一”（OvO）策略。如果有 N 个类。你需要训练N*(N-1)/2个分类器。</li>
</ul>
<p><strong>一些算法（比如 SVM 分类器）在训练集的大小上很难扩展，所以对于这些算法，OvO 是比较好的，因为它可以在小的数据集上面可以更多地训练，较之于巨大的数据集而言。但是，对于大部分的二分类器来说，OvA 是更好的选择。Scikit-Learn 可以探测出你想使用一个二分类器去完成多分类的任务，它会自动地执行 OvA（除了 SVM 分类器，它使用 OvO）</strong>让我们试一下SGDClassifier.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.fit(X_train, y_train) <span class="comment"># y_train, not y_train_5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br></pre></td></tr></table></figure>
<p>上面的代码在训练集上训练了一个SGDClassifier。这个分类器处理原始的目标class，从 0 到 9（y_train），而不是仅仅探测是否为 5 （y_train_5）。然后它做出一个判断（在这个案例下只有一个正确的数字）。<strong>在幕后，Scikit-Learn 实际上训练了 10 个二分类器，每个分类器都产到一张图片的决策数值，选择数值最高的那个类</strong>。</p>
<p>为了证明这是真实的，<strong>你可以调用decision_function()方法。不是返回每个样例的一个数值，而是返回 10 个数值，一个数值对应于一个类</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_digit_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_digit_scores</span><br><span class="line">array([[<span class="number">-311402.62954431</span>, <span class="number">-363517.28355739</span>, <span class="number">-446449.5306454</span> ,</span><br><span class="line">        <span class="number">-183226.61023518</span>, <span class="number">-414337.15339485</span>, <span class="number">161855.74572176</span>,</span><br><span class="line">        <span class="number">-452576.39616343</span>, <span class="number">-471957.14962573</span>, <span class="number">-518542.33997148</span>,</span><br><span class="line">        <span class="number">-536774.63961222</span>]])</span><br></pre></td></tr></table></figure>
<p>最高数值是对应于类别 5 :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argmax(some_digit_scores)    <span class="comment"># 找最大值的索引</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.classes_</span><br><span class="line">array([ <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_clf.classes[<span class="number">5</span>]    <span class="comment"># 用索引匹配类别</span></span><br><span class="line"><span class="number">5.0</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>一个分类器被训练好了之后，它会保存目标类别列表到它的属性classes<em> 中去，按照值排序。在本例子当中，在classes</em> 数组当中的每个类的索引方便地匹配了类本身，比如，索引为 5 的类恰好是类别 5 本身。但通常不会这么幸运。</p>
</blockquote>
<p><strong>如果你想强制 Scikit-Learn 使用 OvO 策略或者 OvA 策略，你可以使用OneVsOneClassifier类或者OneVsRestClassifier类。创建一个样例，传递一个二分类器给它的构造函数</strong>。举例子，下面的代码会创建一个多类分类器，使用 OvO 策略，基于SGDClassifier。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=<span class="number">42</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ovo_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(ovo_clf.estimators_)</span><br><span class="line"><span class="number">45</span></span><br></pre></td></tr></table></figure>
<p>训练一个RandomForestClassifier同样简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.fit(X_train, y_train)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.predict([some_digit])</span><br><span class="line">array([ <span class="number">5.</span>])</span><br></pre></td></tr></table></figure>
<p>这次 Scikit-Learn 没有必要去运行 OvO 或者 OvA，因为<strong>随机森林分类器能够直接将一个样例分到多个类别。你可以调用predict_proba()，得到样例对应的类别的概率值的列表</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_clf.predict_proba([some_digit])</span><br><span class="line">array([[ <span class="number">0.1</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.1</span>, <span class="number">0.</span> , <span class="number">0.8</span>, <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.</span> ]])</span><br></pre></td></tr></table></figure>
<p>你可以看到这个分类器相当确信它的预测：在数组的索引 5 上的 0.8，意味着这个模型以 80% 的概率估算这张图片代表数字 5。它也认为这个图片可能是数字 0 或者数字 3，分别都是 10% 的几率。</p>
<p>现在当然你想评估这些分类器。<strong>像平常一样，你想使用交叉验证</strong>。让我们用cross_val_score()来评估SGDClassifier的精度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">array([ <span class="number">0.84063187</span>, <span class="number">0.84899245</span>, <span class="number">0.86652998</span>])</span><br></pre></td></tr></table></figure>
<p>在所有测试折（test fold）上，它有 84% 的精度。如果你是用一个随机的分类器，你将会得到 10% 的正确率。所以这不是一个坏的分数，但是你可以做的更好。举例子，简单将输入正则化，将会提高精度到 90% 以上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scaler = StandardScaler()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))    <span class="comment"># 特征正则化，没说用哪种</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cross_val_score(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">array([ <span class="number">0.91011798</span>, <span class="number">0.90874544</span>, <span class="number">0.906636</span> ])</span><br></pre></td></tr></table></figure>
<h2 id="误差分析："><a href="#误差分析：" class="headerlink" title="误差分析："></a>误差分析：</h2><p><strong>首先，你可以检查混淆矩阵。你需要使用cross_val_predict()做出预测，然后调用confusion_matrix()函数</strong>，像你早前做的那样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>conf_mx = confusion_matrix(y_train, y_train_pred)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>conf_mx</span><br><span class="line">array([[<span class="number">5725</span>, <span class="number">3</span>, <span class="number">24</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">49</span>, <span class="number">50</span>, <span class="number">10</span>, <span class="number">39</span>, <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">2</span>, <span class="number">6493</span>, <span class="number">43</span>, <span class="number">25</span>, <span class="number">7</span>, <span class="number">40</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">109</span>, <span class="number">8</span>],</span><br><span class="line">        [ <span class="number">51</span>, <span class="number">41</span>, <span class="number">5321</span>, <span class="number">104</span>, <span class="number">89</span>, <span class="number">26</span>, <span class="number">87</span>, <span class="number">60</span>, <span class="number">166</span>, <span class="number">13</span>],</span><br><span class="line">        [ <span class="number">47</span>, <span class="number">46</span>, <span class="number">141</span>, <span class="number">5342</span>, <span class="number">1</span>, <span class="number">231</span>, <span class="number">40</span>, <span class="number">50</span>, <span class="number">141</span>, <span class="number">92</span>],</span><br><span class="line">        [ <span class="number">19</span>, <span class="number">29</span>, <span class="number">41</span>, <span class="number">10</span>, <span class="number">5366</span>, <span class="number">9</span>, <span class="number">56</span>, <span class="number">37</span>, <span class="number">86</span>, <span class="number">189</span>],</span><br><span class="line">        [ <span class="number">73</span>, <span class="number">45</span>, <span class="number">36</span>, <span class="number">193</span>, <span class="number">64</span>, <span class="number">4582</span>, <span class="number">111</span>, <span class="number">30</span>, <span class="number">193</span>, <span class="number">94</span>],</span><br><span class="line">        [ <span class="number">29</span>, <span class="number">34</span>, <span class="number">44</span>, <span class="number">2</span>, <span class="number">42</span>, <span class="number">85</span>, <span class="number">5627</span>, <span class="number">10</span>, <span class="number">45</span>, <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">25</span>, <span class="number">24</span>, <span class="number">74</span>, <span class="number">32</span>, <span class="number">54</span>, <span class="number">12</span>, <span class="number">6</span>, <span class="number">5787</span>, <span class="number">15</span>, <span class="number">236</span>],</span><br><span class="line">        [ <span class="number">52</span>, <span class="number">161</span>, <span class="number">73</span>, <span class="number">156</span>, <span class="number">10</span>, <span class="number">163</span>, <span class="number">61</span>, <span class="number">25</span>, <span class="number">5027</span>, <span class="number">123</span>],</span><br><span class="line">        [ <span class="number">43</span>, <span class="number">35</span>, <span class="number">26</span>, <span class="number">92</span>, <span class="number">178</span>, <span class="number">28</span>, <span class="number">2</span>, <span class="number">223</span>, <span class="number">82</span>, <span class="number">5240</span>]])</span><br></pre></td></tr></table></figure>
<p>这里是一对数字。使用 Matplotlib 的matshow()函数，将混淆矩阵以图像的方式呈现，将会更加方便</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.matshow(conf_mx, cmap=plt.cm.gray)    <span class="comment"># #灰度图,对应位置的值越大色块越亮</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/07.png" alt></p>
<p>这个混淆矩阵看起来相当好，因为大多数的图片在主对角线上。在主对角线上意味着被分类正确。数字 5 对应的格子看起来比其他数字要暗淡许多。这可能是数据集当中数字 5 的图片比较少，又或者是分类器对于数字 5 的表现不如其他数字那么好。你可以验证两种情况.</p>
<p>让我们关注仅包含误差数据的图像呈现。<strong>首先你需要将混淆矩阵的每一个值除以相应类别的图片的总数目。这样子，你可以比较错误率，而不是绝对的错误数（这对大的类别不公平）</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">row_sums = conf_mx.sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">norm_conf_mx = conf_mx / row_sums</span><br></pre></td></tr></table></figure>
<p><strong>现在让我们用 0 来填充对角线。这样子就只保留了被错误分类的数据</strong>。让我们画出这个结果。(此时数值为错误率)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.fill_diagonal(norm_conf_mx, <span class="number">0</span>)</span><br><span class="line">plt.matshow(norm_conf_mx, cmap=plt.cm.gray)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/08.png" alt></p>
<p>现在你可以清楚看出分类器制造出来的各类误差。记住：行代表实际类别，列代表预测的类别。第 8、9 列相当亮，这告诉你许多图片被误分成数字 8 或者数字 9。相似的，第 8、9 行也相当亮，告诉你数字 8、数字 9 经常被误以为是其他数字。相反，一些行相当黑，比如第一行：这意味着大部分的数字 1 被正确分类（一些被误分类为数字 8 ）。留意到误差图不是严格对称的。举例子，比起将数字 8 误分类为数字 5 的数量，有更多的数字 5 被误分类为数字 8。</p>
<p><strong>分析混淆矩阵通常可以给你提供深刻的见解去改善你的分类器</strong>。回顾这幅图，看样子你应该努力改善分类器在数字 8 和数字 9 上的表现，和纠正 3/5 的混淆。举例子，你可以尝试去收集更多的数据，或者你可以构造新的、有助于分类器的特征。举例子，写一个算法去数闭合的环（比如，数字 8 有两个环，数字 6 有一个， 5 没有）。又或者你可以预处理图片（比如，使用 Scikit-Learn，Pillow， OpenCV）去构造一个模式，比如闭合的环。</p>
<p>分析独特的误差，是获得关于你的分类器是如何工作及其为什么失败的洞见的一个好途径。但是这相对难和耗时。举例子，我们可以画出数字 3 和 5 的例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cl_a, cl_b = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)]</span><br><span class="line">X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)]</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>); plot_digits(X_aa[:<span class="number">25</span>], ../images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">222</span>); plot_digits(X_ab[:<span class="number">25</span>], ../images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">223</span>); plot_digits(X_ba[:<span class="number">25</span>], ../images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">224</span>); plot_digits(X_bb[:<span class="number">25</span>], ../images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/09.png" alt></p>
<p>左边两个5*5的块将数字识别为 3，右边的将数字识别为 5。一些被分类器错误分类的数字（比如左下角和右上角的块）是书写地相当差，甚至让人类分类都会觉得很困难（比如第 8 行第 1 列的数字 5，看起来非常像数字 3 ）。但是，大部分被误分类的数字，在我们看来都是显而易见的错误。很难明白为什么分类器会分错。原因是我们使用的简单的SGDClassifier，这是一个线性模型。它所做的全部工作就是分配一个类权重给每一个像素，然后当它看到一张新的图片，它就将加权的像素强度相加，每个类得到一个新的值。所以，因为 3 和 5 只有一小部分的像素有差异，这个模型很容易混淆它们。</p>
<p>3 和 5 之间的主要差异是连接顶部的线和底部的线的细线的位置。如果你画一个 3，连接处稍微向左偏移，分类器很可能将它分类成 5。反之亦然。<strong>换一个说法，这个分类器对于图片的位移和旋转相当敏感。所以，减轻 3/5 混淆的一个方法是对图片进行预处理，确保它们都很好地中心化和不过度旋转。这同样很可能帮助减轻其他类型的错误</strong>。</p>
<h2 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h2><p>先看一个简单点的例子，仅仅是为了阐明的目的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">y_train_large = (y_train &gt;= <span class="number">7</span>)</span><br><span class="line">y_train_odd = (y_train % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">y_multilabel = np.c_[y_train_large, y_train_odd]</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train, y_multilabel)</span><br></pre></td></tr></table></figure>
<p><strong>这段代码创造了一个y_multilabel数组，里面包含两个目标标签。第一个标签指出这个数字是否为大数字（7，8 或者 9），第二个标签指出这个数字是否是奇数</strong>。<strong>接下来几行代码会创建一个KNeighborsClassifier样例（它支持多标签分类，但不是所有分类器都可以）</strong>，然后我们使用多目标数组来训练它。现在你可以生成一个预测，然后它输出两个标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>knn_clf.predict([some_digit])</span><br><span class="line">array([[<span class="literal">False</span>, <span class="literal">True</span>]], dtype=bool)</span><br></pre></td></tr></table></figure>
<p>它工作正确。数字 5 不是大数（False），同时是一个奇数（True）</p>
<p>有许多方法去评估一个多标签分类器，和选择正确的量度标准，这取决于你的项目。举个例子，<strong>一个方法是对每个个体标签去量度 F1 值（或者前面讨论过的其他任意的二分类器的量度标准），然后计算平均值</strong>。下面的代码计算全部标签的平均 F1 值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f1_score(y_train, y_train_knn_pred, average=<span class="string">"macro"</span>)</span><br><span class="line"><span class="number">0.96845540180280221</span></span><br></pre></td></tr></table></figure>
<p><strong>这里假设所有标签有着同等的重要性，但可能不是这样</strong>。特别是，如果你的 Alice 的照片比 Bob 或者 Charlie 更多的时候，也许你想让分类器在 Alice 的照片上具有更大的权重。<strong>一个简单的选项是：给每一个标签的权重等于它的支持度（比如，那个标签的样例的数目）。为了做到这点，简单地在上面代码中设置average=”weighted”。</strong></p>
<h2 id="多输出分类"><a href="#多输出分类" class="headerlink" title="多输出分类"></a>多输出分类</h2><p>我们即将讨论的最后一种分类任务被叫做“多输出-多类分类”（或者简称为多输出分类）。它是多标签分类的简单泛化，在这里每一个标签可以是多类别的（比如说，它可以有多于两个可能值）。</p>
<p><strong>为了说明这点，我们建立一个系统，它可以去除图片当中的噪音。它将一张混有噪音的图片作为输入，期待它输出一张干净的数字图片，用一个像素强度的数组表示，就像 MNIST 图片那样。注意到这个分类器的输出是多标签的（一个像素一个标签）和每个标签可以有多个值（像素强度取值范围从 0 到 255）。所以它是一个多输出分类系统的例子。</strong></p>
<blockquote>
<p>分类与回归之间的界限是模糊的，比如这个例子。按理说，预测一个像素的强度更类似于一个回归任务，而不是一个分类任务。而且，多输出系统不限于分类任务。你甚至可以让你一个系统给每一个样例都输出多个标签，包括类标签和值标签。</p>
</blockquote>
<p>让我们从 MNIST 的图片创建训练集和测试集开始，然后给图片的像素强度添加噪声，这里是用 NumPy 的randint()函数。目标图像是原始图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">noise = rnd.randint(<span class="number">0</span>, <span class="number">100</span>, (len(X_train), <span class="number">784</span>))</span><br><span class="line">noise = rnd.randint(<span class="number">0</span>, <span class="number">100</span>, (len(X_test), <span class="number">784</span>))</span><br><span class="line">X_train_mod = X_train + noise</span><br><span class="line">X_test_mod = X_test + noise</span><br><span class="line">y_train_mod = X_train</span><br><span class="line">y_test_mod = X_test</span><br></pre></td></tr></table></figure>
<p>让我们看一下测试集当中的一张图片（是的，我们在窥探测试集，所以你应该马上邹眉）：</p>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/10.png" alt></p>
<p>左边的加噪声的输入图片。右边是干净的目标图片。现在我们训练分类器，让它清洁这张图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_mod, y_train_mod)</span><br><span class="line">clean_digit = knn_clf.predict([X_test_mod[some_index]])</span><br><span class="line">plot_digit(clean_digit)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/07/11/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（二）：分类/11.png" alt></p>
<hr>
<p>到这里就讲完分类的内容了，有点混乱对不对，我们来总结梳理一下。</p>
<ul>
<li><p>要掌握自定义k折交叉验证的方法（≈cross_val_score）</p>
</li>
<li><p>cross_val_score为验证模型的一个好方法，但是只能得到准确率的评估分数</p>
</li>
<li><p>如果正反例数据偏差大，我们需要用到混淆矩阵，这个矩阵要用到预测值而不是评估分数，所以改cross_val_predict，这会返回每个测试折做出的预测值，即y_train_pred </p>
</li>
<li><p>利用预测值y_train_pred可以得到混淆矩阵，精确率，召回率，F1</p>
</li>
<li><p>有时我们需要阈值来平衡精确率，召回率，而Scikit-Learn 不让你直接设置阈值，它会调用decision_function()方法。<strong>返回样例的分数值</strong>，然后基于这个分数值，使用你想要的任何阈值做出预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_scores</span><br><span class="line">array([ <span class="number">161855.74572176</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>threshold = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line">array([ <span class="literal">True</span>], dtype=bool)</span><br></pre></td></tr></table></figure>
</li>
<li><p>每次都设定阈值不是一个完美的方法，如何才能找到合适的阈值呢？你需要再次使用cross_val_predict()得到每一个样例的分数值，<strong>但是这一次指定返回一个决策分数</strong>，<strong>而不是预测值</strong>。(阈值相关，就要进行打分)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, </span><br><span class="line">                            method=<span class="string">"decision_function"</span>)</span><br></pre></td></tr></table></figure>
<p>现在有了这些分数值。对于任何可能的阈值，使用precision_recall_curve(),你都可以计算精确率和召回率；precisions, recalls, thresholds是任何阈值的范围值，可以变化曲线和PR曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>
</li>
<li><p>与PR曲线另一个相关的是ROC曲线（TPR/FPR），为了画出 ROC 曲线，你首先需要计算各种不同阈值下的 TPR、FPR，使用roc_curve()函数（还是要打分）；跳过ROC曲线(其实相当于已经做了)，想直接计算出ROC AUC也行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">roc_auc_score(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>
<p>如果想得到RandomForestClassifier的ROC曲线，由于RandomForestClassifier不提供decision_function()方法，相反，它提供了predict_proba()方法（另外一种概率打分），返回概率值，此时用正例概率作为分值。例如70%的概率是垃圾邮件。</p>
<p>另外，<strong>因为 ROC 曲线跟准确率/召回率曲线（或者叫 PR）很类似，你或许会好奇如何决定使用哪一个曲线呢？一个笨拙的规则是，优先使用 PR 曲线当正例很少，或者当你关注假正例多于假反例的时候。其他情况使用 ROC 曲线</strong></p>
</li>
<li><p>多类别分类有一对一ovo, 一对多ova两种方法，一般svm由于在训练集的大小上很难扩展，因为它可以在小的数据集上面可以更多地训练，故用ovo，其他大部分用ova。如果Scikit-Learn嗅探出你想做一个多分类任务，它会自动使用ova，svm训练器除外</p>
</li>
<li><p>误差分析，将混淆矩阵归一化后用图片色块输出，查看哪些类别经常被错误分类。</p>
</li>
</ul>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/09/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（一）：一个完整的程序/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/09/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（一）：一个完整的程序/" itemprop="url">Sklearn 与 TensorFlow 机器学习实用指南（一）：一个完整的程序</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-09T16:06:11+08:00">
                2018-07-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sklearn-与-TensorFlow-机器学习实用指南/" itemprop="url" rel="index">
                    <span itemprop="name">Sklearn 与 TensorFlow 机器学习实用指南</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9,367
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  38
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>写在前面：这个系列打算把「Hands-On Machine Learning with Scikit-Learn and TensorFlow 」重新梳理一遍，这本书在看完机器学习基础知识之后有一个很好的算法实践，对于算法落地有很多帮助。这次写的Sklearn 与 TensorFlow 机器学习实用指南系列，目的是让自己更清楚算法的每个流程处理，加强对一些机器学习模型理解。这本书在<a href="https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF" target="_blank" rel="noopener">github</a>有中文的翻译版本（还在更新）.</p>
<hr>
<h1 id="拆分数据集"><a href="#拆分数据集" class="headerlink" title="拆分数据集"></a>拆分数据集</h1><h2 id="训练集-测试集"><a href="#训练集-测试集" class="headerlink" title="训练集+测试集"></a>训练集+测试集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_train_test</span><span class="params">(data, test_ratio)</span>:</span></span><br><span class="line">    shuffled_indices = np.random.permutation(len(data))    <span class="comment"># 打乱序列</span></span><br><span class="line">    test_set_size = int(len(data) * test_ratio)    <span class="comment"># 拆分比例</span></span><br><span class="line">    test_indices = shuffled_indices[:test_set_size]</span><br><span class="line">    train_indices = shuffled_indices[test_set_size:]</span><br><span class="line">    <span class="keyword">return</span> data.iloc[train_indices], data.iloc[test_indices]</span><br><span class="line">    </span><br><span class="line">train_set, test_set = split_train_test(housing, <span class="number">0.2</span>)    <span class="comment"># housing数据二八拆分</span></span><br></pre></td></tr></table></figure>
<p>或者直接将整体数据打乱，然后按需取量。(california_housing_dataframe为谷歌机器学习教程提供的加州住房数据)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(	<span class="comment"># 整体打乱</span></span><br><span class="line">    np.random.permutation(california_housing_dataframe.index))</span><br><span class="line">train_set = california_housing_dataframe.head(<span class="number">12000</span>)</span><br><span class="line">test_set = california_housing_dataframe.tail(<span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<p>以上为训练集+测试集的拆分方式</p>
<h2 id="训练集-验证集-测试集"><a href="#训练集-验证集-测试集" class="headerlink" title="训练集+验证集+测试集"></a>训练集+验证集+测试集</h2><p><strong>这样的拆分方式主要有存在一些不足。1、程序多次运行后，测试集的数据有可能会加入到训练集当中，调参时用于改进模型超参数的测试集会造成过拟合。2、不便于新数据的加入</strong></p>
<p>更好的办法是将数据集拆分为训练集+验证集+测试集。</p>
<p><img src="/2018/07/09/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（一）：一个完整的程序/01.png" alt></p>
<p>那如何解决新加入数据的问题呢？<strong>一个通常的解决办法是使用每个实例的识别码</strong>，以判定是否这个实例是否应该放入测试集（假设实例有单一且不变的识别码）。<strong>例如，你可以计算出每个实例识别码的哈希值，只保留其最后一个字节，如果值小于等于 51（约为 256 的 20%），就将其放入测试集。这样可以保证在多次运行中，测试集保持不变，即使更新了数据集。新的测试集会包含新实例中的 20%，但不会有之前位于训练集的实例</strong>。可能很多数据没有稳定的特征，最简单的办法就是利用索引作为识别码。下面的代码根据识别码按0.7,0.2,0.1比例拆分训练集、验证集和测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数identifier为单一且不变的识别码，可以为索引id</span></span><br><span class="line"><span class="comment"># hash(np.int64(identifier)).digest()[-1]返回识别码的哈希摘要值的最后一个字节</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate_set_check</span><span class="params">(identifier, validate_ratio, test_ratio, hash)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">256</span> * test_ratio &lt;= hash(np.int64(identifier)).digest()[<span class="number">-1</span>] &lt; <span class="number">256</span> *      (validate_ratio+test_ratio)  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_set_check</span><span class="params">(identifier, test_ratio, hash)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> hash(np.int64(identifier)).digest()[<span class="number">-1</span>] &lt; <span class="number">256</span> * test_ratio    <span class="comment"># 记录满足条件的索引</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_train_test_by_id</span><span class="params">(data, validate_ratio, test_ratio, id_column, hash=hashlib.md5)</span>:</span></span><br><span class="line">    ids = data[id_column]	<span class="comment"># 确定识别码</span></span><br><span class="line">    in_validate_set = ids.apply(<span class="keyword">lambda</span> id_: validate_set_check(id_, validate_ratio, test_ratio，hash))</span><br><span class="line">    in_test_set = ids.apply(<span class="keyword">lambda</span> id_: test_set_check(id_, test_ratio, hash))</span><br><span class="line">    combine_set = np.bitwise_or(in_validate_set, in_test_set)</span><br><span class="line">    <span class="keyword">return</span> data.loc[~combine_set], data.loc[in_validate_set], data.loc[in_test_set]			                       </span><br><span class="line">housing_with_id = housing.reset_index()   <span class="comment"># housing数据增加一个索引列，放在数据的第一列</span></span><br><span class="line">train_set, validate_set, test_set = split_train_test_by_id(housing_with_id, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="string">"index"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="分成采样"><a href="#分成采样" class="headerlink" title="分成采样"></a>分成采样</h2><p>另外一种拆分方式：<strong>分成采样</strong></p>
<p><strong>将人群分成均匀的子分组，称为分层</strong>，从每个分层去除合适数量的实例，以保证测试集对总人数有代表性。例如，美国人口的 51.3% 是女性，48.7% 是男性。所以在美国，严谨的调查需要保证样本也是这个比例：513 名女性，487 名男性作为数据样本。数据集中的每个分层都要有足够的实例位于你的数据中，这点很重要。否则，对分层重要性的评估就会有偏差。这意味着，<strong>你不能有过多的分层</strong>，<strong>且每个分层都要足够大</strong>。后面的代码通过将收入中位数除以 1.5（以限制收入分类的数量），创建了一个收入类别属性，<strong>用ceil对值舍入（以产生离散的分类），然后将所有大于 5的分类归入到分类5 </strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预处理，创建"income_cat"属性 </span></span><br><span class="line"><span class="comment"># 凡是会对原数组作出修改并返回一个新数组的，往往都有一个 inplace可选参数</span></span><br><span class="line"><span class="comment"># inplace=True,原数组名对应的内存值直接改变;inplace=False,原数组名对应的内存值并不改变，新的结果赋给一个新的数组.</span></span><br><span class="line">housing[<span class="string">"income_cat"</span>] = np.ceil(housing[<span class="string">"median_income"</span>] / <span class="number">1.5</span>)</span><br><span class="line">housing[<span class="string">"income_cat"</span>].where(housing[<span class="string">"income_cat"</span>] &lt; <span class="number">5</span>, <span class="number">5.0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在，就可以根据收入分类，进行分层采样。你可以使用 Scikit-Learn 的StratifiedShuffleSplit类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"></span><br><span class="line"><span class="comment"># random_state为随机种子生成器，可以得到相同的随机结果</span></span><br><span class="line"><span class="comment"># n_splits是将训练数据分成train/test对的组数，这里汇总成一组数据</span></span><br><span class="line">split = StratifiedShuffleSplit(n_splits=<span class="number">1</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)    </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> split.split(housing, housing[<span class="string">"income_cat"</span>]):</span><br><span class="line">    strat_train_set = housing.loc[train_index]</span><br><span class="line">    strat_test_set = housing.loc[test_index]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在，你需要删除income_cat属性，使数据回到初始状态：    </span></span><br><span class="line"><span class="keyword">for</span> set <span class="keyword">in</span> (strat_train_set, strat_test_set):</span><br><span class="line">    set.drop([<span class="string">"income_cat"</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><h2 id="将原始数据映射到特征"><a href="#将原始数据映射到特征" class="headerlink" title="将原始数据映射到特征"></a>将原始数据映射到特征</h2><p>我们在进行机器学习的时候，采用的数据样本往往是<strong>矢量</strong>（特征矢量），而我们的<strong>原始数据</strong>并不是以矢量的形式呈现给我们的，这是便需要将数据映射到特征</p>
<h3 id="整数和浮点数映射"><a href="#整数和浮点数映射" class="headerlink" title="整数和浮点数映射"></a>整数和浮点数映射</h3><p>直接映射便ok（虽然机器学习是<strong>根据浮点值进行的训练</strong>，但是不需要将整数6转换为6.0，这个过程是默认的）</p>
<h3 id="字符串映射"><a href="#字符串映射" class="headerlink" title="字符串映射"></a>字符串映射</h3><p>好多时候，有的特征是字符串，比如此前训练的加利福尼亚房产数据集中的<strong>街区名称</strong>，机器学习是无法根据字符串来学习规律的，所以需要转换。但是存在一个问题，如果字符特征是’’一环’’ ‘’二环’’ ‘’三环’’…（代表某个城市的地理位置），那么对其进行数值转换的时候，是不可以编码为形如1，2，3，4…这样的数据的，因为其存在数据大小的问题，学习模型会把他们的大小关系作为特征而学习，所以我们需要引入<a href="https://www.cnblogs.com/king-lps/p/7846414.html" target="_blank" rel="noopener"><strong>独热编码</strong></a>,（具体解释见链接，解释的很好）.<strong>我们需要把这些文本标签转换为数字</strong>。Scikit-Learn 为这个任务提供了一个转换器LabelEncoder：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单来说 LabelEncoder 是对不连续的数字或者文本进行编号</span></span><br><span class="line"><span class="comment"># le.fit([1,5,67,100])</span></span><br><span class="line"><span class="comment"># le.transform([1,1,100,67,5])</span></span><br><span class="line"><span class="comment"># 输出： array([0,0,3,2,1])</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>encoder = LabelEncoder()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat = housing[<span class="string">"ocean_proximity"</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat_encoded = encoder.fit_transform(housing_cat)	<span class="comment"># 装换器</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat_encoded</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, ..., <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>译注:</p>
<p>在原书中使用<code>LabelEncoder</code>转换器来转换文本特征列的方式是错误的，该转换器只能用来转换标签（正如其名）。在这里使用<code>LabelEncoder</code>没有出错的原因是该数据只有一列文本特征值，在有多个文本特征列的时候就会出错。应使用<code>factorize()</code>方法来进行操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; housing_cat_encoded, housing_categories = housing_cat.factorize()</span><br><span class="line">&gt; housing_cat_encoded[:<span class="number">10</span>]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<p>处理离散特征这还不够，Scikit-Learn 提供了一个编码器OneHotEncoder，用于将整书分类值转变为独热向量。注意fit_transform()用于 2D 数组，而housing_cat_encoded是一个 1D 数组，所以需要将其变形：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># reshape(-1,1)里面的-1代表将数据自动计算有多少行，但是列数明确设置为1</span></span><br><span class="line"><span class="comment"># reshape(-1)则是变形为1行和自动计算有多少列</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>encoder = OneHotEncoder()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat_1hot</span><br><span class="line">&lt;<span class="number">16513</span>x5 sparse matrix of type <span class="string">'&lt;class '</span>numpy.float64<span class="string">'&gt;'</span></span><br><span class="line">    <span class="keyword">with</span> <span class="number">16513</span> stored elements <span class="keyword">in</span> Compressed Sparse Row format&gt;</span><br></pre></td></tr></table></figure>
<p>注意输出结果是一个 SciPy 稀疏矩阵，而不是 NumPy 数组。当类别属性有数千个分类时，这样非常有用。经过独热编码，我们得到了一个有数千列的矩阵，这个矩阵每行只有一个 1，其余都是 0。使用大量内存来存储这些 0 非常浪费，所以稀疏矩阵只存储非零元素的位置。你可以像一个 2D 数据那样进行使用，但是如果你真的想将其转变成一个（密集的）NumPy 数组，只需调用toarray()方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat_1hot.toarray()</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>使用类LabelBinarizer，我们可以用一步执行这两个转换（从文本分类到整数分类，再从整数分类到独热向量）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>encoder = LabelBinarizer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat_1hot = encoder.fit_transform(housing_cat)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_cat_1hot</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<p>注意默认返回的结果是一个密集 NumPy 数组。向构造器LabelBinarizer传递sparse_output=True，就可以得到一个稀疏矩阵。</p>
<blockquote>
<p>译注:</p>
<p>在原书中使用<code>LabelBinarizer</code>的方式也是错误的，该类也应用于标签列的转换。正确做法是使用sklearn即将提供的<code>CategoricalEncoder</code>类。如果在你阅读此文时sklearn中尚未提供此类，用如下方式代替：（来自<a href="https://github.com/scikit-learn/scikit-learn/pull/9151" target="_blank" rel="noopener">Pull Request #9151）</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">#from sklearn.preprocessing import CategoricalEncoder # in future versions of Scikit-Learn</span></span><br><span class="line">&gt;</span><br><span class="line">&gt; cat_encoder = CategoricalEncoder()</span><br><span class="line">&gt; housing_cat_reshaped = housing_cat.values.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">&gt; housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)</span><br><span class="line">&gt; housing_cat_1hot</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
</blockquote>
<h2 id="寻找良好特征（的特点）"><a href="#寻找良好特征（的特点）" class="headerlink" title="寻找良好特征（的特点）"></a>寻找良好特征（的特点）</h2><p>当得到特征之后，还是要进行筛选的，因为有的特征没有参考价值，就像我们的在做合成特征的时候，正常的特征数据是人均几间房间，而有的人是几十间，这明显没有参考价值<br>良好特征的几点原则</p>
<ul>
<li><p>避免很少使用的离散特征值：如果只是出现了一两次的特征几乎是没有意义的</p>
</li>
<li><p>最好具有清晰明确的含义：特征的含义不仅仅是让机器学习的模型学习的，人也要知道其具体的含义，不然不利于分析数据（最好将数值很大的秒转换为天数，或者年，让人看起来直观一些）</p>
</li>
<li><p>将“神奇”的值与实际数据混为一谈：有些特征中会出现一些”神奇的数据”，当然这些数据并不是很少的特征，而是超出范围的异常值，比如特征应该是介于0——1之间的，但是因为这个数据是空缺的，而采用的默认数值-1，那么这样的数值就是”神奇”，解决办法是，将该特征转换为两个特征：</p>
<ul>
<li>一个特征只存储质正常范围的值，不含神奇值。</li>
<li>一个特征存储布尔值，表示的信息为是否为空</li>
</ul>
</li>
<li><p>考虑上游不稳定性：由经验可知，特征的定义不应随时间发生变化，代表城市名称的话，那么特征值始终都该是城市的名称，但是有的时候，上游模型将特征值处理完毕后，返还给下游模型的却变成了数值，这样是不好的，因为这种表示在未来运行其他模型时可能轻易发生变化，那么特征就乱套了</p>
<p>​</p>
</li>
</ul>
<h3 id="可视化数据寻找规律："><a href="#可视化数据寻找规律：" class="headerlink" title="可视化数据寻找规律："></a>可视化数据寻找规律：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">housing.plot(kind=<span class="string">"scatter"</span>, x=<span class="string">"longitude"</span>, y=<span class="string">"latitude"</span>, alpha=<span class="number">0.4</span>,</span><br><span class="line">    s=housing[<span class="string">"population"</span>]/<span class="number">100</span>, label=<span class="string">"population"</span>,</span><br><span class="line">    c=<span class="string">"median_house_value"</span>, cmap=plt.get_cmap(<span class="string">"jet"</span>), colorbar=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p>每个圈的半径表示街区的人口（选项s），颜色代表价格（选项c）。我们用预先定义的名为jet的颜色图（选项cmap），它的范围是从蓝色（低价）到红色（高价）：</p>
<p><img src="/2018/07/09/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（一）：一个完整的程序/02.png" alt></p>
<h3 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corr_matrix = housing.corr()</span><br></pre></td></tr></table></figure>
<h3 id="Pandas-的scatter-matrix函数"><a href="#Pandas-的scatter-matrix函数" class="headerlink" title="Pandas 的scatter_matrix函数"></a>Pandas 的scatter_matrix函数</h3><p>另一种检测属性间相关系数的方法是使用 Pandas 的scatter_matrix函数,它能画出每个数值属性对每个其它数值属性的图。因为现在共有 11 个数值属性，你可以得到11 ** 2 = 121张图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas.tools.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"></span><br><span class="line">attributes = [<span class="string">"median_house_value"</span>, <span class="string">"median_income"</span>, <span class="string">"total_rooms"</span>,</span><br><span class="line">              <span class="string">"housing_median_age"</span>]</span><br><span class="line">scatter_matrix(housing[attributes], figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br></pre></td></tr></table></figure>
<p>得到两个属性的散点图</p>
<p><img src="/2018/07/09/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（一）：一个完整的程序/03.png" alt></p>
<h2 id="清查数据"><a href="#清查数据" class="headerlink" title="清查数据"></a>清查数据</h2><p>截至目前，我们假定用于训练和测试的所有数据都是值得信赖的。在现实生活中，数据集中的很多样本是不可靠的，原因有以下一种或多种：</p>
<ul>
<li><strong>遗漏值。</strong> 例如，有人忘记为某个房屋的年龄输入值。(值会为-1，所以要分为两个特征，忘了的看上面)</li>
<li><strong>重复样本。</strong> 例如，服务器错误地将同一条记录上传了两次。</li>
<li><strong>不良标签。</strong> 例如，有人错误地将一颗橡树的图片标记为枫树。</li>
<li><strong>不良特征值。</strong> 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。</li>
</ul>
<p>一旦检测到存在这些问题，通常需要将相应样本从数据集中移除，从而“修正”不良样本。要检测遗漏值或重复样本，可以编写一个简单的程序。检测不良特征值或标签可能会比较棘手，可采用可视化数据的方法。</p>
<p><strong>对于处理特征丢失的问题</strong>。前面，你应该注意到了属性total_bedrooms有一些缺失值。有三个解决选项：</p>
<ul>
<li>去掉对应的街区；（数据大可用）</li>
<li>去掉整个属性；</li>
<li>进行赋值（0、平均值、中位数等等）。</li>
</ul>
<p>用DataFrame的dropna()，drop()，和fillna()方法，可以方便地实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">housing.dropna(subset=[<span class="string">"total_bedrooms"</span>])    <span class="comment"># 选项1</span></span><br><span class="line">housing.drop(<span class="string">"total_bedrooms"</span>, axis=<span class="number">1</span>)       <span class="comment"># 选项2    axis=0对行操作，axis=1对列操作</span></span><br><span class="line">median = housing[<span class="string">"total_bedrooms"</span>].median()</span><br><span class="line">housing[<span class="string">"total_bedrooms"</span>].fillna(median)     <span class="comment"># 选项3</span></span><br></pre></td></tr></table></figure>
<p>如果选择选项 3，你需要计算训练集的中位数，用中位数填充训练集的缺失值，<strong>不要忘记保存该中位数</strong>。后面用<strong>测试集</strong>评估系统时，<strong>需要替换测试集中的缺失值</strong>，也可以用来实时替换新数据中的缺失值。</p>
<p>Scikit-Learn 提供了一个方便的类来处理缺失值：Imputer。下面是其使用方法：首先，需要创建一个Imputer实例，指定用该属性的中位数替换它的每个缺失值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"></span><br><span class="line">imputer = Imputer(strategy=<span class="string">"median"</span>)    <span class="comment"># 进行中位数赋值</span></span><br></pre></td></tr></table></figure>
<p><strong>因为只有数值属性才能算出中位数，我们需要创建一份不包括文本属性ocean_proximity的数据副本：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">housing_num = housing.drop(<span class="string">"ocean_proximity"</span>, axis=<span class="number">1</span>) <span class="comment"># 去除ocean_proximity不为数值属性的特征</span></span><br></pre></td></tr></table></figure>
<p>现在，就可以用fit()方法将imputer实例拟合到训练数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">imputer.fit(housing_num)</span><br></pre></td></tr></table></figure>
<p>imputer计算出了每个属性的中位数，并将结果保存在了实例变量statistics_中。只有属性total_bedrooms有缺失值，但是我们<strong>确保一旦系统运行起来，新的数据中没有缺失值</strong>，所以<strong>安全的做法是将imputer应用到每个数值</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>imputer.statistics_    <span class="comment"># 实例变量statistics_和housing_num数值数据得到的中位数是一样的</span></span><br><span class="line">array([ <span class="number">-118.51</span> , <span class="number">34.26</span> , <span class="number">29.</span> , <span class="number">2119.</span> , <span class="number">433.</span> , <span class="number">1164.</span> , <span class="number">408.</span> , <span class="number">3.5414</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_num.median().values</span><br><span class="line">array([ <span class="number">-118.51</span> , <span class="number">34.26</span> , <span class="number">29.</span> , <span class="number">2119.</span> , <span class="number">433.</span> , <span class="number">1164.</span> , <span class="number">408.</span> , <span class="number">3.5414</span>])</span><br></pre></td></tr></table></figure>
<p>现在，你就可以使用这个“训练过的”imputer来对训练集进行转换，通过将缺失值替换为中位数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = imputer.transform(housing_num)</span><br></pre></td></tr></table></figure>
<p>结果是一个普通的 Numpy 数组，包含有转换后的特征。如果你想将其放回到 PandasDataFrame中，也很简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">housing_tr = pd.DataFrame(X, columns=housing_num.columns) <span class="comment"># 得到处理缺失值后的DF数据</span></span><br></pre></td></tr></table></figure>
<h2 id="整理数据："><a href="#整理数据：" class="headerlink" title="整理数据："></a>整理数据：</h2><h3 id="数据缩放"><a href="#数据缩放" class="headerlink" title="数据缩放"></a>数据缩放</h3><p>有两种常见的方法可以让所有的属性有相同的量度：<strong>线性函数归一化（Min-Max scaling）和标准化（standardization</strong>）。Scikit-Learn 提供了一个转换器MinMaxScaler来实现这个功能。它有一个超参数feature_range，可以让你改变范围，如果不希望范围是 0 到 1；Scikit-Learn 提供了一个转换器StandardScaler来进行标准化</p>
<p>min-max方式,对应的方法为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MinMaxScaler(self, feature_range=(<span class="number">0</span>, <span class="number">1</span>), copy=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>standardization 标准化数据,对应的方法为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StandardScaler(self, copy=<span class="literal">True</span>, with_mean=<span class="literal">True</span>, with_std=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>警告：与所有的转换一样，缩放器只能向训练集拟合，而不是向完整的数据集（包括测试集）。只有这样，你才能用缩放器转换训练集和测试集（和新数据）。</p>
</blockquote>
<h3 id="处理极端离群值"><a href="#处理极端离群值" class="headerlink" title="处理极端离群值"></a>处理极端离群值</h3><p>还是举加利福尼亚州住房数据集中的人均住房数的例子，有的极端值达到了50<br>对于这些极端值其实很好处理，无非几个办法</p>
<ul>
<li><strong>对数缩放</strong>  </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roomsPerPerson = log((totalRooms / population) + 1)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>特征值限制到 某个上限或者下限</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roomsPerPerson = min(totalRooms / population, <span class="number">4</span>)	<span class="comment"># 大于4.0的取4.0</span></span><br></pre></td></tr></table></figure>
<h3 id="分箱"><a href="#分箱" class="headerlink" title="分箱"></a>分箱</h3><p><strong>分箱</strong>其实是一个形象化的说法，就是把数据分开来，装在一个个箱子里，这样一个箱子里的数据就是一家人了。<br>那有什么用呢？下面就举个栗子！</p>
<p><img src="/2018/07/09/Sklearn 与 TensorFlow 机器学习实用指南/Sklearn-与-TensorFlow-机器学习实用指南（一）：一个完整的程序/04.png" alt></p>
<p>在数据集中，<code>latitude</code> 是一个浮点值。不过，在我们的模型中将 <code>latitude</code> 表示为浮点特征没有意义。这是因为纬度和房屋价值之间不存在线性关系。例如，纬度 35 处的房屋并不比纬度 34 处的房屋贵 35/34（或更便宜）。但是，纬度或许能很好地预测房屋价值。</p>
<p>我们现在拥有 11 个不同的布尔值特征（<code>LatitudeBin1</code>、<code>LatitudeBin2</code>、…、<code>LatitudeBin11</code>），而不是一个浮点特征。拥有 11 个不同的特征有点不方便，因此我们将它们统一成一个 11 元素矢量。这样做之后，我们可以将纬度 37.4 表示为：</p>
<p>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</p>
<p>分箱之后，我们的模型现在可以为每个纬度学习完全不同的权重。（是不是觉得有点像独热编码，没错，就是的）</p>
<blockquote>
<p>为了简单起见，我们在纬度样本中使用整数作为分箱边界。如果我们需要更精细的解决方案，我们可以每隔 1/10 个纬度拆分一次分箱边界。添加更多箱可让模型从纬度 37.4 处学习和维度 37.5 处不一样的行为，但前提是每 1/10 个纬度均有充足的样本可供学习。</p>
<p>另一种方法是按<a href="https://wikipedia.org/wiki/Quantile" target="_blank" rel="noopener">分位数</a>分箱，这种方法可以确保每个桶内的样本数量是相等的。按分位数分箱完全无需担心离群值。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">分桶也称为分箱。</span></span><br><span class="line"><span class="string">例如，我们可以将 population 分为以下 3 个分桶：</span></span><br><span class="line"><span class="string">bucket_0 (&lt; 5000)：对应于人口分布较少的街区</span></span><br><span class="line"><span class="string">bucket_1 (5000 - 25000)：对应于人口分布适中的街区</span></span><br><span class="line"><span class="string">bucket_2 (&gt; 25000)：对应于人口分布较多的街区</span></span><br><span class="line"><span class="string">根据前面的分桶定义，以下 population 矢量：</span></span><br><span class="line"><span class="string">[[10001], [42004], [2500], [18000]]</span></span><br><span class="line"><span class="string">将变成以下经过分桶的特征矢量：</span></span><br><span class="line"><span class="string">[[1], [2], [0], [1]]</span></span><br><span class="line"><span class="string">这些特征值现在是分桶索引。请注意，这些索引被视为离散特征。通常情况下，这些特征将被进一步转换为上述独热表示法，但这是以透明方式实现的。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">要为分桶特征定义特征列，我们可以使用 bucketized_column（而不是使用 numeric_column），该列将数字列作为输入，并使用 boundardies 参数中指定的分桶边界将其转换为分桶特征。以下代码为 households 和 longitude 定义了分桶特征列；get_quantile_based_boundaries 函数会根据分位数计算边界，以便每个分桶包含相同数量的元素</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_quantile_based_boundaries</span><span class="params">(feature_values, num_buckets)</span>:</span></span><br><span class="line">    boundaries = np.arange(<span class="number">1.0</span>, num_buckets) / num_buckets</span><br><span class="line">    quantiles = feature_values.quantile(boundaries)</span><br><span class="line">    <span class="keyword">return</span> [quantiles[q] <span class="keyword">for</span> q <span class="keyword">in</span> quantiles.keys()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Divide households into 7 buckets.</span></span><br><span class="line">households = tf.feature_column.numeric_column(<span class="string">"households"</span>)		<span class="comment"># 定义数值特征</span></span><br><span class="line"><span class="comment"># 分桶特征bucketized_column第一个参数用数字列 numeric_column得到的households，第二个参数用上面get_quantile_based_boundaries方法得到的分桶数据，返回的bucketized_households为可使用的分桶特征</span></span><br><span class="line">bucketized_households = tf.feature_column.bucketized_column(       </span><br><span class="line">    households,boundaries=get_quantile_based_boundaries(california_housing_dataframe[<span class="string">"households"</span>], <span class="number">7</span>))</span><br></pre></td></tr></table></figure>
<h2 id="自定义转换器"><a href="#自定义转换器" class="headerlink" title="自定义转换器"></a>自定义转换器</h2><p>尽管 Scikit-Learn 提供了许多有用的转换器，你还是需要自己动手写转换器执行任务，比如自定义的清理操作，或属性组合。<strong>你需要让自制的转换器与 Scikit-Learn 组件（比如流水线）无缝衔接工作，因为 Scikit-Learn 是依赖鸭子类型的（而不是继承，忽略对象，只要行为像就行），你所需要做的是创建一个类并执行三个方法：fit()（返回self），transform()，和fit_transform()</strong>。<strong>通过添加TransformerMixin作为基类，可以很容易地得到最后一个。另外，如果你添加BaseEstimator作为基类（且构造器中避免使用args和kargs</strong>），<strong>你就能得到两个额外的方法（get_params()和set_params()），二者可以方便地进行超参数自动微调</strong>。例如，一个小转换器类添加了上面讨论的属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加一个特征组合的装换器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line">rooms_ix, bedrooms_ix, population_ix, household_ix = <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的示例没有定义fit_transform()，可能是因为fit()没有做任何动作（我猜的</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CombinedAttributesAdder</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, add_bedrooms_per_room = True)</span>:</span> <span class="comment"># no *args or **kargs</span></span><br><span class="line">        self.add_bedrooms_per_room = add_bedrooms_per_room</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self  <span class="comment"># nothing else to do</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]  <span class="comment"># X[:,3]表示的是第4列所有数据</span></span><br><span class="line">        population_per_household = X[:, population_ix] / X[:, household_ix]</span><br><span class="line">        <span class="keyword">if</span> self.add_bedrooms_per_room:</span><br><span class="line">            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]</span><br><span class="line">            <span class="keyword">return</span> np.c_[X, rooms_per_household, population_per_household, <span class="comment"># np.c_表示的是拼接数组。</span></span><br><span class="line">                         bedrooms_per_room]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.c_[X, rooms_per_household, population_per_household]</span><br><span class="line"></span><br><span class="line">attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=<span class="literal">False</span>)</span><br><span class="line">housing_extra_attribs = attr_adder.transform(housing.values)    <span class="comment"># 返回一个加入新特征的数据</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，转换器有一个超参数add_bedrooms_per_room，默认设为True（提供一个合理的默认值很有帮助）。这个超参数可以让你方便地发现添加了这个属性是否对机器学习算法有帮助。更一般地，你可以为每个不能完全确保的数据准备步骤添加一个超参数。数据准备步骤越自动化，可以自动化的操作组合就越多，越容易发现更好用的组合（并能节省大量时间）。</p>
<p><strong>另外sklearn是不能直接处理DataFrames的，那么我们需要自定义一个处理的方法将之转化为numpy类型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataFrameSelector</span><span class="params">(BaseEstimator,TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,attribute_names)</span>:</span> <span class="comment">#可以为列表</span></span><br><span class="line">        self.attribute_names = attribute_names</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,X,y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X[self.attribute_names].values <span class="comment">#返回的为numpy array</span></span><br></pre></td></tr></table></figure>
<h2 id="转换流水线"><a href="#转换流水线" class="headerlink" title="转换流水线"></a>转换流水线</h2><p>目前在数据预处理阶段，<strong>我们需要对缺失值进行处理、特征组合和特征缩放。每一步的执行都有着先后顺序，存在许多数据转换步骤，需要按一定的顺序执行。</strong>sklearn提供了Pipeline帮助顺序完成转换幸运的是，Scikit-Learn 提供了类Pipeline，来进行这一系列的转换。下面是一个数值属性的小流水线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">num_pipeline = Pipeline([</span><br><span class="line">        (<span class="string">'imputer'</span>, Imputer(strategy=<span class="string">"median"</span>)),    <span class="comment"># 处理缺失值</span></span><br><span class="line">        (<span class="string">'attribs_adder'</span>, CombinedAttributesAdder()),    <span class="comment"># 特征组合</span></span><br><span class="line">        (<span class="string">'std_scaler'</span>, StandardScaler()),    <span class="comment"># 特征缩放</span></span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">housing_num_tr = num_pipeline.fit_transform(housing_num)</span><br></pre></td></tr></table></figure>
<p><strong>Pipeline构造器需要一个定义步骤顺序的名字/估计器对的列表。除了最后一个估计器，其余都要是转换器（即，它们都要有fit_transform()方法）</strong>。名字可以随意起。</p>
<p><strong>当你调用流水线的fit()方法，就会对所有转换器顺序调用fit_transform()方法，将每次调用的输出作为参数传递给下一个调用，一直到最后一个估计器，它只执行fit()方法。</strong></p>
<p>估计器（Estimator）：很多时候可以直接理解成分类器，主要包含两个函数：fit()和predict()<br>转换器（Transformer）：转换器用于数据预处理和数据转换，主要是三个方法：fit（）,transform()和fit_transform()</p>
<p>最后的估计器是一个StandardScaler，它是一个转换器，因此这个流水线有一个transform()方法，可以顺序对数据做所有转换（它还有一个fit_transform方法可以使用，就不必先调用fit()再进行transform()）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_attribs = list(housing_num) <span class="comment"># 返回的为列名[col1,col2,....]</span></span><br><span class="line">cat_attribs = [<span class="string">"ocean_proximity"</span>]</span><br><span class="line"></span><br><span class="line">num_pipeline = Pipeline([ <span class="comment"># 数值类型</span></span><br><span class="line">        (<span class="string">'selector'</span>, DataFrameSelector(num_attribs)),	<span class="comment"># DataFrames转为numpy array</span></span><br><span class="line">        (<span class="string">'imputer'</span>, Imputer(strategy=<span class="string">"median"</span>)),	<span class="comment"># 缺失值处理</span></span><br><span class="line">        (<span class="string">'attribs_adder'</span>, CombinedAttributesAdder()),	<span class="comment"># 特征组合</span></span><br><span class="line">        (<span class="string">'std_scaler'</span>, StandardScaler()),	<span class="comment"># 缩放</span></span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">cat_pipeline = Pipeline([ <span class="comment"># 标签类型</span></span><br><span class="line">        (<span class="string">'selector'</span>, DataFrameSelector(cat_attribs)), 	<span class="comment"># DataFrames转为numpy array</span></span><br><span class="line">        (<span class="string">'cat_encoder'</span>, CategoricalEncoder(encoding=<span class="string">"onehot-dense"</span>)),	</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>
<p>上面定义的为分别处理数值类型和标签类型的转换流程，housing_num为DataFrame类型，list(DataFrame)的结果返回的为列名字。上面着两个流程还可以再整合一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> FeatureUnion</span><br><span class="line">full_pipeline = FeatureUnion(transformer_list=[</span><br><span class="line">        (<span class="string">"num_pipeline"</span>, num_pipeline),</span><br><span class="line">        (<span class="string">"cat_pipeline"</span>, cat_pipeline),</span><br><span class="line">    ])</span><br><span class="line">housing_prepared = full_pipeline.fit_transform(housing) <span class="comment"># 最终的结果</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_prepared</span><br><span class="line">array([[ <span class="number">0.73225807</span>, <span class="number">-0.67331551</span>,  <span class="number">0.58426443</span>, ...,  <span class="number">0.</span>        ,</span><br><span class="line">         <span class="number">0.</span>        ,  <span class="number">0.</span>        ],</span><br><span class="line">       [<span class="number">-0.99102923</span>,  <span class="number">1.63234656</span>, <span class="number">-0.92655887</span>, ...,  <span class="number">0.</span>        ,</span><br><span class="line">         <span class="number">0.</span>        ,  <span class="number">0.</span>        ],</span><br><span class="line">       [...]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_prepared.shape</span><br><span class="line">(<span class="number">16513</span>, <span class="number">17</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>译注:</p>
<p>如果你在上面代码中的<code>cat_pipeline</code>流水线使用<code>LabelBinarizer</code>转换器会导致执行错误，解决方案是用上文提到的<code>CategoricalEncoder</code>转换器来代替：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; cat_pipeline = Pipeline([</span><br><span class="line">&gt;         (<span class="string">'selector'</span>, DataFrameSelector(cat_attribs)),</span><br><span class="line">&gt;         (<span class="string">'cat_encoder'</span>, CategoricalEncoder(encoding=<span class="string">"onehot-dense"</span>)),</span><br><span class="line">&gt;     ])</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
</blockquote>
<p><strong>每个子流水线都以一个选择转换器开始：通过选择对应的属性（数值或分类）、丢弃其它的，来转换数据，并将输出DataFrame转变成一个 NumPy 数组。Scikit-Learn 没有工具来处理 PandasDataFrame，因此我们需要写一个简单的自定义转换器来做这项工作：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataFrameSelector</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, attribute_names)</span>:</span></span><br><span class="line">        self.attribute_names = attribute_names</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X[self.attribute_names].values</span><br></pre></td></tr></table></figure>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>我们先来训练一个线性回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(housing_prepared, housing_labels)    <span class="comment"># 利用预处理好的数据进行训练模型</span></span><br></pre></td></tr></table></figure>
<p>完毕！你现在就有了一个可用的线性回归模型。用一些训练集中的实例做下验证：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_data = housing.iloc[:<span class="number">5</span>]    <span class="comment"># 前五个作为预测数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_labels = housing_labels.iloc[:<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>some_data_prepared = full_pipeline.transform(some_data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"Predictions:\t"</span>, lin_reg.predict(some_data_prepared))    <span class="comment"># 预测结果</span></span><br><span class="line">Predictions:     [ <span class="number">303104.</span>   <span class="number">44800.</span>  <span class="number">308928.</span>  <span class="number">294208.</span>  <span class="number">368704.</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">"Labels:\t\t"</span>, list(some_labels))</span><br><span class="line">Labels:         [<span class="number">359400.0</span>, <span class="number">69700.0</span>, <span class="number">302100.0</span>, <span class="number">301300.0</span>, <span class="number">351900.0</span>]    <span class="comment"># 实际结果</span></span><br></pre></td></tr></table></figure>
<p>行的通，尽管预测并不怎么准确（比如，第二个预测偏离了 50%！）。让我们使用 Scikit-Learn 的mean_squared_error函数，用全部训练集来计算下这个回归模型的 RMSE：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_predictions = lin_reg.predict(housing_prepared)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_mse = mean_squared_error(housing_labels, housing_predictions)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_rmse = np.sqrt(lin_mse)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_rmse</span><br><span class="line"><span class="number">68628.413493824875</span></span><br></pre></td></tr></table></figure>
<p>OK，有总比没有强，但显然结果并不好，这是一个模型欠拟合训练数据的例子。当这种情况发生时，意味着特征没有提供足够多的信息来做出一个好的预测，或者模型并不强大，修复欠拟合的主要方法是选择一个更强大的模型，给训练算法提供更好的特征，或去掉模型上的限制，你可以尝试添加更多特征（比如，人口的对数值），但是首先让我们尝试一个更为复杂的模型，看看效果。训练一个决策树模型DecisionTreeRegressor。这是一个强大的模型，可以发现数据中复杂的非线性关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">tree_reg = DecisionTreeRegressor()</span><br><span class="line">tree_reg.fit(housing_prepared, housing_labels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>housing_predictions = tree_reg.predict(housing_prepared)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tree_mse = mean_squared_error(housing_labels, housing_predictions)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tree_rmse = np.sqrt(tree_mse)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tree_rmse</span><br><span class="line"><span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>等一下，发生了什么？没有误差？这个模型可能是绝对完美的吗？当然，更大可能性是这个模型严重过拟合数据。如何确定呢？如前所述，直到你准备运行一个具备足够信心的模型，都不要碰测试集，因此你需要使用训练集的部分数据来做训练，用一部分来做模型验证。</p>
<h2 id="用交叉验证做更佳的评估"><a href="#用交叉验证做更佳的评估" class="headerlink" title="用交叉验证做更佳的评估"></a>用交叉验证做更佳的评估</h2><p>使用 Scikit-Learn 的交叉验证功能。下面的代码采用了 <strong>K 折交叉验证（K-fold cross-validation）：它随机地将训练集分成十个不同的子集，成为“折”，然后训练评估决策树模型 10 次，每次选一个不用的折来做评估，用其它 9 个来做训练。结果是一个包含 10 个评分的数组：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">scores = cross_val_score(tree_reg, housing_prepared, housing_labels,</span><br><span class="line">                         scoring=<span class="string">"neg_mean_squared_error"</span>, cv=<span class="number">10</span>)</span><br><span class="line">rmse_scores = np.sqrt(-scores)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>警告：Scikit-Learn 交叉验证功能期望的是效用函数（越大越好）而不是损失函数（越低越好），因此得分函数实际上与 MSE 相反（即负值），这就是为什么前面的代码在计算平方根之前先计算-scores。</p>
</blockquote>
<p>来看下结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">display_scores</span><span class="params">(scores)</span>:</span></span><br><span class="line"><span class="meta">... </span>    print(<span class="string">"Scores:"</span>, scores)</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">"Mean:"</span>, scores.mean())</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">"Standard deviation:"</span>, scores.std())</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>display_scores(tree_rmse_scores)</span><br><span class="line">Scores: [ <span class="number">74678.4916885</span>   <span class="number">64766.2398337</span>   <span class="number">69632.86942005</span>  <span class="number">69166.67693232</span></span><br><span class="line">          <span class="number">71486.76507766</span>  <span class="number">73321.65695983</span>  <span class="number">71860.04741226</span>  <span class="number">71086.32691692</span></span><br><span class="line">          <span class="number">76934.2726093</span>   <span class="number">69060.93319262</span>]</span><br><span class="line">Mean: <span class="number">71199.4280043</span></span><br><span class="line">Standard deviation: <span class="number">3202.70522793</span></span><br></pre></td></tr></table></figure>
<p>现在决策树就不像前面看起来那么好了。实际上，它看起来比线性回归模型还糟！注意到交叉验证不仅可以让你得到模型性能的评估，还能测量评估的准确性（即，它的标准差）。决策树的评分大约是 71200，通常波动有 ±3200。如果只有一个验证集，就得不到这些信息。但是交叉验证的代价是训练了模型多次，不可能总是这样。</p>
<p>让我们计算下线性回归模型的的相同分数，以做确保：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,</span><br><span class="line"><span class="meta">... </span>                             scoring=<span class="string">"neg_mean_squared_error"</span>, cv=<span class="number">10</span>)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_rmse_scores = np.sqrt(-lin_scores)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>display_scores(lin_rmse_scores)</span><br><span class="line">Scores: [ <span class="number">70423.5893262</span>   <span class="number">65804.84913139</span>  <span class="number">66620.84314068</span>  <span class="number">72510.11362141</span></span><br><span class="line">          <span class="number">66414.74423281</span>  <span class="number">71958.89083606</span>  <span class="number">67624.90198297</span>  <span class="number">67825.36117664</span></span><br><span class="line">          <span class="number">72512.36533141</span>  <span class="number">68028.11688067</span>]</span><br><span class="line">Mean: <span class="number">68972.377566</span></span><br><span class="line">Standard deviation: <span class="number">2493.98819069</span></span><br></pre></td></tr></table></figure>
<p>判断没错：决策树模型过拟合很严重，它的性能比线性回归模型还差</p>
<p>现在再尝试最后一个模型：RandomForestRegressor（随机森林），随机森林是通过用特征的随机子集训练许多决策树。在其它多个模型之上建立模型成为集成学习（Ensemble Learning），它是推进 ML 算法的一种好方法。我们会跳过大部分的代码，因为代码本质上和其它模型一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_reg = RandomForestRegressor()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_reg.fit(housing_prepared, housing_labels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[...]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>forest_rmse</span><br><span class="line"><span class="number">22542.396440343684</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>display_scores(forest_rmse_scores)</span><br><span class="line">Scores: [ <span class="number">53789.2879722</span>   <span class="number">50256.19806622</span>  <span class="number">52521.55342602</span>  <span class="number">53237.44937943</span></span><br><span class="line">          <span class="number">52428.82176158</span>  <span class="number">55854.61222549</span>  <span class="number">52158.02291609</span>  <span class="number">50093.66125649</span></span><br><span class="line">          <span class="number">53240.80406125</span>  <span class="number">52761.50852822</span>]</span><br><span class="line">Mean: <span class="number">52634.1919593</span></span><br><span class="line">Standard deviation: <span class="number">1576.20472269</span></span><br></pre></td></tr></table></figure>
<p>现在好多了：随机森林看起来很有希望。但是，训练集的评分仍然比验证集的评分低很多。解决过拟合可以通过简化模型，给模型加限制（即，正则化），或用更多的训练数据。在深入随机森林之前，你应该尝试下机器学习算法的其它类型模型（不同核心的支持向量机，神经网络，等等），不要在调节超参数上花费太多时间。目标是列出一个可能模型的列表（两到五个）。</p>
<blockquote>
<p>提示：你要保存每个试验过的模型，以便后续可以再用。要确保有超参数和训练参数，以及交叉验证评分，和实际的预测值。这可以让你比较不同类型模型的评分，还可以比较误差种类。你可以用 Python 的模块pickle，非常方便地保存 Scikit-Learn 模型，或使用sklearn.externals.joblib，后者序列化大 NumPy 数组更有效率：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line">joblib.dump(my_model, <span class="string">"my_model.pkl"</span>)</span><br><span class="line"><span class="comment"># 然后</span></span><br><span class="line">my_model_loaded = joblib.load(<span class="string">"my_model.pkl"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型微调"><a href="#模型微调" class="headerlink" title="模型微调"></a>模型微调</h2><p><strong>网格搜索</strong>：使用 Scikit-Learn 的GridSearchCV来做这项搜索工作。<strong>你所需要做的是告诉GridSearchCV要试验有哪些超参数，要试验什么值，GridSearchCV就能用交叉验证试验所有可能超参数值的组合</strong>。例如，下面的代码搜索了RandomForestRegressor超参数值的最佳组合（很费时间）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = [</span><br><span class="line">    &#123;<span class="string">'n_estimators'</span>: [<span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>], <span class="string">'max_features'</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>]&#125;,</span><br><span class="line">    &#123;<span class="string">'bootstrap'</span>: [<span class="literal">False</span>], <span class="string">'n_estimators'</span>: [<span class="number">3</span>, <span class="number">10</span>], <span class="string">'max_features'</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;,</span><br><span class="line">  ]</span><br><span class="line"></span><br><span class="line">forest_reg = RandomForestRegressor()</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(forest_reg, param_grid, cv=<span class="number">5</span>,</span><br><span class="line">                           scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br><span class="line"></span><br><span class="line">grid_search.fit(housing_prepared, housing_labels)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>当你不能确定超参数该有什么值，一个简单的方法是尝试连续的 10 的幂（如果想要一个粒度更小的搜寻，可以用更小的数，就像在这个例子中对超参数n_estimators做的）。</p>
</blockquote>
<p>param_grid告诉 Scikit-Learn 首先评估所有的列在第一个dict中的n_estimators和max_features的3 × 4 = 12种组合（不用担心这些超参数的含义，会在第 7 章中解释）。然后尝试第二个dict中超参数的2 × 3 = 6种组合，这次会将超参数bootstrap设为False而不是True（后者是该超参数的默认值）。完成后，你就能获得参数的最佳组合，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>grid_search.best_params_</span><br><span class="line">&#123;<span class="string">'max_features'</span>: <span class="number">6</span>, <span class="string">'n_estimators'</span>: <span class="number">30</span>&#125;</span><br></pre></td></tr></table></figure>
<p>你还能直接得到最佳的估计器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>grid_search.best_estimator_</span><br><span class="line">RandomForestRegressor(bootstrap=<span class="literal">True</span>, criterion=<span class="string">'mse'</span>, max_depth=<span class="literal">None</span>,</span><br><span class="line">           max_features=<span class="number">6</span>, max_leaf_nodes=<span class="literal">None</span>, min_samples_leaf=<span class="number">1</span>,</span><br><span class="line">           min_samples_split=<span class="number">2</span>, min_weight_fraction_leaf=<span class="number">0.0</span>,</span><br><span class="line">           n_estimators=<span class="number">30</span>, n_jobs=<span class="number">1</span>, oob_score=<span class="literal">False</span>, random_state=<span class="literal">None</span>,</span><br><span class="line">           verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>当然，也可以得到评估得分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cvres = grid_search.cv_results_</span><br><span class="line"><span class="meta">... </span><span class="keyword">for</span> mean_score, params <span class="keyword">in</span> zip(cvres[<span class="string">"mean_test_score"</span>], cvres[<span class="string">"params"</span>]):</span><br><span class="line"><span class="meta">... </span>    print(np.sqrt(-mean_score), params)</span><br><span class="line">...</span><br><span class="line"><span class="number">64912.0351358</span> &#123;<span class="string">'max_features'</span>: <span class="number">2</span>, <span class="string">'n_estimators'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="number">55535.2786524</span> &#123;<span class="string">'max_features'</span>: <span class="number">2</span>, <span class="string">'n_estimators'</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">52940.2696165</span> &#123;<span class="string">'max_features'</span>: <span class="number">2</span>, <span class="string">'n_estimators'</span>: <span class="number">30</span>&#125;</span><br><span class="line"><span class="number">60384.0908354</span> &#123;<span class="string">'max_features'</span>: <span class="number">4</span>, <span class="string">'n_estimators'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="number">52709.9199934</span> &#123;<span class="string">'max_features'</span>: <span class="number">4</span>, <span class="string">'n_estimators'</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">50503.5985321</span> &#123;<span class="string">'max_features'</span>: <span class="number">4</span>, <span class="string">'n_estimators'</span>: <span class="number">30</span>&#125;</span><br><span class="line"><span class="number">59058.1153485</span> &#123;<span class="string">'max_features'</span>: <span class="number">6</span>, <span class="string">'n_estimators'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="number">52172.0292957</span> &#123;<span class="string">'max_features'</span>: <span class="number">6</span>, <span class="string">'n_estimators'</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">49958.9555932</span> &#123;<span class="string">'max_features'</span>: <span class="number">6</span>, <span class="string">'n_estimators'</span>: <span class="number">30</span>&#125;</span><br><span class="line"><span class="number">59122.260006</span> &#123;<span class="string">'max_features'</span>: <span class="number">8</span>, <span class="string">'n_estimators'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="number">52441.5896087</span> &#123;<span class="string">'max_features'</span>: <span class="number">8</span>, <span class="string">'n_estimators'</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">50041.4899416</span> &#123;<span class="string">'max_features'</span>: <span class="number">8</span>, <span class="string">'n_estimators'</span>: <span class="number">30</span>&#125;</span><br><span class="line"><span class="number">62371.1221202</span> &#123;<span class="string">'bootstrap'</span>: <span class="literal">False</span>, <span class="string">'max_features'</span>: <span class="number">2</span>, <span class="string">'n_estimators'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="number">54572.2557534</span> &#123;<span class="string">'bootstrap'</span>: <span class="literal">False</span>, <span class="string">'max_features'</span>: <span class="number">2</span>, <span class="string">'n_estimators'</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">59634.0533132</span> &#123;<span class="string">'bootstrap'</span>: <span class="literal">False</span>, <span class="string">'max_features'</span>: <span class="number">3</span>, <span class="string">'n_estimators'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="number">52456.0883904</span> &#123;<span class="string">'bootstrap'</span>: <span class="literal">False</span>, <span class="string">'max_features'</span>: <span class="number">3</span>, <span class="string">'n_estimators'</span>: <span class="number">10</span>&#125;</span><br><span class="line"><span class="number">58825.665239</span> &#123;<span class="string">'bootstrap'</span>: <span class="literal">False</span>, <span class="string">'max_features'</span>: <span class="number">4</span>, <span class="string">'n_estimators'</span>: <span class="number">3</span>&#125;</span><br><span class="line"><span class="number">52012.9945396</span> &#123;<span class="string">'bootstrap'</span>: <span class="literal">False</span>, <span class="string">'max_features'</span>: <span class="number">4</span>, <span class="string">'n_estimators'</span>: <span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure>
<p>在这个例子中，我们通过设定超参数max_features为 6，n_estimators为 30，得到了最佳方案。对这个组合，RMSE 的值是 49959，这比之前使用默认的超参数的值（52634）要稍微好一些。祝贺你，你成功地微调了最佳模型！</p>
<p><strong>随机搜索：</strong>当探索相对较少的组合时，就像前面的例子，网格搜索还可以。但是当超参数的搜索空间很大时，最好使用RandomizedSearchCV。这个类的使用方法和类GridSearchCV很相似，但它不是尝试所有可能的组合，而是通过选择每个超参数的一个随机值的特定数量的随机组合。这个方法有两个优点：</p>
<ul>
<li>如果你让随机搜索运行，比如 1000 次，它会探索每个超参数的 1000 个不同的值（而不是像网格搜索那样，只搜索每个超参数的几个值）</li>
<li>你可以方便地通过设定搜索次数，控制超参数搜索的计算量。</li>
</ul>
<h2 id="分析最佳模型和它们的误差"><a href="#分析最佳模型和它们的误差" class="headerlink" title="分析最佳模型和它们的误差"></a>分析最佳模型和它们的误差</h2><p>通过<strong>分析最佳模型</strong>，常常可以获得对问题更深的了解。比如，RandomForestRegressor可以指出<strong>每个属性对于做出准确预测的相对重要性</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; feature_importances = grid_search.best_estimator_.feature_importances_</span><br><span class="line">&gt;&gt;&gt; feature_importances</span><br><span class="line">array([  7.14156423e-02,   6.76139189e-02,   4.44260894e-02,</span><br><span class="line">         1.66308583e-02,   1.66076861e-02,   1.82402545e-02,</span><br><span class="line">         1.63458761e-02,   3.26497987e-01,   6.04365775e-02,</span><br><span class="line">         1.13055290e-01,   7.79324766e-02,   1.12166442e-02,</span><br><span class="line">         1.53344918e-01,   8.41308969e-05,   2.68483884e-03,</span><br><span class="line">         3.46681181e-03])</span><br></pre></td></tr></table></figure>
<p>将重要性分数和属性名放到一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>extra_attribs = [<span class="string">"rooms_per_hhold"</span>, <span class="string">"pop_per_hhold"</span>, <span class="string">"bedrooms_per_room"</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cat_one_hot_attribs = list(encoder.classes_)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>attributes = num_attribs + extra_attribs + cat_one_hot_attribs</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted(zip(feature_importances,attributes), reverse=<span class="literal">True</span>)</span><br><span class="line">[(<span class="number">0.32649798665134971</span>, <span class="string">'median_income'</span>),</span><br><span class="line"> (<span class="number">0.15334491760305854</span>, <span class="string">'INLAND'</span>),</span><br><span class="line"> (<span class="number">0.11305529021187399</span>, <span class="string">'pop_per_hhold'</span>),</span><br><span class="line"> (<span class="number">0.07793247662544775</span>, <span class="string">'bedrooms_per_room'</span>),</span><br><span class="line"> (<span class="number">0.071415642259275158</span>, <span class="string">'longitude'</span>),</span><br><span class="line"> (<span class="number">0.067613918945568688</span>, <span class="string">'latitude'</span>),</span><br><span class="line"> (<span class="number">0.060436577499703222</span>, <span class="string">'rooms_per_hhold'</span>),</span><br><span class="line"> (<span class="number">0.04442608939578685</span>, <span class="string">'housing_median_age'</span>),</span><br><span class="line"> (<span class="number">0.018240254462909437</span>, <span class="string">'population'</span>),</span><br><span class="line"> (<span class="number">0.01663085833886218</span>, <span class="string">'total_rooms'</span>),</span><br><span class="line"> (<span class="number">0.016607686091288865</span>, <span class="string">'total_bedrooms'</span>),</span><br><span class="line"> (<span class="number">0.016345876147580776</span>, <span class="string">'households'</span>),</span><br><span class="line"> (<span class="number">0.011216644219017424</span>, <span class="string">'&lt;1H OCEAN'</span>),</span><br><span class="line"> (<span class="number">0.0034668118081117387</span>, <span class="string">'NEAR OCEAN'</span>),</span><br><span class="line"> (<span class="number">0.0026848388432755429</span>, <span class="string">'NEAR BAY'</span>),</span><br><span class="line"> (<span class="number">8.4130896890070617e-05</span>, <span class="string">'ISLAND'</span>)]</span><br></pre></td></tr></table></figure>
<p>有了这个信息，你就可以丢弃一些不那么重要的特征（比如，显然只要一个分类ocean_proximity就够了，所以可以丢弃掉其它的）。你还应该看一下系统犯的误差，搞清为什么会有些误差，以及如何改正问题（添加更多的特征，或相反，去掉没有什么信息的特征，清洗异常值等等）。</p>
<h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><p>调节完系统之后，你终于有了一个性能足够好的系统。现在就可以用测试集评估最后的模型了。这个过程没有什么特殊的：从测试集得到预测值和标签，运行full_pipeline转换数据（调用transform()，而不是fit_transform()！），再用测试集评估最终模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">final_model = grid_search.best_estimator_</span><br><span class="line"></span><br><span class="line">X_test = strat_test_set.drop(<span class="string">"median_house_value"</span>, axis=<span class="number">1</span>)</span><br><span class="line">y_test = strat_test_set[<span class="string">"median_house_value"</span>].copy()</span><br><span class="line"></span><br><span class="line">X_test_prepared = full_pipeline.transform(X_test)</span><br><span class="line"></span><br><span class="line">final_predictions = final_model.predict(X_test_prepared)</span><br><span class="line"></span><br><span class="line">final_mse = mean_squared_error(y_test, final_predictions)</span><br><span class="line">final_rmse = np.sqrt(final_mse)   <span class="comment"># =&gt; evaluates to 48,209.6</span></span><br></pre></td></tr></table></figure>
<p>评估结果通常要比交叉验证的效果差一点，如果你之前做过很多超参数微调（因为你的系统在验证集上微调，得到了不错的性能，通常不会在未知的数据集上有同样好的效果）。这个例子不属于这种情况，但是当发生这种情况时，你一定要忍住不要调节超参数，使测试集的效果变好；这样的提升不能推广到新数据上。</p>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/04/python编程进阶/python编程进阶（13）：兼容、缓存、上下文/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/04/python编程进阶/python编程进阶（13）：兼容、缓存、上下文/" itemprop="url">python编程进阶（13）：兼容、缓存、上下文</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-04T09:14:32+08:00">
                2018-07-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python编程进阶/" itemprop="url" rel="index">
                    <span itemprop="name">python编程进阶</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2,523
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  10
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="兼容Python2-和Python3"><a href="#兼容Python2-和Python3" class="headerlink" title="兼容Python2+和Python3+"></a>兼容Python2+和Python3+</h1><p>很多时候你可能希望你开发的程序能够同时兼容Python2+和Python3+。</p>
<p>试想你有一个非常出名的Python模块被很多开发者使用着，但并不是所有人都只使用Python2或者Python3。这时候你有两个办法。第一个办法是开发两个模块，针对Python2一个，针对Python3一个。还有一个办法就是调整你现在的代码使其同时兼容Python2和Python3。</p>
<p>本节中，我将介绍一些技巧，让你的脚本同时兼容Python2和Python3。</p>
<h2 id="Future模块导入"><a href="#Future模块导入" class="headerlink" title="Future模块导入"></a>Future模块导入</h2><p><strong>第一种也是最重要的方法，就是导入<code>__future__</code>模块。它可以帮你在Python2中导入Python3的功能</strong>。这有一组例子：</p>
<p>上下文管理器是Python2.6+引入的新特性，如果你想在Python2.5中使用它可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> with_statement</span><br></pre></td></tr></table></figure>
<p>在Python3中<code>print</code>已经变为一个函数。如果你想在Python2中使用它可以通过<code>__future__</code>导入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line">print(<span class="keyword">print</span>)</span><br><span class="line"><span class="comment"># Output: &lt;built-in function print&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="模块重命名"><a href="#模块重命名" class="headerlink" title="模块重命名"></a>模块重命名</h2><p>首先，告诉我你是如何在你的脚本中导入模块的。大多时候我们会这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> foo </span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="keyword">from</span> foo <span class="keyword">import</span> bar</span><br></pre></td></tr></table></figure>
<p>你知道么，其实你也可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> foo <span class="keyword">as</span> foo</span><br></pre></td></tr></table></figure>
<p>这样做可以起到和上面代码同样的功能，但最重要的是它能让你的脚本同时兼容Python2和Python3。现在我们来看下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> urllib.request <span class="keyword">as</span> urllib_request  <span class="comment"># for Python 3</span></span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">import</span> urllib2 <span class="keyword">as</span> urllib_request  <span class="comment"># for Python 2</span></span><br></pre></td></tr></table></figure>
<h2 id="过期的Python2内置功能"><a href="#过期的Python2内置功能" class="headerlink" title="过期的Python2内置功能"></a>过期的Python2内置功能</h2><p>另一个需要了解的事情就是Python2中有12个内置功能在Python3中已经被移除了。要确保在Python2代码中不要出现这些功能来保证对Python3的兼容。这有一个强制让你放弃12内置功能的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> future.builtins.disabled <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<p>现在，只要你尝试在Python3中使用这些被遗弃的模块时，就会抛出一个<code>NameError</code>异常如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> future.builtins.disabled <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">apply()</span><br><span class="line"><span class="comment"># Output: NameError: obsolete Python 2 builtin apply is disabled</span></span><br></pre></td></tr></table></figure>
<p><strong>标准库向下兼容的外部支持</strong></p>
<p>有一些包在非官方的支持下为Python2提供了Python3的功能。例如，我们有：</p>
<ul>
<li>enum <code>pip install enum34</code></li>
<li>singledispatch <code>pip install singledispatch</code></li>
<li>pathlib <code>pip install pathlib</code></li>
</ul>
<p>想更多了解，在Python文档中有一个<a href="https://docs.python.org/3/howto/pyporting.html" target="_blank" rel="noopener">全面的指南</a>可以帮助你让你的代码同时兼容Python2和Python3。</p>
<h1 id="函数缓存-Function-caching"><a href="#函数缓存-Function-caching" class="headerlink" title="函数缓存 (Function caching)"></a>函数缓存 (Function caching)</h1><p><strong>函数缓存允许我们将一个函数对于给定参数的返回值缓存起来</strong>。当一个I/O密集的函数被频繁<strong>使用相同的参数调用的时候，函数缓存可以节约时间</strong>。在Python 3.2版本以前我们只有写一个自定义的实现。<strong>在Python 3.2以后版本，有个<code>lru_cache</code>的装饰器，允许我们将一个函数的返回值快速地缓存或取消缓存</strong>。</p>
<h2 id="Python-3-2及以后版本"><a href="#Python-3-2及以后版本" class="headerlink" title="Python 3.2及以后版本"></a>Python 3.2及以后版本</h2><p>我们来实现一个斐波那契计算器，并使用<code>lru_cache</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache(maxsize=32)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fib(n<span class="number">-1</span>) + fib(n<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print([fib(n) <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line"><span class="comment"># Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</span></span><br></pre></td></tr></table></figure>
<p>那个<code>maxsize</code>参数是告诉<code>lru_cache</code>，最多缓存最近多少个返回值。</p>
<p>我们也可以轻松地对返回值清空缓存，通过这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fib.cache_clear()</span><br></pre></td></tr></table></figure>
<h1 id="上下文管理器-Context-managers"><a href="#上下文管理器-Context-managers" class="headerlink" title="上下文管理器(Context managers)"></a>上下文管理器(Context managers)</h1><p>上下文管理器允许你在有需要的时候，精确地分配和释放资源。<strong>上下文管理器的常用于一些资源的操作，需要在资源的正确获取与释放相关的操作</strong> ，先看一个例子,我们经常会用到 try … catch … finally 语句确保一些系统资源得以正确释放。如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = open(<span class="string">'somefile'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        print(line)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<p>我们经常用到上面的代码模式，用复用代码的模式来讲，并不够好。于是 with 语句出现了，通过定义一个上下文管理器来封装这个代码块:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'somefile'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        print(line)</span><br></pre></td></tr></table></figure>
<p><strong>使用上下文管理器最广泛的案例就是<code>with</code>语句了</strong>。想象下你有两个需要结对执行的相关操作，然后还要在它们中间放置一段代码。 <strong>上下文管理器就是专门让你做这种事情的。上面这段代码打开了一个文件，往里面写入了一些数据，然后关闭该文件。如果在往文件写数据时发生异常，它也会尝试去关闭文件</strong>。这就是<code>with</code>语句的主要优势，它确保我们的文件会被关闭，而不用关注嵌套代码如何退出。</p>
<p>上下文管理器的一个常见用例，是资源的加锁和解锁，以及关闭已打开的文件（就像我已经展示给你看的）。</p>
<p>实际上，我们可以同时处理多个上下文管理器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> A() <span class="keyword">as</span> a, B() <span class="keyword">as</span> b:</span><br><span class="line">    suite</span><br></pre></td></tr></table></figure>
<h2 id="上下文管理协议"><a href="#上下文管理协议" class="headerlink" title="上下文管理协议"></a>上下文管理协议</h2><p>与迭代器类似，实现了迭代协议的函数/对象即为迭代器。实现了上下文协议的函数/对象即为上下文管理器。迭代器协议是实现了<code>__iter__</code>方法。上下文管理协议则是一个类实现<code>__enter__</code> (self)和<code>__exit__</code>(self, exc_type, exc_valye, traceback)方法就可以了。实行如下结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Contextor</span>:</span></span><br><span class="line">    <span class="comment"># __enter__返回一个对象，通常是当前类的实例，也可以是其他对象。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, exc_type, exc_val, exc_tb)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">contextor = Contextor()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> contextor [<span class="keyword">as</span> var]:</span><br><span class="line">    with_body</span><br></pre></td></tr></table></figure>
<p><strong><code>Contextor</code> 实现了<code>__enter__</code>和<code>__exit__</code>这两个上下文管理器协议，当Contextor调用/实例化的时候，则创建了上下文管理器<code>contextor</code></strong></p>
<p>通过定义<code>__enter__</code>和<code>__exit__</code>方法的类（包括自己定义的类，只要加上特定的两个方法即可），我们<strong>可以在<code>with</code>语句里使用它</strong>。我们来看看在底层都发生了什么。</p>
<p><strong>执行步骤：</strong></p>
<ol>
<li>执行 contextor (实例化具有上下文协议的对象，这里也称为上下文表达式)以获取上下文管理器，上下文表达式就是 with 和 as 之间的代码。</li>
<li>加载上下文管理器对象的 <strong>exit</strong>()方法，备用。</li>
<li>调用上下文管理器的 <strong>enter</strong>() 方法</li>
<li>如果有 as var 从句，则将 <strong>enter</strong>() 方法的返回值赋给 var</li>
<li>执行子代码块 with_body</li>
<li>with语句调用上下文管理器之前暂存的 <strong>exit</strong>() 方法，如果 with<em>body 的退出是由异常引发的，那么该异常的 type、value 和 traceback 会作为参数传给 <strong>exit</strong>()，否则传三个 None。然后，<em>_exit</em></em>()需要明确地返回 True 或 False。当返回 True 时，异常不会被向上抛出，当返回 False 时曾会向上抛出。</li>
<li>如果 with_body 的退出由异常引发，它让<strong>exit()</strong>方法来处理异常，并且 <strong>exit</strong>() 的返回值等于 False，那么这个异常将被with语句重新引发抛出一次；如果 <strong>exit</strong>() 的返回值等于 True，那么这个异常就被无视掉，继续执行后面的代码。</li>
</ol>
<h2 id="上下文管理器工具"><a href="#上下文管理器工具" class="headerlink" title="上下文管理器工具"></a>上下文管理器工具</h2><p>通过实现上下文协议定义创建上下文管理器很方便，Python为了更优雅，还专门提供了一个模块用于实现更函数式的上下文管理器用法。Python的<code>contextlib</code>模块专门用于这个目的。</p>
<p><strong>AbstractContextManager</strong> ： 此类在 Python3.6中新增，提供了默认的<strong>enter</strong>()和<strong>exit</strong>()实现。<strong>enter</strong>()返回自身，<strong>exit</strong>()返回 None。</p>
<p><strong>contextmanager： </strong>我们要实现上下文管理器，总是要写一个类。此函数则容许我们<strong>通过一个装饰一个生成器函数</strong>得到一个上下文管理器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> contextlib</span><br><span class="line"></span><br><span class="line"><span class="meta">@contextlib.contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">database</span><span class="params">()</span>:</span></span><br><span class="line">    db = Database()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> db.connected:</span><br><span class="line">            db.connect()</span><br><span class="line">        <span class="keyword">yield</span> db	<span class="comment"># 生成器</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        db.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_query</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> database() <span class="keyword">as</span> db:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'handle ---'</span>, db.query()</span><br></pre></td></tr></table></figure>
<p>使用contextlib 定义一个上下文管理器函数，通过with语句，database调用生成一个上下文管理器，然后调用函数隐式的<code>__enter__</code>方法，并将结果通yield返回。最后退出上下文环境的时候，在exception代码块中执行了<code>__exit__</code>方法。当然我们可以手动模拟上述代码的执行的细节。注意：yield 只能返回一次，返回的对象 被绑定到 as 后的变量，不需要返回时可以直接 yield，不带返回值。退出时则从 yield 之后执行。由于contextmanager继承自ContextDecorator，所以被contextmanager装饰过的生成器也可以用作装饰器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: context = database()    <span class="comment"># 创建上下文管理器</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: context</span><br><span class="line">&lt;contextlib.GeneratorContextManager object at <span class="number">0x107188f10</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: db = context.__enter__() <span class="comment"># 进入with语句</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: db                          <span class="comment"># as语句，返回 Database实例</span></span><br><span class="line">Out[<span class="number">4</span>]: &lt;__main__.Database at <span class="number">0x107188a10</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: db.query()       </span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">'query data'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: db.connected</span><br><span class="line">Out[<span class="number">6</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: db.__exit__(<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>)    <span class="comment"># 退出with语句</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: db</span><br><span class="line">Out[<span class="number">8</span>]: &lt;__main__.Database at <span class="number">0x107188a10</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: db.connected</span><br><span class="line">Out[<span class="number">9</span>]: <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p><strong>ContextDecorator</strong>：  我们可以实现一个上下文管理器，同时可以用作装饰器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AContext</span><span class="params">(ContextDecorator)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'Starting'</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, exc_type, exc_value, traceback)</span>:</span></span><br><span class="line">        print(<span class="string">'Finishing'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 with 中使用</span></span><br><span class="line"><span class="keyword">with</span> AContext():</span><br><span class="line">    print(<span class="string">'祖国伟大'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用作装饰器</span></span><br><span class="line"><span class="meta">@AContext()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_sth</span><span class="params">(sth)</span>:</span></span><br><span class="line">    print(sth)</span><br><span class="line"></span><br><span class="line">print_sth(<span class="string">'祖国伟大'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在这两种写法中，有没有发现，第二种写法更好，因为我们减少了一次代码缩进，可读性更强</span></span><br></pre></td></tr></table></figure>
<p><strong>还有一种好处：当我们已经实现了某个上下文管理器时，只要增加一个继承类，该上下文管理器立刻编程装饰器。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> ContextDecorator</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mycontext</span><span class="params">(ContextBaseClass, ContextDecorator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *exc)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/03/python编程进阶/python编程进阶（12）：协程与异步IO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/python编程进阶/python编程进阶（12）：协程与异步IO/" itemprop="url">python编程进阶（12）：协程与异步IO</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-03T15:14:32+08:00">
                2018-07-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python编程进阶/" itemprop="url" rel="index">
                    <span itemprop="name">python编程进阶</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2,483
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  10
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="协程"><a href="#协程" class="headerlink" title="协程"></a>协程</h1><p>子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。</p>
<p>子程序调用总是一个入口，一次返回，调用顺序是明确的。<strong>而协程的调用和子程序不同。协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行</strong>。</p>
<p><strong>注意，在一个子程序中中断，去执行其他子程序，不是函数调用，有点类似CPU的中断</strong>。比如子程序A、B：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">A</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'1'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'2'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'3'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">B</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'x'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'y'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'z'</span></span><br></pre></td></tr></table></figure>
<p>假设由协程执行，在执行A的过程中，可以随时中断，去执行B，B也可能在执行过程中中断再去执行A，结果可能是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">x</span><br><span class="line">y</span><br><span class="line"><span class="number">3</span></span><br><span class="line">z</span><br></pre></td></tr></table></figure>
<h2 id="多线程比，协程有何优势？"><a href="#多线程比，协程有何优势？" class="headerlink" title="多线程比，协程有何优势？"></a>多线程比，协程有何优势？</h2><p><strong>最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销</strong>，和多线程比，线程数量越多，协程的性能优势就越明显。</p>
<p><strong>第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突</strong>，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。</p>
<p><strong>因为协程是一个线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程</strong>，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。Python对协程的支持是通过generator实现的。</p>
<h2 id="一个例子：生产者－消费者的协程"><a href="#一个例子：生产者－消费者的协程" class="headerlink" title="一个例子：生产者－消费者的协程"></a>一个例子：生产者－消费者的协程</h2><p><strong>现在我们要让生产者发送1,2,3,4,5给消费者，消费者接受数字，返回状态给生产者，而我们的消费者只需要3,4,5就行了，当数字等于3时，会返回一个错误的状态。最终我们需要由主程序来监控生产者－消费者的过程状态，调度结束程序。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding:utf-8</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span><span class="params">()</span>:</span></span><br><span class="line">    status = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        n = <span class="keyword">yield</span> status</span><br><span class="line">        print(<span class="string">"我拿到了&#123;&#125;!"</span>.format(n))</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">3</span>:</span><br><span class="line">            status = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span><span class="params">(consumer)</span>:</span></span><br><span class="line">    n = <span class="number">5</span></span><br><span class="line">    <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># yield给主程序返回消费者的状态</span></span><br><span class="line">        <span class="comment"># consumer.send(n)把n传值给c生成器，同时返回c生成器yield的结果（相当于fetch取一个放一个东西）</span></span><br><span class="line">        <span class="keyword">yield</span> consumer.send(n)	</span><br><span class="line">        n -= <span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    c = consumer()	<span class="comment"># c产生一个生成器(带yield语句)</span></span><br><span class="line">    c.send(<span class="literal">None</span>)	<span class="comment"># consumer()程序推进到yield，但yield还未被执行.send()是传值给生成器的语句</span></span><br><span class="line">    p = producer(c)	<span class="comment"># p也产生一个生成器，但传入c生成器，与p进行通信</span></span><br><span class="line">    <span class="keyword">for</span> status <span class="keyword">in</span> p:<span class="comment"># 循环获取p生成器yield回来的状态</span></span><br><span class="line">        <span class="keyword">if</span> status == <span class="literal">False</span>:</span><br><span class="line">            print(<span class="string">"我只要3,4,5就行啦"</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">"程序结束"</span>)</span><br></pre></td></tr></table></figure>
<p>上面这个例子是典型的生产者－消费者问题，我们用协程的方式来实现它。</p>
<p>第一句<code>c = consumer()</code>，因为consumer函数中存在yield语句，python会把它当成一个generator，因此在运行这条语句后，python并不会像执行函数一样，而是返回了一个generator object。</p>
<p>第二条语句<code>c.send(None)</code>，这条语句的作用是将consumer（即变量c，它是一个generator）中的语句推进到第一个yield语句出现的位置，那么在例子中，consumer中的<code>status = True</code>和<code>while True:</code>都已经被执行了，程序停留在<code>n = yield status</code>的位置（注意：此时这条语句还没有被执行），上面说的send(None)语句十分重要，如果漏写这一句，那么程序直接报错</p>
<p>第三句<code>p = producer(c)</code>，这里则像上面一样定义了producer的生成器，注意的是这里我们传入了消费者的生成器，来让producer跟consumer通信。</p>
<p>第四句<code>for status in p:</code>，这条语句会循环地运行producer和获取它yield回来的状态。</p>
<p>现在程序流进入了producer里面，我们直接看<code>yield consumer.send(n)</code>，生产者调用了消费者的<strong>send()方法，把n发送给consumer</strong>（即c），在consumer中的<code>n = yield status</code>，n拿到的是消费者发送的数字，同时，consumer用yield的方式把状态（status）返回给消费者，注意：这时producer（即消费者）的<code>consumer.send()</code>调用返回的就是consumer中yield的status！消费者马上将status返回给调度它的主程序，主程序获取状态，判断是否错误，若错误，则终止循环，结束程序。上面看起来有点绕，<strong>其实这里面<code>generator.send(n)</code>的作用是：把n发送generator(生成器)中yield的赋值语句中，同时返回generator中yield的变量（结果）。</strong></p>
<p>于是程序便一直运作，直至consumer中获取的n的值变为3！此时consumer把status变为False，最后返回到主程序，主程序中断循环，程序结束。</p>
<h2 id="Coroutine与Generator"><a href="#Coroutine与Generator" class="headerlink" title="Coroutine与Generator"></a>Coroutine与Generator</h2><p>有些人会把生成器（generator）和协程（coroutine）的概念混淆，我以前也会这样，不过其实发现，两者的区别还是很大的。</p>
<p>直接上最重要的区别：</p>
<ul>
<li>generator总是生成值，一般是迭代的序列</li>
<li>coroutine关注的是消耗值，是数据(data)的消费者</li>
<li>coroutine不会与迭代操作关联，而generator会</li>
<li>coroutine强调协同控制程序流，generator强调保存状态和产生数据</li>
</ul>
<p>相似的是，它们都是不用return来实现重复调用的函数/对象，都用到了yield(中断/恢复)的方式来实现</p>
<h1 id="asyncio"><a href="#asyncio" class="headerlink" title="asyncio"></a>asyncio</h1><p><code>asyncio</code>是Python 3.4版本引入的标准库，直接内置了对异步IO的支持。</p>
<p><code>asyncio</code>的编程模型就是一个消息循环。<strong>我们从<code>asyncio</code>模块中直接获取一个<code>EventLoop</code>的引用，然后把需要执行的协程扔到<code>EventLoop</code>中执行，就实现了异步IO</strong>。用<code>asyncio</code>实现<code>Hello world</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="comment">#@asyncio.coroutine把一个generator标记为coroutine类型，然后，我们就把这个coroutine扔到EventLoop中执行。</span></span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"Hello world!"</span>)</span><br><span class="line">    <span class="comment"># 异步调用asyncio.sleep(1):</span></span><br><span class="line">    r = <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">"Hello again!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取EventLoop:</span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"><span class="comment"># 执行coroutine</span></span><br><span class="line">loop.run_until_complete(hello())	<span class="comment"># 循环执行EventLoop里要完成的事件</span></span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure>
<p><code>hello()</code>会首先打印出<code>Hello world!</code>，然后，<strong><code>yield from</code>语法可以让我们方便地调用另一个<code>generator</code></strong>。由于<strong><code>asyncio.sleep()</code>也是一个<code>coroutine</code></strong>，<strong>所以线程不会等待<code>asyncio.sleep()</code>，而是直接中断并执行下一个消息循环</strong>。当<code>asyncio.sleep()</code>返回时，线程就可以从<code>yield from</code>拿到返回值（此处是<code>None</code>），然后接着执行下一行语句。<strong>把<code>asyncio.sleep(1)</code>看成是一个耗时1秒的IO操作，在此期间，主线程并未等待，而是去执行<code>EventLoop</code>中其他可以执行的<code>coroutine</code>了，因此可以实现并发执行</strong>。</p>
<p>我们用Task封装两个<code>coroutine</code>试试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'Hello world! (%s)'</span> % threading.currentThread())</span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'Hello again! (%s)'</span> % threading.currentThread())</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">tasks = [hello(), hello()]</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure>
<p>观察执行过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hello world! (&lt;_MainThread(MainThread, started <span class="number">140735195337472</span>)&gt;)</span><br><span class="line">Hello world! (&lt;_MainThread(MainThread, started <span class="number">140735195337472</span>)&gt;)	<span class="comment"># asyncio.sleep(1)会挂起并去执行下一个hello()协程</span></span><br><span class="line">(暂停约<span class="number">1</span>秒)</span><br><span class="line">Hello again! (&lt;_MainThread(MainThread, started <span class="number">140735195337472</span>)&gt;)</span><br><span class="line">Hello again! (&lt;_MainThread(MainThread, started <span class="number">140735195337472</span>)&gt;)</span><br></pre></td></tr></table></figure>
<p>总结：</p>
<ul>
<li>asyncio<code>提供了完善的异步IO支持；</code></li>
<li><code>异步操作需要在</code>coroutine<code>中通过</code>yield from`完成；</li>
<li>多个<code>coroutine</code>可以封装成一组Task然后并发执行</li>
</ul>
<h1 id="async-await"><a href="#async-await" class="headerlink" title="async/await"></a>async/await</h1><p>用<code>asyncio</code>提供的<code>@asyncio.coroutine</code>可以把一个generator标记为coroutine类型，然后在coroutine内部用<code>yield from</code>调用另一个coroutine实现异步操作。</p>
<p>为了简化并更好地标识异步IO，从Python 3.5开始引入了新的语法<code>async</code>和<code>await</code>，可以让coroutine的代码更简洁易读。</p>
<p>请注意，<code>async</code>和<code>await</code>是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换：</p>
<ol>
<li>把<code>@asyncio.coroutine</code>替换为<code>async</code>；</li>
<li>把<code>yield from</code>替换为<code>await</code>。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    print(<span class="string">"Compute %s + %s ..."</span> % (x, y))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> x + y</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">print_sum</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    result = <span class="keyword">await</span> compute(x, y)</span><br><span class="line">    print(<span class="string">"%s + %s = %s"</span> % (x, y, result))</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(print_sum(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># tasks = [print_sum(1, 2), print_sum(3, 4)]</span></span><br><span class="line"><span class="comment"># loop.run_until_complete(asyncio.wait(tasks))</span></span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure>
<p>当事件循环开始运行时，它会在Task中寻找coroutine来执行调度，因为事件循环注册了<code>print_sum()</code>，因此<code>print_sum()</code>被调用，<strong>执行<code>result = await compute(x, y)</code>这条语句（等同于<code>result = yield from compute(x, y)）</code></strong>，<strong>因为<code>compute()</code>自身就是一个coroutine</strong>，因此<code>print_sum()</code>这个协程就会暂时被挂起，<strong><code>compute()</code>被加入到事件循环中</strong>，程序流执行compute()中的print语句，打印”Compute %s + %s …”，然后执行了<code>await asyncio.sleep(1.0)</code>，因为<strong><code>asyncio.sleep()</code>也是一个coroutine，接着<code>compute()</code>就会被挂起，等待计时器读秒，在这1秒的过程中，事件循环会在队列中查询可以被调度的coroutine</strong>，而因为此前<code>print_sum()</code>与<code>compute()</code>都被挂起了，因此事件循环会停下来等待协程的调度(如果有其他协程task就会在等待时间内去执行并返回)，当计时器读秒结束后，程序流便会返回到<code>compute()</code>中执行return语句，结果会返回到<code>print_sum()</code>中的result中，最后打印result，事件队列中没有可以调度的任务了，此时<code>loop.close()</code>把事件队列关闭，程序结束。</p>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/01/python编程进阶/python编程进阶（11）：使用C扩展/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/01/python编程进阶/python编程进阶（11）：使用C扩展/" itemprop="url">python编程进阶（11）：使用C扩展</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-01T09:14:32+08:00">
                2018-07-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python编程进阶/" itemprop="url" rel="index">
                    <span itemprop="name">python编程进阶</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3,061
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  13
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="使用C扩展"><a href="#使用C扩展" class="headerlink" title="使用C扩展"></a>使用C扩展</h1><p>CPython还为开发者实现了一个有趣的特性，使用Python可以轻松调用C代码</p>
<p>开发者有三种方法可以在自己的Python代码中来调用C编写的函数-<code>ctypes</code>，<code>SWIG</code>，<code>Python/C API</code>。每种方式也都有各自的利弊。</p>
<p>首先，我们要明确为什么要在Python中调用C？</p>
<p>常见原因如下：</p>
<ul>
<li>你要提升代码的运行速度，而且你知道C要比Python快50倍以上</li>
<li>C语言中有很多传统类库，而且有些正是你想要的，但你又不想用Python去重写它们</li>
<li>想对从内存到文件接口这样的底层资源进行访问</li>
<li>不需要理由，就是想这样做</li>
</ul>
<h2 id="CTypes"><a href="#CTypes" class="headerlink" title="CTypes"></a>CTypes</h2><p>Python中的<a href="https://docs.python.org/2/library/ctypes.html" target="_blank" rel="noopener">ctypes模块</a>可能是Python调用C方法中最简单的一种。ctypes模块提供了和C语言兼容的数据类型和函数来加载dll文件，因此在调用时不需对源文件做任何的修改。也正是如此奠定了这种方法的简单性。</p>
<p>示例如下</p>
<p>实现两数求和的C代码，保存为<code>add.c</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//sample C file to add 2 numbers - int and floats</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span>, <span class="keyword">float</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span> num1, <span class="keyword">float</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来将C文件编译为<code>.so</code>文件(windows下为DLL)。下面操作会生成adder.so文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#For Linux</span></span><br><span class="line">$  gcc -shared -Wl,-soname,adder -o adder.so -fPIC add.c</span><br><span class="line"></span><br><span class="line"><span class="comment">#For Mac</span></span><br><span class="line">$ gcc -shared -Wl,-install_name,adder.so -o adder.so -fPIC add.c</span><br></pre></td></tr></table></figure>
<p>现在在你的Python代码中来调用它</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">#load the shared object file</span></span><br><span class="line">adder = CDLL(<span class="string">'./adder.so'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find sum of integers</span></span><br><span class="line">res_int = adder.add_int(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of 4 and 5 = "</span> + str(res_int)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find sum of floats</span></span><br><span class="line">a = c_float(<span class="number">5.5</span>)</span><br><span class="line">b = c_float(<span class="number">4.1</span>)</span><br><span class="line"></span><br><span class="line">add_float = adder.add_float</span><br><span class="line">add_float.restype = c_float</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of 5.5 and 4.1 = "</span>, str(add_float(a, b))</span><br></pre></td></tr></table></figure>
<p>输出如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sum of <span class="number">4</span> <span class="keyword">and</span> <span class="number">5</span> = <span class="number">9</span></span><br><span class="line">Sum of <span class="number">5.5</span> <span class="keyword">and</span> <span class="number">4.1</span> =  <span class="number">9.60000038147</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，C文件是自解释的，它包含两个函数，分别实现了整形求和和浮点型求和。</p>
<p>在Python文件中，一开始先导入ctypes模块，然后使用CDLL函数来加载我们创建的库文件。这样我们就可以通过变量<code>adder</code>来使用C类库中的函数了。当<code>adder.add_int()</code>被调用时，内部将发起一个对C函数<code>add_int</code>的调用。ctypes接口允许我们在调用C函数时使用原生Python中默认的字符串型和整型。</p>
<p>而对于其他类似布尔型和浮点型这样的类型，必须要使用正确的ctype类型才可以。如向<code>adder.add_float()</code>函数传参时, 我们要先将Python中的十进制值转化为c_float类型，然后才能传送给C函数。这种方法虽然简单，清晰，但是却很受限。例如，并不能在C中对对象进行操作。</p>
<h2 id="SWIG"><a href="#SWIG" class="headerlink" title="SWIG"></a>SWIG</h2><p>SWIG是Simplified Wrapper and Interface Generator的缩写。是Python中调用C代码的另一种方法。在这个方法中，开发人员必须编写一个额外的接口文件来作为SWIG(终端工具)的入口。</p>
<p>Python开发者一般不会采用这种方法，因为大多数情况它会带来不必要的复杂。而当你有一个C/C++代码库需要被多种语言调用时，这将是个非常不错的选择。</p>
<p>示例如下(来自<a href="http://www.swig.org/tutorial.html" target="_blank" rel="noopener">SWIG官网</a>)</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">​```C</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="keyword">double</span> My_variable = <span class="number">3.0</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fact</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> n*fact(n<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">my_mod</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x%y);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">get_time</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">time_t</span> ltime;</span><br><span class="line">    time(&amp;ltime);</span><br><span class="line">    <span class="keyword">return</span> ctime(&amp;ltime);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编译它</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">unix % swig -python example.i</span><br><span class="line">unix % gcc -c example.c example_wrap.c \</span><br><span class="line">    -I/usr/local/include/python2<span class="number">.1</span></span><br><span class="line">unix % ld -shared example.o example_wrap.o -o _example.so</span><br></pre></td></tr></table></figure>
<p>最后，Python的输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> example</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.fact(<span class="number">5</span>)</span><br><span class="line"><span class="number">120</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.my_mod(<span class="number">7</span>,<span class="number">3</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>example.get_time()</span><br><span class="line"><span class="string">'Sun Feb 11 23:01:07 1996'</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>我们可以看到，使用SWIG确实达到了同样的效果，虽然下了更多的工夫，但如果你的目标是多语言还是很值得的。</p>
<h2 id="Python-C-API"><a href="#Python-C-API" class="headerlink" title="Python/C API"></a>Python/C API</h2><p><a href="https://docs.python.org/2/c-api/" target="_blank" rel="noopener">Python/C API</a>可能是被最广泛使用的方法。它不仅简单，而且可以在C代码中操作你的Python对象。</p>
<p>这种方法需要以特定的方式来编写C代码以供Python去调用它。所有的Python对象都被表示为一种叫做PyObject的结构体，并且<code>Python.h</code>头文件中提供了各种操作它的函数。例如，如果PyObject表示为PyListType(列表类型)时，那么我们便可以使用<code>PyList_Size()</code>函数来获取该结构的长度，类似Python中的<code>len(list)</code>函数。大部分对Python原生对象的基础函数和操作在<code>Python.h</code>头文件中都能找到。</p>
<p>示例</p>
<p>编写一个C扩展，添加所有元素到一个Python列表(所有元素都是数字)</p>
<p>来看一下我们要实现的效果，这里演示了用Python调用C扩展的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Though it looks like an ordinary python import, the addList module is implemented in C</span></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l))</span><br></pre></td></tr></table></figure>
<p>上面的代码和普通的Python文件并没有什么分别，导入并使用了另一个叫做<code>addList</code>的Python模块。唯一差别就是这个模块(addList)并不是用Python编写的，而是C。</p>
<p>接下来我们看看如何用C编写<code>addList</code>模块，这可能看起来有点让人难以接受，但是一旦你了解了这之中的各种组成，你就可以一往无前了。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Python.h has all the required function definitions to manipulate the Python objects</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Python.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//This is the function that is called from your python code</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> PyObject* <span class="title">addList_add</span><span class="params">(PyObject* self, PyObject* args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    PyObject * listObj;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//The input arguments come as a tuple, we parse the args to get the various variables</span></span><br><span class="line">    <span class="comment">//In this case it's only one list variable, which will now be referenced by listObj</span></span><br><span class="line">    <span class="keyword">if</span> (! PyArg_ParseTuple( args, <span class="string">"O"</span>, &amp;listObj ))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//length of the list</span></span><br><span class="line">    <span class="keyword">long</span> length = PyList_Size(listObj);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//iterate over all the elements</span></span><br><span class="line">    <span class="keyword">int</span> i, sum =<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">        <span class="comment">//get an element out of the list - the element is also a python objects</span></span><br><span class="line">        PyObject* temp = PyList_GetItem(listObj, i);</span><br><span class="line">        <span class="comment">//we know that object represents an integer - so convert it into C long</span></span><br><span class="line">        <span class="keyword">long</span> elem = PyInt_AsLong(temp);</span><br><span class="line">        sum += elem;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//value returned back to python code - another python object</span></span><br><span class="line">    <span class="comment">//build value here converts the C long to a python integer</span></span><br><span class="line">    <span class="keyword">return</span> Py_BuildValue(<span class="string">"i"</span>, sum);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//This is the docstring that corresponds to our 'add' function.</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> addList_docs[] =</span><br><span class="line"><span class="string">"add(  ): add all elements of the list\n"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* This table contains the relavent info mapping -</span></span><br><span class="line"><span class="comment">   &lt;function-name in python module&gt;, &lt;actual-function&gt;,</span></span><br><span class="line"><span class="comment">   &lt;type-of-args the function expects&gt;, &lt;docstring associated with the function&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> PyMethodDef addList_funcs[] = &#123;</span><br><span class="line">    &#123;<span class="string">"add"</span>, (PyCFunction)addList_add, METH_VARARGS, addList_docs&#125;,</span><br><span class="line">    &#123;<span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>&#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   addList is the module name, and this is the initialization block of the module.</span></span><br><span class="line"><span class="comment">   &lt;desired module name&gt;, &lt;the-info-table&gt;, &lt;module's-docstring&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">initaddList</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    Py_InitModule3(<span class="string">"addList"</span>, addList_funcs,</span><br><span class="line">            <span class="string">"Add all ze lists"</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>逐步解释</p>
<ul>
<li><code>Python.h</code>头文件中包含了所有需要的类型(Python对象类型的表示)和函数定义(对Python对象的操作)</li>
<li>接下来我们编写将要在Python调用的函数, 函数传统的命名方式由{模块名}_{函数名}组成，所以我们将其命名为<code>addList_add</code></li>
<li>然后填写想在模块内实现函数的相关信息表，每行一个函数，以空行作为结束</li>
<li>最后的模块初始化块签名为<code>PyMODINIT_FUNC init{模块名}</code>。</li>
</ul>
<p>函数<code>addList_add</code>接受的参数类型为PyObject类型结构(同时也表示为元组类型，因为Python中万物皆为对象，所以我们先用PyObject来定义)。传入的参数则通过<code>PyArg_ParseTuple()</code>来解析。第一个参数是被解析的参数变量。第二个参数是一个字符串，告诉我们如何去解析元组中每一个元素。字符串的第n个字母正是代表着元组中第n个参数的类型。例如，”i”代表整形，”s”代表字符串类型, “O”则代表一个Python对象。接下来的参数都是你想要通过<code>PyArg_ParseTuple()</code>函数解析并保存的元素。这样参数的数量和模块中函数期待得到的参数数量就可以保持一致，并保证了位置的完整性。例如，我们想传入一个字符串，一个整数和一个Python列表，可以这样去写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int n;</span><br><span class="line">char *s;</span><br><span class="line">PyObject* list;</span><br><span class="line">PyArg_ParseTuple(args, <span class="string">"siO"</span>, &amp;n, &amp;s, &amp;list);</span><br></pre></td></tr></table></figure>
<p>在这种情况下，我们只需要提取一个列表对象，并将它存储在<code>listObj</code>变量中。然后用列表对象中的<code>PyList_Size()</code>函数来获取它的长度。就像Python中调用<code>len(list)</code>。</p>
<p>现在我们通过循环列表，使用<code>PyList_GetItem(list, index)</code>函数来获取每个元素。这将返回一个<code>PyObject*</code>对象。既然Python对象也能表示<code>PyIntType</code>，我们只要使用<code>PyInt_AsLong(PyObj *)</code>函数便可获得我们所需要的值。我们对每个元素都这样处理，最后再得到它们的总和。</p>
<p>总和将被转化为一个Python对象并通过<code>Py_BuildValue()</code>返回给Python代码，这里的i表示我们要返回一个Python整形对象。</p>
<p>现在我们已经编写完C模块了。将下列代码保存为<code>setup.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#build the modules</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">'addList'</span>, version=<span class="string">'1.0'</span>,  \</span><br><span class="line">      ext_modules=[Extension(<span class="string">'addList'</span>, [<span class="string">'adder.c'</span>])])</span><br></pre></td></tr></table></figure>
<p>并且运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>
<p>现在应该已经将我们的C文件编译安装到我们的Python模块中了。</p>
<p>在一番辛苦后，让我们来验证下我们的模块是否有效</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#module that talks to the C code</span></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l))</span><br></pre></td></tr></table></figure>
<p>输出结果如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sum of List - [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>] = <span class="number">15</span></span><br></pre></td></tr></table></figure>
<p>如你所见，我们已经使用Python.h API成功开发出了我们第一个Python C扩展。这种方法看似复杂，但你一旦习惯，它将变的非常有效。</p>
<p>Python调用C代码的另一种方式便是使用<a href="http://cython.org/" target="_blank" rel="noopener">Cython</a>让Python编译的更快。但是Cython和传统的Python比起来可以将它理解为另一种语言，所以我们就不在这里过多描述了。</p>
<h1 id="补充两个知识点"><a href="#补充两个知识点" class="headerlink" title="补充两个知识点"></a>补充两个知识点</h1><h2 id="列表辗平"><a href="#列表辗平" class="headerlink" title="列表辗平"></a>列表辗平</h2><p>可以通过使用<code>itertools</code>包中的<code>itertools.chain.from_iterable</code>轻松快速的辗平一个列表。下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a_list = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">print(list(itertools.chain.from_iterable(a_list)))</span><br><span class="line"><span class="comment"># Output: [1, 2, 3, 4, 5, 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">print(list(itertools.chain(*a_list)))</span><br><span class="line"><span class="comment"># Output: [1, 2, 3, 4, 5, 6]</span></span><br></pre></td></tr></table></figure>
<h2 id="for-else从句"><a href="#for-else从句" class="headerlink" title="for-else从句"></a>for-else从句</h2><p><code>for</code>循环还有一个<code>else</code>从句，我们大多数人并不熟悉。<strong>这个<code>else</code>从句会在循环正常结束时执行。这意味着，循环没有遇到任何break。若循环被某些因素打破，则不会执行else语句</strong>. 一旦你掌握了何时何地使用它，它真的会非常有用。我自己对它真是相见恨晚。</p>
<p>有个常见的构造是跑一个循环，并查找一个元素。如果这个元素被找到了，我们使用<code>break</code>来中断这个循环。有两个场景会让循环停下来。</p>
<ul>
<li>第一个是当一个元素被找到，<code>break</code>被触发。</li>
<li>第二个场景是循环结束。</li>
</ul>
<p>现在我们也许想知道其中哪一个，才是导致循环完成的原因。一个方法是先设置一个标记，然后在循环结束时打上标记。另一个是使用<code>else</code>从句。</p>
<p>这就是<code>for/else</code>循环的基本结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> container:</span><br><span class="line">    <span class="keyword">if</span> search_something(item):</span><br><span class="line">        <span class="comment"># Found it!</span></span><br><span class="line">        process(item)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Didn't find anything..</span></span><br><span class="line">    not_found_in_container()</span><br></pre></td></tr></table></figure>
<p>考虑下这个简单的案例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line">            print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n / x)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>它会找出2到10之间的数字的因子。现在是趣味环节了。我们可以加上一个附加的else语句块，来抓住质数，并且告诉我们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line">        <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line">            print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n / x)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:	<span class="comment"># 输出没有循环结束仍未找到因子的质数</span></span><br><span class="line">        <span class="comment"># loop fell through without finding a factor</span></span><br><span class="line">        print(n, <span class="string">'is a prime number'</span>)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/30/python编程进阶/python编程进阶（10）：sort、lambda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/30/python编程进阶/python编程进阶（10）：sort、lambda/" itemprop="url">python编程进阶（10）：sort、lambda</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-30T22:14:32+08:00">
                2018-06-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python编程进阶/" itemprop="url" rel="index">
                    <span itemprop="name">python编程进阶</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1,461
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="sort与sorted区别"><a href="#sort与sorted区别" class="headerlink" title="sort与sorted区别"></a>sort与sorted区别</h1><p>我们需要对List进行排序，Python提供了两个方法对给定的List L进行排序，</p>
<ul>
<li>方法1.用List的成员函数sort进行排序</li>
<li>方法2.用built-in函数sorted进行排序</li>
</ul>
<p>list.sort()与sorted()的不同在于，list.sort是在原位重新排列列表，而sorted()是产生一个新的列表。python中列表的内置函数list.sort（）只可以对列表中的元素进行排序，而全局性的sorted（）函数则对所有可迭代的对象都是适用的；并且list.sort（）函数是内置函数，会改变当前对象，而sorted（）函数只会返回一个排序后的当前对象的副本，而不会改变当前对象。</p>
<blockquote>
<p>原型：sort（fun，key，reverse=False）</p>
<p>sorted(itrearble, cmp=None, key=None,reverse=False)</p>
</blockquote>
<h2 id="内置函数sort（）"><a href="#内置函数sort（）" class="headerlink" title="内置函数sort（）"></a>内置函数sort（）</h2><p>参数fun是表明此sort函数是基于何种算法进行排序的，一般默认情况下python中用的是归并排序，并且一般情况下我们是不会重写此参数的，所以基本可以忽略；</p>
<p>参数key用来指定一个函数，此函数在每次元素比较时被调用，此函数代表排序的规则，也就是你按照什么规则对你的序列进行排序；</p>
<p>参数reverse是用来表明是否逆序，默认的False情况下是按照升序的规则进行排序的，当reverse=True时，便会按照降序进行排序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> attrgetter,itemgetter</span><br><span class="line"> </span><br><span class="line">list1 = [(<span class="number">2</span>,<span class="string">'huan'</span>,<span class="number">23</span>),(<span class="number">12</span>,<span class="string">'the'</span>,<span class="number">14</span>),(<span class="number">23</span>,<span class="string">'liu'</span>,<span class="number">90</span>)]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用默认参数进行排序，即按照元组中第一个元素进行排序</span></span><br><span class="line">list1.sort()</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"><span class="comment">#输出结果为[(2, 'huan', 23), (12, 'the', 14), (23, 'liu', 90)]</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用匿名表达式重写key所代表的函数,按照元组的第二个元素（下标为1）进行排序</span></span><br><span class="line">list1.sort(key=<span class="keyword">lambda</span> x:(x[<span class="number">1</span>]))</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"><span class="comment">#[(2, 'huan', 23), (23, 'liu', 90), (12, 'the', 14)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用匿名函数重写key所代表的函数，先按照元组中下标为2的进行排序，</span></span><br><span class="line"><span class="comment"># 对于下标2处元素相同的，则按下标为0处的元素进行排序</span></span><br><span class="line">list1.sort(key=<span class="keyword">lambda</span> x:(x[<span class="number">2</span>],x[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"><span class="comment">#[(12, 'the', 14), (2, 'huan', 23), (23, 'liu', 90)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用operator模块中的itemgetter函数进行重写key所代表的函数，按照下标为1处的元素(第二个)进行排序</span></span><br><span class="line">list1.sort(key=itemgetter(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"><span class="comment">#[(2, 'huan', 23), (23, 'liu', 90), (12, 'the', 14)]</span></span><br></pre></td></tr></table></figure>
<h2 id="全局函数sorted（）"><a href="#全局函数sorted（）" class="headerlink" title="全局函数sorted（）"></a>全局函数sorted（）</h2><p>对于sorted（）函数中key的重写，和sort（）函数中是一样的，所以刚刚对于sort（）中讲解的方法，都是适用于sorted（）函数中。sorted（）最后会将排序的结果放到一个新的列表中，而不是对iterable本身进行修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sorted(<span class="string">'123456'</span>)  <span class="comment"># 字符串</span></span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>, <span class="string">'5'</span>, <span class="string">'6'</span>]</span><br><span class="line"></span><br><span class="line">sorted([<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>])  <span class="comment"># 列表</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">sorted(&#123;<span class="number">1</span>:<span class="string">'q'</span>,<span class="number">3</span>:<span class="string">'c'</span>,<span class="number">2</span>:<span class="string">'g'</span>&#125;) <span class="comment"># 字典， 默认对字典的键进行排序</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">sorted(&#123;<span class="number">1</span>:<span class="string">'q'</span>,<span class="number">3</span>:<span class="string">'c'</span>,<span class="number">2</span>:<span class="string">'g'</span>&#125;.keys())  <span class="comment"># 对字典的键</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">sorted(&#123;<span class="number">1</span>:<span class="string">'q'</span>,<span class="number">3</span>:<span class="string">'c'</span>,<span class="number">2</span>:<span class="string">'g'</span>&#125;.values())  <span class="comment"># 对字典的值</span></span><br><span class="line">[<span class="string">'c'</span>, <span class="string">'g'</span>, <span class="string">'q'</span>]</span><br><span class="line"></span><br><span class="line">sorted(&#123;<span class="number">1</span>:<span class="string">'q'</span>,<span class="number">3</span>:<span class="string">'c'</span>,<span class="number">2</span>:<span class="string">'g'</span>&#125;.items())  <span class="comment"># 对键值对组成的元组的列表</span></span><br><span class="line">[(<span class="number">1</span>, <span class="string">'q'</span>), (<span class="number">2</span>, <span class="string">'g'</span>), (<span class="number">3</span>, <span class="string">'c'</span>)]</span><br></pre></td></tr></table></figure>
<p>对元素指定的某一部分进行排序,关键字排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 想要按照-后的数字的大小升序排序。要用到key</span></span><br><span class="line">s =[<span class="string">'Chr1-10.txt'</span>,<span class="string">'Chr1-1.txt'</span>,<span class="string">'Chr1-2.txt'</span>,<span class="string">'Chr1-14.txt'</span>,<span class="string">'Chr1-3.txt'</span>,<span class="string">'Chr1-20.txt'</span>,<span class="string">'Chr1-5.txt'</span>]</span><br><span class="line"></span><br><span class="line">sorted(s, key=<span class="keyword">lambda</span> d :int(d.split(<span class="string">'-'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 ['Chr1-1.txt', 'Chr1-2.txt', 'Chr1-3.txt','Chr1-5.txt', 'Chr1-10.txt', 'Chr1-14.txt', 'Chr1-20.txt']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这就是key的功能，制定排序的关键字，通常都是一个lambda函数，当然你也可以事先定义好这个函数。如果不讲这个关键字转化为整型，结果是这样的：</span></span><br><span class="line">sorted(s, key=<span class="keyword">lambda</span> d : d.split(<span class="string">'-'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 ['Chr1-1.txt', 'Chr1-10.txt','Chr1-14.txt', 'Chr1-2.txt', 'Chr1-20.txt', 'Chr1-3.txt', 'Chr1-5.txt']</span></span><br></pre></td></tr></table></figure>
<p>这相当于把这个关键字当做字符串了，很显然，在python中，’2’ &gt; ‘10’。cmp不怎么用，因为key和reverse比单独一个cmp效率要高。</p>
<h1 id="lambda的各种用法"><a href="#lambda的各种用法" class="headerlink" title="lambda的各种用法"></a>lambda的各种用法</h1><p>1， 用在过滤函数中，指定过滤列表元素的条件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filter(<span class="keyword">lambda</span> x: x % <span class="number">3</span> == <span class="number">0</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"> &gt; [<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<p>2， 用在排序函数中，指定对列表中所有元素进行排序的准则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sorted([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], key=<span class="keyword">lambda</span> x: abs(<span class="number">5</span>-x))</span><br><span class="line">&gt; [<span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<p>3， 用在reduce函数中，指定列表中两两相邻元素的结合条件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reduce(<span class="keyword">lambda</span> a, b: <span class="string">'&#123;&#125;, &#123;&#125;'</span>.format(a, b), [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">&gt; <span class="string">'1, 2, 3, 4, 5, 6, 7, 8, 9'</span></span><br></pre></td></tr></table></figure>
<p>4， 用在map函数中，指定对列表中每一个元素的共同操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">map(<span class="keyword">lambda</span> x: x+<span class="number">1</span>, [<span class="number">1</span>, <span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">&gt; [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<p>5， 从另一函数中返回一个函数，常用来实现函数装饰器(Wrapper)，例如python的function decorators</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(n)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> x: x + n</span><br><span class="line">f = transform(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> f(<span class="number">3</span>)</span><br><span class="line">&gt; <span class="number">7</span></span><br></pre></td></tr></table></figure>
<p>6，列表排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">9</span>, <span class="number">10</span>), (<span class="number">13</span>, <span class="number">-3</span>)]</span><br><span class="line">a.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># Output: [(13, -3), (4, 1), (1, 2), (9, 10)]</span></span><br></pre></td></tr></table></figure>
<p>7，列表并行排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = zip(list1, list2)</span><br><span class="line">data = sorted(data)</span><br><span class="line">list1, list2 = map(<span class="keyword">lambda</span> t: list(t), zip(*data))</span><br><span class="line"><span class="comment"># zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/30/python编程进阶/python编程进阶（9）：枚举、自省、推导式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mosbyllc">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mosbyllc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/30/python编程进阶/python编程进阶（9）：枚举、自省、推导式/" itemprop="url">python编程进阶（9）：枚举、自省、推导式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-30T19:54:32+08:00">
                2018-06-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python编程进阶/" itemprop="url" rel="index">
                    <span itemprop="name">python编程进阶</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1,470
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h1><p>枚举(<code>enumerate</code>)是Python内置函数。它的用处很难在简单的一行中说明，但是大多数的新人，甚至一些高级程序员都没有意识到它。</p>
<p>它允许我们遍历数据并自动计数，</p>
<p>下面是一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> counter, value <span class="keyword">in</span> enumerate(some_list):</span><br><span class="line">    print(counter, value)</span><br></pre></td></tr></table></figure>
<p>不只如此，<code>enumerate</code>也接受一些可选参数，这使它更有用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grapes'</span>, <span class="string">'pear'</span>]</span><br><span class="line"><span class="keyword">for</span> c, value <span class="keyword">in</span> enumerate(my_list, <span class="number">1</span>):</span><br><span class="line">    print(c, value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line">(<span class="number">1</span>, <span class="string">'apple'</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="string">'banana'</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="string">'grapes'</span>)</span><br><span class="line">(<span class="number">4</span>, <span class="string">'pear'</span>)</span><br></pre></td></tr></table></figure>
<p>上面这个可选参数允许我们定制从哪个数字开始枚举。<br>你还可以用来创建包含索引的元组列表， 例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="string">'apple'</span>, <span class="string">'banana'</span>, <span class="string">'grapes'</span>, <span class="string">'pear'</span>]</span><br><span class="line">counter_list = list(enumerate(my_list, <span class="number">1</span>))</span><br><span class="line">print(counter_list)</span><br><span class="line"><span class="comment"># 输出: [(1, 'apple'), (2, 'banana'), (3, 'grapes'), (4, 'pear')]</span></span><br></pre></td></tr></table></figure>
<h1 id="对象自省"><a href="#对象自省" class="headerlink" title="对象自省"></a>对象自省</h1><p>自省(introspection)，在计算机编程领域里，是指在运行时来判断一个对象的类型的能力。它是Python的强项之一。Python中所有一切都是一个对象，而且我们可以仔细勘察那些对象。Python还包含了许多内置函数和模块来帮助我们。</p>
<h2 id="dir"><a href="#dir" class="headerlink" title="dir"></a>dir</h2><p>在这个小节里我们会学习到<code>dir</code>以及它在自省方面如何给我们提供便利。</p>
<p>它是用于自省的最重要的函数之一。<strong>它返回一个列表，列出了一个对象所拥有的属性和方法</strong>。这里是一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">dir(my_list)</span><br><span class="line"><span class="comment"># Output: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',</span></span><br><span class="line"><span class="comment"># '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',</span></span><br><span class="line"><span class="comment"># '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',</span></span><br><span class="line"><span class="comment"># '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',</span></span><br><span class="line"><span class="comment"># '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',</span></span><br><span class="line"><span class="comment"># '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',</span></span><br><span class="line"><span class="comment"># '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',</span></span><br><span class="line"><span class="comment"># 'remove', 'reverse', 'sort']</span></span><br></pre></td></tr></table></figure>
<p>上面的自省给了我们一个列表对象的所有方法的名字。当你没法回忆起一个方法的名字，这会非常有帮助。如果我们运行<code>dir()</code>而不传入参数，那么它会返回当前作用域的所有名字。</p>
<h2 id="type和id"><a href="#type和id" class="headerlink" title="type和id"></a>type和id</h2><p><code>type</code>函数返回一个对象的类型。举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(type(<span class="string">''</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'str'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type([]))</span><br><span class="line"><span class="comment"># Output: &lt;type 'list'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(&#123;&#125;))</span><br><span class="line"><span class="comment"># Output: &lt;type 'dict'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(dict))</span><br><span class="line"><span class="comment"># Output: &lt;type 'type'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'int'&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>id()</code>函数返回任意不同种类对象的唯一ID内存地址，举个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">"Yasoob"</span></span><br><span class="line">print(id(name))</span><br><span class="line"><span class="comment"># Output: 139972439030304</span></span><br></pre></td></tr></table></figure>
<h2 id="inspect模块"><a href="#inspect模块" class="headerlink" title="inspect模块"></a>inspect模块</h2><p><code>inspect</code>模块也提供了许多有用的函数，来获取活跃对象的信息。比方说，你可以查看一个对象的成员，只需运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line">print(inspect.getmembers(str))</span><br><span class="line"><span class="comment"># Output: [('__add__', &lt;slot wrapper '__add__' of ... ...</span></span><br></pre></td></tr></table></figure>
<p>还有好多个其他方法也能有助于自省。如果你愿意，你可以去探索它们。</p>
<p>inspect.ismodule(object)： 是否为模块<br>inspect.isclass(object)：是否为类<br>inspect.ismethod(object)：是否为方法（bound method written in python）<br>inspect.isfunction(object)：是否为函数(python function, including lambda expression)<br>inspect.isgeneratorfunction(object)：是否为python生成器函数<br>inspect.isgenerator(object):是否为生成器<br>inspect.istraceback(object)： 是否为traceback<br>inspect.isframe(object)：是否为frame<br>inspect.iscode(object)：是否为code<br>inspect.isbuiltin(object)：是否为built-in函数或built-in方法<br>inspect.isroutine(object)：是否为用户自定义或者built-in函数或方法<br>inspect.isabstract(object)：是否为抽象基类<br>inspect.ismethoddescriptor(object)：是否为方法标识符<br>inspect.isdatadescriptor(object)：是否为数字标识符，数字标识符有<code>__get__</code> 和<code>__set__属性； 通常也有__name__和__doc__属性</code><br>inspect.isgetsetdescriptor(object)：是否为getset descriptor<br>inspect.ismemberdescriptor(object)：是否为member descriptor</p>
<h1 id="各种推导式-comprehensions"><a href="#各种推导式-comprehensions" class="headerlink" title="各种推导式(comprehensions)"></a>各种推导式(comprehensions)</h1><p>推导式（又称解析式）是Python的一种独有特性，如果我被迫离开了它，我会非常想念。推导式是可以从一个数据序列构建另一个新的数据序列的结构体。 共有三种推导，在Python2和3中都有支持：</p>
<ul>
<li>列表(<code>list</code>)推导式</li>
<li>字典(<code>dict</code>)推导式</li>
<li>集合(<code>set</code>)推导式</li>
</ul>
<p>我们将一一进行讨论。一旦你知道了使用列表推导式的诀窍，你就能轻易使用任意一种推导式了。</p>
<h2 id="列表推导式（list-comprehensions）"><a href="#列表推导式（list-comprehensions）" class="headerlink" title="列表推导式（list comprehensions）"></a>列表推导式（list comprehensions）</h2><p>列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表。<br>它的结构是在一个中括号里包含一个表达式，然后是一个<code>for</code>语句，然后是0个或多个<code>for</code>或者<code>if</code>语句。那个表达式可以是任意的，意思是你可以在列表中放入任意类型的对象。返回结果将是一个新的列表，在这个以<code>if</code>和<code>for</code>语句为上下文的表达式运行完成之后产生。</p>
<h3 id="规范"><a href="#规范" class="headerlink" title="规范"></a>规范</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">variable = [out_exp <span class="keyword">for</span> out_exp <span class="keyword">in</span> input_list <span class="keyword">if</span> out_exp == <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<h2 id="字典推导式（dict-comprehensions）"><a href="#字典推导式（dict-comprehensions）" class="headerlink" title="字典推导式（dict comprehensions）"></a>字典推导式（dict comprehensions）</h2><p><strong>字典推导和列表推导的使用方法是类似的,只不中括号该改成大括号，毕竟字典本身用的就是大括号。</strong>这里有个我最近发现的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mcase = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">34</span>, <span class="string">'A'</span>: <span class="number">7</span>, <span class="string">'Z'</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">mcase_frequency = &#123;</span><br><span class="line">    k.lower(): mcase.get(k.lower(), <span class="number">0</span>) + mcase.get(k.upper(), <span class="number">0</span>)	<span class="comment"># 执行函数，k为每个字典的关键字</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> mcase.keys()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># mcase_frequency == &#123;'a': 17, 'z': 3, 'b': 34&#125;</span></span><br></pre></td></tr></table></figure>
<p>在上面的例子中我们把同一个字母但不同大小写的值合并起来了。</p>
<p>就我个人来说没有大量使用字典推导式。</p>
<p>你还可以快速对换一个字典的键和值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> some_dict.items()&#125;</span><br></pre></td></tr></table></figure>
<h2 id="集合推导式（set-comprehensions）"><a href="#集合推导式（set-comprehensions）" class="headerlink" title="集合推导式（set comprehensions）"></a>集合推导式（set comprehensions）</h2><p><strong>集合推导式跟列表推导式差不多，都是对一个列表的元素全部执行相同的操作，但集合是一种无重复无序的序列</strong><br><strong>区别：跟列表推到式的区别在于：1.不使用中括号，使用大括号；2.结果中无重复；3.结果是一个set()集合，集合里面是一个序列：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">squared = &#123;x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]&#125;</span><br><span class="line">print(squared)</span><br><span class="line"><span class="comment"># Output: &#123;1, 4&#125;</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    
    
    
    
    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Mosbyllc">
            
              <p class="site-author-name" itemprop="name">Mosbyllc</p>
              <p class="site-description motion-element" itemprop="description">Sometimes thing have to fall apart to make way for better things.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">62</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">74</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/kugua233" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1499913789@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Recommended reading
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://wulc.me/" title="Wulc" target="_blank">Wulc</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/qq_15262671/article/details/78481922" title="Pinard" target="_blank">Pinard</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://donche.github.io/" title="Donche" target="_blank">Donche</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://xtf615.com/" title="XFT" target="_blank">XFT</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://seawaylee.github.io/" title="Seawaylee" target="_blank">Seawaylee</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mosbyllc</span>

  
</div>











        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
